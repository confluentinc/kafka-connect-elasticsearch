Exporting environment variables
Exporting CI
Exporting SEMAPHORE
Exporting SEMAPHORE_AGENT_MACHINE_ENVIRONMENT_TYPE
Exporting SEMAPHORE_AGENT_MACHINE_OS_IMAGE
Exporting SEMAPHORE_AGENT_MACHINE_TYPE
Exporting SEMAPHORE_ARTIFACT_TOKEN
Exporting SEMAPHORE_BLOCK_NAME
Exporting SEMAPHORE_CACHE_BACKEND
Exporting SEMAPHORE_CACHE_S3_BUCKET
Exporting SEMAPHORE_CACHE_USE_EC2_INSTANCE_PROFILE
Exporting SEMAPHORE_GIT_BRANCH
Exporting SEMAPHORE_GIT_COMMITTER
Exporting SEMAPHORE_GIT_COMMIT_AUTHOR
Exporting SEMAPHORE_GIT_COMMIT_RANGE
Exporting SEMAPHORE_GIT_DIR
Exporting SEMAPHORE_GIT_PROVIDER
Exporting SEMAPHORE_GIT_PR_BRANCH
Exporting SEMAPHORE_GIT_PR_NAME
Exporting SEMAPHORE_GIT_PR_NUMBER
Exporting SEMAPHORE_GIT_PR_SHA
Exporting SEMAPHORE_GIT_PR_SLUG
Exporting SEMAPHORE_GIT_REF
Exporting SEMAPHORE_GIT_REF_TYPE
Exporting SEMAPHORE_GIT_REPO_NAME
Exporting SEMAPHORE_GIT_REPO_SLUG
Exporting SEMAPHORE_GIT_SHA
Exporting SEMAPHORE_GIT_URL
Exporting SEMAPHORE_GIT_WORKING_BRANCH
Exporting SEMAPHORE_JOB_CREATION_TIME
Exporting SEMAPHORE_JOB_ID
Exporting SEMAPHORE_JOB_NAME
Exporting SEMAPHORE_JOB_TYPE
Exporting SEMAPHORE_OIDC_TOKEN
Exporting SEMAPHORE_ORGANIZATION_URL
Exporting SEMAPHORE_PIPELINE_0_ARTEFACT_ID
Exporting SEMAPHORE_PIPELINE_ARTEFACT_ID
Exporting SEMAPHORE_PIPELINE_ID
Exporting SEMAPHORE_PIPELINE_NAME
Exporting SEMAPHORE_PIPELINE_PROMOTED_BY
Exporting SEMAPHORE_PIPELINE_PROMOTION
Exporting SEMAPHORE_PIPELINE_RERUN
Exporting SEMAPHORE_PROJECT_ID
Exporting SEMAPHORE_PROJECT_NAME
Exporting SEMAPHORE_TOOLBOX_METRICS_ENABLED
Exporting SEMAPHORE_WORKFLOW_HOOK_SOURCE
Exporting SEMAPHORE_WORKFLOW_ID
Exporting SEMAPHORE_WORKFLOW_NUMBER
Exporting SEMAPHORE_WORKFLOW_RERUN
Exporting SEMAPHORE_WORKFLOW_TRIGGERED_BY
Exporting SEMAPHORE_WORKFLOW_TRIGGERED_BY_API
Exporting SEMAPHORE_WORKFLOW_TRIGGERED_BY_HOOK
Exporting SEMAPHORE_WORKFLOW_TRIGGERED_BY_MANUAL_RUN
Exporting SEMAPHORE_WORKFLOW_TRIGGERED_BY_SCHEDULE
Exporting TERM
Injecting Files
Injecting /home/semaphore/.ssh/id_rsa with file mode 0600
Running the pre-job hook configured in the agent
Running: source /opt/semaphore/agent/hooks/pre-job
Warning: The agent is configured to fail the job if the pre-job hook fails.
Fetching IMDS token
Fetching instance ID
Running on self hosted agent: i-03184953bb4d98938
Fetching AMI ID
Running AMI: ami-0c28fc744e47a0000
Fetching region
Running in us-west-2
Running on agent: s1-prod-ubuntu24-04-amd64-4
Calculating job queue time
Queue time: 10 seconds
Pushing metrics to CloudWatch
Disabling EC2 instance profile for cache
Assuming Semaphore OIDC role
Additional logs and metrics on build instance:
Cloudwatch Logs: https://d-926757b88b.awsapps.com/start/#/console?account_id=519856050701&role_name=developer&destination=https%3A%2F%2Fconsole.aws.amazon.com%2Fcloudwatch%2Fhome%3Fregion%3Dus-west-2%23logsV2%3Alogs-insights%243FqueryDetail%243D%7E%28end%7E0%7Estart%7E-86400%7EtimeType%7E%27RELATIVE%7Etz%7E%27LOCAL%7Eunit%7E%27seconds%7EeditorString%7E%27fields*20*40timestamp*2c*20*40log*2c*20*40message*0a*7c*20sort*20*40timestamp*20desc*0a*7c*20limit*2010000*0a*7c*20filter*20*40logStream*20like*20*22i-03184953bb4d98938*22%7EqueryId%7E%277ad2fb3c-d30c-4bdb-9d65-ea38e786c121%7Esource%7E%28%7E%27*2fsemaphore*2fagent%7E%27*2fsemaphore*2fkern.log%7E%27*2fsemaphore*2fsyslog%7E%27*2fsemaphore*2fsystem%29%7Elang%7E%27CWLI%29
Cloudwatch Metrics: https://d-926757b88b.awsapps.com/start/#/console?account_id=519856050701&role_name=developer&destination=https%3A%2F%2Fconsole.aws.amazon.com%2Fcloudwatch%2Fhome%3Fregion%3Dus-west-2%23metricsV2%3Fgraph%3D%7E%28%29%26query%3D%7E%27i-03184953bb4d98938
Log and Metric FAQ Guide: https://confluentinc.atlassian.net/wiki/spaces/TOOLS/pages/1342058645/Semaphore+FAQ#System-logging-and-metrics-of-my-Semaphore-build-agent
Setting tags
[1]   Done                    aws cloudwatch put-metric-data --metric-name QueueTime --namespace Semaphore --value $queue_time --unit Seconds --dimensions "MachineType=$machine_type"
[2]   Done                    aws cloudwatch put-metric-data --metric-name QueueTime --namespace Semaphore --value $queue_time --unit Seconds --dimensions "MachineType=$machine_type,InstanceId=$instance_id"
Setting up uv environment for post-job
/home/semaphore
Setting up goproxy-aws
Getting CodeArtifact auth token
Ref Type is pull-request, generating a read-only token
Setting up .netrc
OS version: 24.04
Setting up Semaphore-Agent GitHub App variables
Setting up Vault V3 Agent for .netrc management
Setting up .gitconfig
Setting up Bazel downloader config
Setting up .bazelrc
[0m==> Note: Vault Agent version does not match Vault server version. Vault Agent version: 1.20.4, Vault server version: 1.18.5[0m
[0m==> Vault Agent started! Log data will stream in below:
[0m
[0m==> Vault Agent configuration:
[0m
[0m           Api Address 1: http://bufconn[0m
[0m                     Cgo: disabled[0m
[0m               Log Level: error[0m
[0m                 Version: Vault v1.20.4, built 2025-09-23T13:22:38Z[0m
[0m             Version Sha: 55bd8f18c6c84aa89fdede4850a622c57f03bd7e[0m
[0m[0m
Setting up CodeArtifact creds for Bazel
Setting up pip.conf
Setting up .pypirc
Setting up uv.toml
Setting up .npmrc
Setting up .yarnrc.yml
Setting up .ci-tools.ini
Running on EBS volume: vol-080cfdfd8ade6a61c
[34mDEBUG[39m uv 0.8.23
Completed setting up .ci-tools.ini
Setting up gradle.properties
[34mDEBUG[39m Acquired shared lock for `/home/semaphore/.cache/uv`
[34mDEBUG[39m Found project root: `/opt/semaphore/agent/hooks`
[34mDEBUG[39m No workspace root found, using project root
[34mDEBUG[39m Discovered project `ci-build-agent-infra` at: /opt/semaphore/agent/hooks
[34mDEBUG[39m Acquired lock for `/opt/semaphore/agent/hooks`
[34mDEBUG[39m No Python version file found in workspace: /opt/semaphore/agent/hooks
[34mDEBUG[39m Using Python request `==3.13.*` from `requires-python` metadata
[34mDEBUG[39m Checking for Python environment at: `.venv`

[3]   Done                    set_tags
Setting up settings.xml
Setting up coursier credentials.properties
Setting up Dockerhub creds
Setting up Confluent Catalog creds
Setting up Jira creds
[34mDEBUG[39m The project environment's Python version satisfies the request: `Python ==3.13.*`
[34mDEBUG[39m Released lock at `/tmp/uv-4f0123aad978f91a.lock`
[34mDEBUG[39m Skipping environment synchronization due to `--no-sync`
[34mDEBUG[39m Using Python 3.13.7 interpreter at: /opt/semaphore/agent/hooks/.venv/bin/python3
Setting up Docker BuildKit
Setting up Launchdarkly creds
[34mDEBUG[39m Running `python --version`
[34mDEBUG[39m Spawned child 3263 in process group 2905
Python 3.13.7
[34mDEBUG[39m Command exited with code: 0
[34mDEBUG[39m Released lock at `/home/semaphore/.cache/uv/.lock`
last 4 chars of the token: 921d
Successfully completed pre-job script
checkout
Cloning into 'kafka-connect-elasticsearch'...
remote: Enumerating objects: 7072, done.[K
remote: Counting objects:   0% (1/7072)[K
remote: Counting objects:   1% (71/7072)[K
remote: Counting objects:   2% (142/7072)[K
remote: Counting objects:   3% (213/7072)[K
remote: Counting objects:   4% (283/7072)[K
remote: Counting objects:   5% (354/7072)[K
remote: Counting objects:   6% (425/7072)[K
remote: Counting objects:   7% (496/7072)[K
remote: Counting objects:   8% (566/7072)[K
remote: Counting objects:   9% (637/7072)[K
remote: Counting objects:  10% (708/7072)[K
remote: Counting objects:  11% (778/7072)[K
remote: Counting objects:  12% (849/7072)[K
remote: Counting objects:  13% (920/7072)[K
remote: Counting objects:  14% (991/7072)[K
remote: Counting objects:  15% (1061/7072)[K
remote: Counting objects:  16% (1132/7072)[K
remote: Counting objects:  17% (1203/7072)[K
remote: Counting objects:  18% (1273/7072)[K
remote: Counting objects:  19% (1344/7072)[K
remote: Counting objects:  20% (1415/7072)[K
remote: Counting objects:  21% (1486/7072)[K
remote: Counting objects:  22% (1556/7072)[K
remote: Counting objects:  23% (1627/7072)[K
remote: Counting objects:  24% (1698/7072)[K
remote: Counting objects:  25% (1768/7072)[K
remote: Counting objects:  26% (1839/7072)[K
remote: Counting objects:  27% (1910/7072)[K
remote: Counting objects:  28% (1981/7072)[K
remote: Counting objects:  29% (2051/7072)[K
remote: Counting objects:  30% (2122/7072)[K
remote: Counting objects:  31% (2193/7072)[K
remote: Counting objects:  32% (2264/7072)[K
remote: Counting objects:  33% (2334/7072)[K
remote: Counting objects:  34% (2405/7072)[K
remote: Counting objects:  35% (2476/7072)[K
remote: Counting objects:  36% (2546/7072)[K
remote: Counting objects:  37% (2617/7072)[K
remote: Counting objects:  38% (2688/7072)[K
remote: Counting objects:  39% (2759/7072)[K
remote: Counting objects:  40% (2829/7072)[K
remote: Counting objects:  41% (2900/7072)[K
remote: Counting objects:  42% (2971/7072)[K
remote: Counting objects:  43% (3041/7072)[K
remote: Counting objects:  44% (3112/7072)[K
remote: Counting objects:  45% (3183/7072)[K
remote: Counting objects:  46% (3254/7072)[K
remote: Counting objects:  47% (3324/7072)[K
remote: Counting objects:  48% (3395/7072)[K
remote: Counting objects:  49% (3466/7072)[K
remote: Counting objects:  50% (3536/7072)[K
remote: Counting objects:  51% (3607/7072)[K
remote: Counting objects:  52% (3678/7072)[K
remote: Counting objects:  53% (3749/7072)[K
remote: Counting objects:  54% (3819/7072)[K
remote: Counting objects:  55% (3890/7072)[K
remote: Counting objects:  56% (3961/7072)[K
remote: Counting objects:  57% (4032/7072)[K
remote: Counting objects:  58% (4102/7072)[K
remote: Counting objects:  59% (4173/7072)[K
remote: Counting objects:  60% (4244/7072)[K
remote: Counting objects:  61% (4314/7072)[K
remote: Counting objects:  62% (4385/7072)[K
remote: Counting objects:  63% (4456/7072)[K
remote: Counting objects:  64% (4527/7072)[K
remote: Counting objects:  65% (4597/7072)[K
remote: Counting objects:  66% (4668/7072)[K
remote: Counting objects:  67% (4739/7072)[K
remote: Counting objects:  68% (4809/7072)[K
remote: Counting objects:  69% (4880/7072)[K
remote: Counting objects:  70% (4951/7072)[K
remote: Counting objects:  71% (5022/7072)[K
remote: Counting objects:  72% (5092/7072)[K
remote: Counting objects:  73% (5163/7072)[K
remote: Counting objects:  74% (5234/7072)[K
remote: Counting objects:  75% (5304/7072)[K
remote: Counting objects:  76% (5375/7072)[K
remote: Counting objects:  77% (5446/7072)[K
remote: Counting objects:  78% (5517/7072)[K
remote: Counting objects:  79% (5587/7072)[K
remote: Counting objects:  80% (5658/7072)[K
remote: Counting objects:  81% (5729/7072)[K
remote: Counting objects:  82% (5800/7072)[K
remote: Counting objects:  83% (5870/7072)[K
remote: Counting objects:  84% (5941/7072)[K
remote: Counting objects:  85% (6012/7072)[K
remote: Counting objects:  86% (6082/7072)[K
remote: Counting objects:  87% (6153/7072)[K
remote: Counting objects:  88% (6224/7072)[K
remote: Counting objects:  89% (6295/7072)[K
remote: Counting objects:  90% (6365/7072)[K
remote: Counting objects:  91% (6436/7072)[K
remote: Counting objects:  92% (6507/7072)[K
remote: Counting objects:  93% (6577/7072)[K
remote: Counting objects:  94% (6648/7072)[K
remote: Counting objects:  95% (6719/7072)[K
remote: Counting objects:  96% (6790/7072)[K
remote: Counting objects:  97% (6860/7072)[K
remote: Counting objects:  98% (6931/7072)[K
remote: Counting objects:  99% (7002/7072)[K
remote: Counting objects: 100% (7072/7072)[K
remote: Counting objects: 100% (7072/7072), done.[K
remote: Compressing objects:   0% (1/2992)[K
remote: Compressing objects:   1% (30/2992)[K
remote: Compressing objects:   2% (60/2992)[K
remote: Compressing objects:   3% (90/2992)[K
remote: Compressing objects:   4% (120/2992)[K
remote: Compressing objects:   5% (150/2992)[K
remote: Compressing objects:   6% (180/2992)[K
remote: Compressing objects:   7% (210/2992)[K
remote: Compressing objects:   8% (240/2992)[K
remote: Compressing objects:   9% (270/2992)[K
remote: Compressing objects:  10% (300/2992)[K
remote: Compressing objects:  11% (330/2992)[K
remote: Compressing objects:  12% (360/2992)[K
remote: Compressing objects:  13% (389/2992)[K
remote: Compressing objects:  14% (419/2992)[K
remote: Compressing objects:  15% (449/2992)[K
remote: Compressing objects:  16% (479/2992)[K
remote: Compressing objects:  17% (509/2992)[K
remote: Compressing objects:  18% (539/2992)[K
remote: Compressing objects:  19% (569/2992)[K
remote: Compressing objects:  20% (599/2992)[K
remote: Compressing objects:  21% (629/2992)[K
remote: Compressing objects:  22% (659/2992)[K
remote: Compressing objects:  23% (689/2992)[K
remote: Compressing objects:  24% (719/2992)[K
remote: Compressing objects:  25% (748/2992)[K
remote: Compressing objects:  26% (778/2992)[K
remote: Compressing objects:  27% (808/2992)[K
remote: Compressing objects:  28% (838/2992)[K
remote: Compressing objects:  29% (868/2992)[K
remote: Compressing objects:  30% (898/2992)[K
remote: Compressing objects:  31% (928/2992)[K
remote: Compressing objects:  32% (958/2992)[K
remote: Compressing objects:  33% (988/2992)[K
remote: Compressing objects:  34% (1018/2992)[K
remote: Compressing objects:  35% (1048/2992)[K
remote: Compressing objects:  36% (1078/2992)[K
remote: Compressing objects:  37% (1108/2992)[K
remote: Compressing objects:  38% (1137/2992)[K
remote: Compressing objects:  39% (1167/2992)[K
remote: Compressing objects:  40% (1197/2992)[K
remote: Compressing objects:  41% (1227/2992)[K
remote: Compressing objects:  42% (1257/2992)[K
remote: Compressing objects:  43% (1287/2992)[K
remote: Compressing objects:  44% (1317/2992)[K
remote: Compressing objects:  45% (1347/2992)[K
remote: Compressing objects:  46% (1377/2992)[K
remote: Compressing objects:  47% (1407/2992)[K
remote: Compressing objects:  48% (1437/2992)[K
remote: Compressing objects:  49% (1467/2992)[K
remote: Compressing objects:  50% (1496/2992)[K
remote: Compressing objects:  51% (1526/2992)[K
remote: Compressing objects:  52% (1556/2992)[K
remote: Compressing objects:  53% (1586/2992)[K
remote: Compressing objects:  54% (1616/2992)[K
remote: Compressing objects:  55% (1646/2992)[K
remote: Compressing objects:  56% (1676/2992)[K
remote: Compressing objects:  57% (1706/2992)[K
remote: Compressing objects:  58% (1736/2992)[K
remote: Compressing objects:  59% (1766/2992)[K
remote: Compressing objects:  60% (1796/2992)[K
remote: Compressing objects:  61% (1826/2992)[K
remote: Compressing objects:  62% (1856/2992)[K
remote: Compressing objects:  63% (1885/2992)[K
remote: Compressing objects:  64% (1915/2992)[K
remote: Compressing objects:  65% (1945/2992)[K
remote: Compressing objects:  66% (1975/2992)[K
remote: Compressing objects:  67% (2005/2992)[K
remote: Compressing objects:  68% (2035/2992)[K
remote: Compressing objects:  69% (2065/2992)[K
remote: Compressing objects:  70% (2095/2992)[K
remote: Compressing objects:  71% (2125/2992)[K
remote: Compressing objects:  72% (2155/2992)[K
remote: Compressing objects:  73% (2185/2992)[K
remote: Compressing objects:  74% (2215/2992)[K
remote: Compressing objects:  75% (2244/2992)[K
remote: Compressing objects:  76% (2274/2992)[K
remote: Compressing objects:  77% (2304/2992)[K
remote: Compressing objects:  78% (2334/2992)[K
remote: Compressing objects:  79% (2364/2992)[K
remote: Compressing objects:  80% (2394/2992)[K
remote: Compressing objects:  81% (2424/2992)[K
remote: Compressing objects:  82% (2454/2992)[K
remote: Compressing objects:  83% (2484/2992)[K
remote: Compressing objects:  84% (2514/2992)[K
remote: Compressing objects:  85% (2544/2992)[K
remote: Compressing objects:  86% (2574/2992)[K
remote: Compressing objects:  87% (2604/2992)[K
remote: Compressing objects:  88% (2633/2992)[K
remote: Compressing objects:  89% (2663/2992)[K
remote: Compressing objects:  90% (2693/2992)[K
remote: Compressing objects:  91% (2723/2992)[K
remote: Compressing objects:  92% (2753/2992)[K
remote: Compressing objects:  93% (2783/2992)[K
remote: Compressing objects:  94% (2813/2992)[K
remote: Compressing objects:  95% (2843/2992)[K
remote: Compressing objects:  96% (2873/2992)[K
remote: Compressing objects:  97% (2903/2992)[K
remote: Compressing objects:  98% (2933/2992)[K
remote: Compressing objects:  99% (2963/2992)[K
remote: Compressing objects: 100% (2992/2992)[K
remote: Compressing objects: 100% (2992/2992), done.[K
Receiving objects:   0% (1/7072)
Receiving objects:   1% (71/7072)
Receiving objects:   2% (142/7072)
Receiving objects:   3% (213/7072)
Receiving objects:   4% (283/7072)
Receiving objects:   5% (354/7072)
Receiving objects:   6% (425/7072)
Receiving objects:   7% (496/7072)
Receiving objects:   8% (566/7072)
Receiving objects:   9% (637/7072)
Receiving objects:  10% (708/7072)
Receiving objects:  11% (778/7072)
Receiving objects:  12% (849/7072)
Receiving objects:  13% (920/7072)
Receiving objects:  14% (991/7072)
Receiving objects:  15% (1061/7072)
Receiving objects:  16% (1132/7072)
Receiving objects:  17% (1203/7072)
Receiving objects:  18% (1273/7072)
Receiving objects:  19% (1344/7072)
Receiving objects:  20% (1415/7072)
Receiving objects:  21% (1486/7072)
Receiving objects:  22% (1556/7072)
Receiving objects:  23% (1627/7072)
Receiving objects:  24% (1698/7072)
Receiving objects:  25% (1768/7072)
Receiving objects:  26% (1839/7072)
Receiving objects:  27% (1910/7072)
Receiving objects:  28% (1981/7072)
Receiving objects:  29% (2051/7072)
Receiving objects:  30% (2122/7072)
Receiving objects:  31% (2193/7072)
Receiving objects:  32% (2264/7072)
Receiving objects:  33% (2334/7072)
Receiving objects:  34% (2405/7072)
Receiving objects:  35% (2476/7072)
Receiving objects:  36% (2546/7072)
Receiving objects:  37% (2617/7072)
Receiving objects:  38% (2688/7072)
Receiving objects:  39% (2759/7072)
Receiving objects:  40% (2829/7072)
Receiving objects:  41% (2900/7072)
Receiving objects:  42% (2971/7072)
Receiving objects:  43% (3041/7072)
Receiving objects:  44% (3112/7072)
Receiving objects:  45% (3183/7072)
Receiving objects:  46% (3254/7072)
Receiving objects:  47% (3324/7072)
Receiving objects:  48% (3395/7072)
Receiving objects:  49% (3466/7072)
Receiving objects:  50% (3536/7072)
Receiving objects:  51% (3607/7072)
Receiving objects:  52% (3678/7072)
Receiving objects:  53% (3749/7072)
Receiving objects:  54% (3819/7072)
Receiving objects:  55% (3890/7072)
Receiving objects:  56% (3961/7072)
Receiving objects:  57% (4032/7072)
Receiving objects:  58% (4102/7072)
Receiving objects:  59% (4173/7072)
Receiving objects:  60% (4244/7072)
Receiving objects:  61% (4314/7072)
Receiving objects:  62% (4385/7072)
Receiving objects:  63% (4456/7072)
Receiving objects:  64% (4527/7072)
Receiving objects:  65% (4597/7072)
Receiving objects:  66% (4668/7072)
Receiving objects:  67% (4739/7072)
Receiving objects:  68% (4809/7072)
Receiving objects:  69% (4880/7072)
Receiving objects:  70% (4951/7072)
Receiving objects:  71% (5022/7072)
Receiving objects:  72% (5092/7072)
Receiving objects:  73% (5163/7072)
Receiving objects:  74% (5234/7072)
Receiving objects:  75% (5304/7072)
Receiving objects:  76% (5375/7072)
Receiving objects:  77% (5446/7072)
Receiving objects:  78% (5517/7072)
Receiving objects:  79% (5587/7072)
Receiving objects:  80% (5658/7072)
Receiving objects:  81% (5729/7072)
Receiving objects:  82% (5800/7072)
Receiving objects:  83% (5870/7072)
Receiving objects:  84% (5941/7072)
Receiving objects:  85% (6012/7072)
Receiving objects:  86% (6082/7072)
Receiving objects:  87% (6153/7072)
Receiving objects:  88% (6224/7072)
Receiving objects:  89% (6295/7072)
Receiving objects:  90% (6365/7072)
Receiving objects:  91% (6436/7072)
Receiving objects:  92% (6507/7072)
Receiving objects:  93% (6577/7072)
Receiving objects:  94% (6648/7072)
Receiving objects:  95% (6719/7072)
Receiving objects:  96% (6790/7072)
remote: Total 7072 (delta 3008), reused 6573 (delta 2607), pack-reused 0 (from 0)[K
Receiving objects:  97% (6860/7072)
Receiving objects:  98% (6931/7072)
Receiving objects:  99% (7002/7072)
Receiving objects: 100% (7072/7072)
Receiving objects: 100% (7072/7072), 1.25 MiB | 41.29 MiB/s, done.
Resolving deltas:   0% (0/3008)
Resolving deltas:   1% (31/3008)
Resolving deltas:   2% (61/3008)
Resolving deltas:   3% (91/3008)
Resolving deltas:   4% (121/3008)
Resolving deltas:   5% (151/3008)
Resolving deltas:   6% (182/3008)
Resolving deltas:   7% (211/3008)
Resolving deltas:   8% (241/3008)
Resolving deltas:   9% (271/3008)
Resolving deltas:  10% (301/3008)
Resolving deltas:  11% (334/3008)
Resolving deltas:  12% (361/3008)
Resolving deltas:  13% (392/3008)
Resolving deltas:  14% (423/3008)
Resolving deltas:  15% (453/3008)
Resolving deltas:  16% (483/3008)
Resolving deltas:  17% (512/3008)
Resolving deltas:  18% (542/3008)
Resolving deltas:  19% (572/3008)
Resolving deltas:  20% (603/3008)
Resolving deltas:  21% (632/3008)
Resolving deltas:  22% (662/3008)
Resolving deltas:  23% (693/3008)
Resolving deltas:  24% (722/3008)
Resolving deltas:  25% (752/3008)
Resolving deltas:  26% (784/3008)
Resolving deltas:  27% (813/3008)
Resolving deltas:  28% (843/3008)
Resolving deltas:  29% (873/3008)
Resolving deltas:  30% (905/3008)
Resolving deltas:  31% (933/3008)
Resolving deltas:  32% (964/3008)
Resolving deltas:  33% (993/3008)
Resolving deltas:  34% (1024/3008)
Resolving deltas:  35% (1053/3008)
Resolving deltas:  36% (1083/3008)
Resolving deltas:  37% (1113/3008)
Resolving deltas:  38% (1144/3008)
Resolving deltas:  39% (1174/3008)
Resolving deltas:  40% (1204/3008)
Resolving deltas:  41% (1234/3008)
Resolving deltas:  42% (1265/3008)
Resolving deltas:  43% (1296/3008)
Resolving deltas:  44% (1324/3008)
Resolving deltas:  45% (1354/3008)
Resolving deltas:  46% (1384/3008)
Resolving deltas:  47% (1416/3008)
Resolving deltas:  48% (1444/3008)
Resolving deltas:  49% (1474/3008)
Resolving deltas:  50% (1506/3008)
Resolving deltas:  51% (1535/3008)
Resolving deltas:  52% (1565/3008)
Resolving deltas:  53% (1595/3008)
Resolving deltas:  54% (1625/3008)
Resolving deltas:  55% (1655/3008)
Resolving deltas:  56% (1688/3008)
Resolving deltas:  57% (1719/3008)
Resolving deltas:  58% (1745/3008)
Resolving deltas:  59% (1775/3008)
Resolving deltas:  60% (1808/3008)
Resolving deltas:  61% (1836/3008)
Resolving deltas:  62% (1865/3008)
Resolving deltas:  63% (1896/3008)
Resolving deltas:  64% (1926/3008)
Resolving deltas:  65% (1956/3008)
Resolving deltas:  66% (1986/3008)
Resolving deltas:  67% (2016/3008)
Resolving deltas:  68% (2047/3008)
Resolving deltas:  69% (2076/3008)
Resolving deltas:  70% (2107/3008)
Resolving deltas:  71% (2136/3008)
Resolving deltas:  72% (2166/3008)
Resolving deltas:  73% (2198/3008)
Resolving deltas:  74% (2227/3008)
Resolving deltas:  75% (2256/3008)
Resolving deltas:  76% (2287/3008)
Resolving deltas:  77% (2317/3008)
Resolving deltas:  78% (2347/3008)
Resolving deltas:  79% (2377/3008)
Resolving deltas:  80% (2408/3008)
Resolving deltas:  81% (2439/3008)
Resolving deltas:  82% (2467/3008)
Resolving deltas:  83% (2497/3008)
Resolving deltas:  84% (2527/3008)
Resolving deltas:  85% (2557/3008)
Resolving deltas:  86% (2588/3008)
Resolving deltas:  87% (2619/3008)
Resolving deltas:  88% (2648/3008)
Resolving deltas:  89% (2678/3008)
Resolving deltas:  90% (2708/3008)
Resolving deltas:  91% (2738/3008)
Resolving deltas:  92% (2768/3008)
Resolving deltas:  93% (2801/3008)
Resolving deltas:  94% (2829/3008)
Resolving deltas:  95% (2859/3008)
Resolving deltas:  96% (2889/3008)
Resolving deltas:  97% (2918/3008)
Resolving deltas:  98% (2948/3008)
Resolving deltas:  99% (2978/3008)
Resolving deltas: 100% (3008/3008)
Resolving deltas: 100% (3008/3008), done.
HEAD is now at 037731caad6c35a7aebe2150b728e8ad0e666f21
sem-version java 8
  1.8
  1.8.0.462
  11.0.28
  21.0.8
* 8 (set by /home/semaphore/.jenv/version)
  openjdk64-1.8.0.462
  openjdk64-11.0.28
  openjdk64-21.0.8

[2025-10-07T11:13:25+00:00]: Switch successful.
. cache-maven restore
cache_key is cache-maven-14.1.x
MISS: 'cache-maven-14.1.x'.
. sem-pint
ðŸ›‘ Skipping pint check, since it is a PR branch ðŸ›‘
mvn -Dcloud -Pjenkins -U -Ddependency.check.skip=true -Dmaven.wagon.http.retryHandler.count=10 --batch-mode --no-transfer-progress clean verify install dependency:analyze validate
[0m[0m[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Detecting the operating system and CPU architecture
[INFO] ------------------------------------------------------------------------
[INFO] os.detected.name: linux
[INFO] os.detected.arch: x86_64
[INFO] os.detected.version: 6.14
[INFO] os.detected.version.major: 6
[INFO] os.detected.version.minor: 14
[INFO] os.detected.release: ubuntu
[INFO] os.detected.release.version: 24.04
[INFO] os.detected.release.like.ubuntu: true
[INFO] os.detected.release.like.debian: true
[INFO] os.detected.classifier: linux-x86_64
[INFO] 
[INFO] --------------< io.confluent:kafka-connect-elasticsearch >--------------
[INFO] Building kafka-connect-elasticsearch 14.1.6-SNAPSHOT
[INFO]   from pom.xml
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- clean:3.1.0:clean (default-clean) @ kafka-connect-elasticsearch ---
[INFO] 
[INFO] --- buildnumber:1.4:create (default) @ kafka-connect-elasticsearch ---
[INFO] Executing: /bin/sh -c cd '/home/semaphore/kafka-connect-elasticsearch' && 'git' 'rev-parse' '--verify' 'HEAD'
[INFO] Working directory: /home/semaphore/kafka-connect-elasticsearch
[INFO] Storing buildNumber: 037731caad6c35a7aebe2150b728e8ad0e666f21 at timestamp: 1759835705346
[WARNING] Cannot get the branch information from the git repository: 
Detecting the current branch failed: fatal: ref HEAD is not a symbolic ref

[INFO] Executing: /bin/sh -c cd '/home/semaphore/kafka-connect-elasticsearch' && 'git' 'rev-parse' '--verify' 'HEAD'
[INFO] Working directory: /home/semaphore/kafka-connect-elasticsearch
[INFO] Storing buildScmBranch: UNKNOWN
[INFO] 
[INFO] --- checkstyle:3.1.1:check (validate) @ kafka-connect-elasticsearch ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- enforcer:3.0.0-M3:enforce (enforce-versions) @ kafka-connect-elasticsearch ---
[INFO] 
[INFO] --- jacoco:0.8.6:prepare-agent (pre-unit-test) @ kafka-connect-elasticsearch ---
[INFO] argLine set to -javaagent:/home/semaphore/.m2/repository/org/jacoco/org.jacoco.agent/0.8.6/org.jacoco.agent-0.8.6-runtime.jar=destfile=/home/semaphore/kafka-connect-elasticsearch/target/jacoco.exec
[INFO] 
[INFO] --- jacoco:0.8.6:prepare-agent (prepare-agent) @ kafka-connect-elasticsearch ---
[INFO] argLine set to -javaagent:/home/semaphore/.m2/repository/org/jacoco/org.jacoco.agent/0.8.6/org.jacoco.agent-0.8.6-runtime.jar=destfile=/home/semaphore/kafka-connect-elasticsearch/target/jacoco.exec
[INFO] 
[INFO] --- resources:3.1.0:resources (default-resources) @ kafka-connect-elasticsearch ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- compiler:3.8.1:compile (default-compile) @ kafka-connect-elasticsearch ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 17 source files to /home/semaphore/kafka-connect-elasticsearch/target/classes
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[27,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ExternalResourceExistenceChecker.java:[22,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[55,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[56,32] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[27,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ExternalResourceExistenceChecker.java:[22,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[55,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[56,32] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[27,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ExternalResourceExistenceChecker.java:[22,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[55,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[56,32] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ExternalResourceExistenceChecker.java:[47,20] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ExternalResourceExistenceChecker.java:[57,27] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ExternalResourceExistenceChecker.java:[71,27] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[166,12] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[452,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[492,39] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[550,35] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[626,11] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[628,16] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[652,5] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[115,17] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[194,5] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[194,47] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[239,10] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[INFO] 
[INFO] --- resources:3.1.0:testResources (default-testResources) @ kafka-connect-elasticsearch ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 14 resources
[INFO] 
[INFO] --- compiler:3.8.1:testCompile (default-testCompile) @ kafka-connect-elasticsearch ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 24 source files to /home/semaphore/kafka-connect-elasticsearch/target/test-classes
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[29,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[30,32] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/ValidatorTest.java:[70,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[29,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[30,32] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/ValidatorTest.java:[70,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[29,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[30,32] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/ValidatorTest.java:[70,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[66,11] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[73,23] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/ValidatorTest.java:[97,11] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/ValidatorTest.java:[104,23] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[INFO] 
[INFO] --- surefire:3.0.0-M4:test (default-test) @ kafka-connect-elasticsearch ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- jacoco:0.8.6:report (report) @ kafka-connect-elasticsearch ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- jar:3.2.0:jar (default-jar) @ kafka-connect-elasticsearch ---
[INFO] Building jar: /home/semaphore/kafka-connect-elasticsearch/target/kafka-connect-elasticsearch-14.1.6-SNAPSHOT.jar
[INFO] 
[INFO] --- kafka-connect:0.11.1:kafka-connect (default) @ kafka-connect-elasticsearch ---
[ERROR] sourceFile /home/semaphore/kafka-connect-elasticsearch/logos/elasticsearch.jpg does not exist
[INFO] Copied /home/semaphore/kafka-connect-elasticsearch/logos/confluent.png to /home/semaphore/kafka-connect-elasticsearch/target/components/assets/confluent.png
[INFO] Copied /home/semaphore/kafka-connect-elasticsearch/logos/confluent.png to /home/semaphore/kafka-connect-elasticsearch/target/components/assets/confluent.png
[INFO] Writing manifest file at: /home/semaphore/kafka-connect-elasticsearch/target/manifest.json
[INFO] Building jar: /home/semaphore/kafka-connect-elasticsearch/target/components/kafka-connect-elasticsearch-14.1.6-SNAPSHOT.jar
[INFO] Reading assembly descriptor: /home/semaphore/kafka-connect-elasticsearch/target/components/component-package.xml
[WARNING] The assembly descriptor contains a filesystem-root relative reference, which is not cross platform compatible /
[WARNING] The following patterns were never triggered in this artifact exclusion filter:
o  'io.confluent:kafka-connect-maven-plugin'
o  'org.apache.kafka:connect-api'
o  'org.apache.kafka:connect-file'
o  'org.apache.kafka:connect-json'
o  'org.apache.kafka:connect-runtime'
o  'org.apache.kafka:connect-transform'
o  'org.apache.kafka:kafka-clients'

[INFO] Copying files to /home/semaphore/kafka-connect-elasticsearch/target/components/packages/confluentinc-kafka-connect-elasticsearch-14.1.6-SNAPSHOT
[WARNING] The assembly descriptor contains a filesystem-root relative reference, which is not cross platform compatible /
[WARNING] The following patterns were never triggered in this artifact exclusion filter:
o  'io.confluent:kafka-connect-maven-plugin'
o  'org.apache.kafka:connect-api'
o  'org.apache.kafka:connect-file'
o  'org.apache.kafka:connect-json'
o  'org.apache.kafka:connect-runtime'
o  'org.apache.kafka:connect-transform'
o  'org.apache.kafka:kafka-clients'

[INFO] Building zip: /home/semaphore/kafka-connect-elasticsearch/target/components/packages/confluentinc-kafka-connect-elasticsearch-14.1.6-SNAPSHOT.zip
[INFO] 
[INFO] --- assembly:3.3.0:single (make-assembly) @ kafka-connect-elasticsearch ---
[INFO] Reading assembly descriptor: src/assembly/development.xml
[INFO] Reading assembly descriptor: src/assembly/package.xml
[WARNING] The following patterns were never triggered in this artifact exclusion filter:
o  'org.apache.kafka:connect-json'

[INFO] Copying files to /home/semaphore/kafka-connect-elasticsearch/target/kafka-connect-elasticsearch-14.1.6-SNAPSHOT-development
[WARNING] Assembly file: /home/semaphore/kafka-connect-elasticsearch/target/kafka-connect-elasticsearch-14.1.6-SNAPSHOT-development is not a regular file (it may be a directory). It cannot be attached to the project build for installation or deployment.
[WARNING] The following patterns were never triggered in this artifact exclusion filter:
o  'org.apache.kafka:connect-json'

[INFO] Copying files to /home/semaphore/kafka-connect-elasticsearch/target/kafka-connect-elasticsearch-14.1.6-SNAPSHOT-package
[WARNING] Assembly file: /home/semaphore/kafka-connect-elasticsearch/target/kafka-connect-elasticsearch-14.1.6-SNAPSHOT-package is not a regular file (it may be a directory). It cannot be attached to the project build for installation or deployment.
[INFO] 
[INFO] --- jar:3.2.0:test-jar (default) @ kafka-connect-elasticsearch ---
[INFO] Building jar: /home/semaphore/kafka-connect-elasticsearch/target/kafka-connect-elasticsearch-14.1.6-SNAPSHOT-tests.jar
[INFO] 
[INFO] --- cyclonedx:2.7.9:makeAggregateBom (default) @ kafka-connect-elasticsearch ---
[INFO] CycloneDX: Resolving Dependencies
[INFO] CycloneDX: Creating BOM version 1.4 with 297 component(s)
[INFO] CycloneDX: Writing and validating BOM (XML): /home/semaphore/kafka-connect-elasticsearch/target/bom.xml
[INFO]            attaching as kafka-connect-elasticsearch-14.1.6-SNAPSHOT-cyclonedx.xml
[INFO] CycloneDX: Writing and validating BOM (JSON): /home/semaphore/kafka-connect-elasticsearch/target/bom.json
[WARNING] Unknown keyword additionalItems - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword
[INFO]            attaching as kafka-connect-elasticsearch-14.1.6-SNAPSHOT-cyclonedx.json
[INFO] 
[INFO] --- jacoco:0.8.6:prepare-agent-integration (prepare-agent-it) @ kafka-connect-elasticsearch ---
[INFO] argLine set to -javaagent:/home/semaphore/.m2/repository/org/jacoco/org.jacoco.agent/0.8.6/org.jacoco.agent-0.8.6-runtime.jar=destfile=/home/semaphore/kafka-connect-elasticsearch/target/jacoco-it.exec
[INFO] 
[INFO] --- failsafe:3.0.0-M3:integration-test (integration-test) @ kafka-connect-elasticsearch ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/semaphore/.m2/repository/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/semaphore/.m2/repository/org/slf4j/slf4j-reload4j/1.7.36/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
[INFO] Running io.confluent.connect.elasticsearch.integration.ElasticsearchSinkTaskIT
[2025-10-07 11:16:48,001] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:48,072] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:44375/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = true
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = false
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:48,104] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:48,223] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:48,223] INFO Using unsecured connection to [http://localhost:44375]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:49,094] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:49,107] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:49,113] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:49,166] ERROR Can't convert record from topic=test partition=0 offset=1. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:294)
org.apache.kafka.connect.errors.DataException: Key is used as document id and can not be null.
	at io.confluent.connect.elasticsearch.DataConverter.convertKey(DataConverter.java:94)
	at io.confluent.connect.elasticsearch.DataConverter.convertRecord(DataConverter.java:165)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.tryWriteRecord(ElasticsearchSinkTask.java:289)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.put(ElasticsearchSinkTask.java:125)
	at io.confluent.connect.elasticsearch.integration.ElasticsearchSinkTaskIT.testConvertDataException(ElasticsearchSinkTaskIT.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at com.github.tomakehurst.wiremock.junit.WireMockRule$1.evaluate(WireMockRule.java:74)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[2025-10-07 11:16:49,211] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=3, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:49,272] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:49,273] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:44375/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = false
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:49,274] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:49,275] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:49,275] INFO Using unsecured connection to [http://localhost:44375]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:49,303] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:49,304] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:49,304] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:49,320] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=0, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:49,351] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:49,352] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:45995/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 1
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 5
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:49,352] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:49,353] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:49,353] INFO Using unsecured connection to [http://localhost:45995]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:49,374] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:49,374] DEBUG Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:49,374] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:49,383] DEBUG Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:49,503] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=2, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:49,522] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:49,523] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:38049/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 1
	max.buffered.records = 2
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 5
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:49,524] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:49,524] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:49,525] INFO Using unsecured connection to [http://localhost:38049]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:49,542] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:49,542] DEBUG Putting 100 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:49,543] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:50,557] WARN Failed to execute bulk request due to java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-7 [ACTIVE]. Retrying attempt (1/3) after backoff of 13 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:16:50,677] DEBUG Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:50,709] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:50,710] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 3
	behavior.on.malformed.documents = ignore
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:39659/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:50,711] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:50,711] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:50,711] INFO Using unsecured connection to [http://localhost:39659]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:50,724] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:50,725] DEBUG Putting 6 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:50,725] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:50,789] DEBUG Encountered an illegal document error. Ignoring and will not index record. Please check DLQ topic for errors. (io.confluent.connect.elasticsearch.ElasticsearchClient:648)
[2025-10-07 11:16:50,793] DEBUG Encountered an illegal document error. Ignoring and will not index record. Please check DLQ topic for errors. (io.confluent.connect.elasticsearch.ElasticsearchClient:648)
[2025-10-07 11:16:50,793] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=6, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:50,794] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:50,795] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 3
	behavior.on.malformed.documents = fail
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:39659/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:50,795] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:50,796] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:50,796] INFO Using unsecured connection to [http://localhost:39659]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:50,807] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:50,807] DEBUG Putting 6 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:50,808] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:50,813] ERROR Encountered an illegal document error. Please check DLQ topic for errors. To ignore future records like this, change the configuration 'behavior.on.malformed.documents' to 'IGNORE'. (io.confluent.connect.elasticsearch.ElasticsearchClient:655)
[2025-10-07 11:16:50,815] DEBUG Tried to flush data to Elasticsearch, but BulkProcessor is already closed. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:137)
java.lang.IllegalStateException: bulk process already closed
	at org.elasticsearch.action.bulk.BulkProcessor.ensureOpen(BulkProcessor.java:454)
	at org.elasticsearch.action.bulk.BulkProcessor.flush(BulkProcessor.java:571)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.flush(ElasticsearchClient.java:303)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.preCommit(ElasticsearchSinkTask.java:135)
	at io.confluent.connect.elasticsearch.integration.ElasticsearchSinkTaskIT.testIndividualFailure(ElasticsearchSinkTaskIT.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at com.github.tomakehurst.wiremock.junit.WireMockRule$1.evaluate(WireMockRule.java:74)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[2025-10-07 11:16:50,816] DEBUG preCommitting offsets {} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:50,829] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:50,830] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:40301/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 1
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 5
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:50,830] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:50,831] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:50,831] INFO Using unsecured connection to [http://localhost:40301]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:50,841] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:50,841] DEBUG Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:50,841] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:51,847] WARN Failed to execute bulk request due to java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-11 [ACTIVE]. Retrying attempt (1/3) after backoff of 16 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:16:51,847] WARN Failed to execute bulk request due to java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-12 [ACTIVE]. Retrying attempt (1/3) after backoff of 3 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:16:52,854] WARN Failed to execute bulk request due to java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-13 [ACTIVE]. Retrying attempt (2/3) after backoff of 18 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:16:52,866] WARN Failed to execute bulk request due to java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-14 [ACTIVE]. Retrying attempt (2/3) after backoff of 35 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:16:53,876] ERROR Failed to execute bulk request due to 'java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-15 [ACTIVE]' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-15 [ACTIVE]
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:917)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-15 [ACTIVE]
	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.timeout(HttpAsyncRequestExecutor.java:387)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:92)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:39)
	at org.apache.http.impl.nio.reactor.AbstractIODispatch.timeout(AbstractIODispatch.java:175)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.sessionTimedOut(BaseIOReactor.java:261)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.timeoutCheck(AbstractIOReactor.java:502)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.validate(BaseIOReactor.java:211)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:280)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591)
	... 1 more
[2025-10-07 11:16:53,877] WARN Bulk request 2 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-15 [ACTIVE]' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-15 [ACTIVE]
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:917)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
Caused by: java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-15 [ACTIVE]
	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.timeout(HttpAsyncRequestExecutor.java:387)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:92)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:39)
	at org.apache.http.impl.nio.reactor.AbstractIODispatch.timeout(AbstractIODispatch.java:175)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.sessionTimedOut(BaseIOReactor.java:261)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.timeoutCheck(AbstractIOReactor.java:502)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.validate(BaseIOReactor.java:211)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:280)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591)
	... 1 more
[2025-10-07 11:16:53,904] ERROR Failed to execute bulk request due to 'java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-16 [ACTIVE]' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-16 [ACTIVE]
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:917)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-16 [ACTIVE]
	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.timeout(HttpAsyncRequestExecutor.java:387)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:92)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:39)
	at org.apache.http.impl.nio.reactor.AbstractIODispatch.timeout(AbstractIODispatch.java:175)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.sessionTimedOut(BaseIOReactor.java:261)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.timeoutCheck(AbstractIOReactor.java:502)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.validate(BaseIOReactor.java:211)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:280)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591)
	... 1 more
[2025-10-07 11:16:53,905] WARN Bulk request 1 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-16 [ACTIVE]' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-16 [ACTIVE]
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:917)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
Caused by: java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-16 [ACTIVE]
	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.timeout(HttpAsyncRequestExecutor.java:387)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:92)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:39)
	at org.apache.http.impl.nio.reactor.AbstractIODispatch.timeout(AbstractIODispatch.java:175)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.sessionTimedOut(BaseIOReactor.java:261)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.timeoutCheck(AbstractIOReactor.java:502)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.validate(BaseIOReactor.java:211)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:280)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591)
	... 1 more
[2025-10-07 11:16:53,905] DEBUG preCommitting offsets {} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:54,419] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:54,419] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = ignore
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:34285/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:54,420] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:54,420] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:54,421] INFO Using unsecured connection to [http://localhost:34285]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:54,456] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:54,457] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:54,457] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:54,462] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=3, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:54,462] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:54,463] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = fail
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:34285/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:54,463] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:54,464] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:54,464] INFO Using unsecured connection to [http://localhost:34285]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:54,486] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:54,487] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:54,487] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:54,496] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=0, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:54,520] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:54,520] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:40191/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = true
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = false
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:54,521] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:54,521] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:54,521] INFO Using unsecured connection to [http://localhost:40191]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:54,533] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:54,533] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:54,536] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:54,539] ERROR Can't convert record from topic=test partition=0 offset=1. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:294)
org.apache.kafka.connect.errors.DataException: Key is used as document id and can not be null.
	at io.confluent.connect.elasticsearch.DataConverter.convertKey(DataConverter.java:94)
	at io.confluent.connect.elasticsearch.DataConverter.convertRecord(DataConverter.java:165)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.tryWriteRecord(ElasticsearchSinkTask.java:289)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.put(ElasticsearchSinkTask.java:125)
	at io.confluent.connect.elasticsearch.integration.ElasticsearchSinkTaskIT.testConvertDataException(ElasticsearchSinkTaskIT.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at com.github.tomakehurst.wiremock.junit.WireMockRule$1.evaluate(WireMockRule.java:74)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[2025-10-07 11:16:54,544] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=3, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:54,544] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:54,545] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:40191/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = false
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:54,545] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:54,546] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:54,546] INFO Using unsecured connection to [http://localhost:40191]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:54,555] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:54,555] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:54,555] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:54,561] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:54,576] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:54,577] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:43971/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 1
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 5
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:54,577] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:54,577] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:54,578] INFO Using unsecured connection to [http://localhost:43971]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:54,589] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:54,589] DEBUG Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:54,589] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:54,694] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:54,695] DEBUG Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:54,796] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=2, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:54,797] WARN Failed to execute bulk request due to org.apache.http.ConnectionClosedException: Connection is closed. Retrying attempt (1/3) after backoff of 18 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:16:54,817] WARN Failed to execute bulk request due to java.net.ConnectException: Connection refused. Retrying attempt (2/3) after backoff of 23 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:16:54,841] ERROR Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 11:16:54,842] WARN Bulk request 2 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 11:16:55,311] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:55,312] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:35937/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 1
	max.buffered.records = 2
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 5
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:55,312] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:55,313] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:55,313] INFO Using unsecured connection to [http://localhost:35937]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:55,328] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:55,329] DEBUG Putting 100 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:55,329] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:56,334] WARN Failed to execute bulk request due to java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-24 [ACTIVE]. Retrying attempt (1/3) after backoff of 6 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:16:56,354] DEBUG Pausing all partitions (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:363)
[2025-10-07 11:16:56,466] DEBUG Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:56,466] DEBUG Resuming all partitions (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:348)
[2025-10-07 11:16:56,486] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:56,486] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:46227/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 1
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 5
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:56,487] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:56,487] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:56,488] INFO Using unsecured connection to [http://localhost:46227]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:56,502] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:56,503] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:56,503] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:56,610] DEBUG preCommitting offsets {test-1=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}, test-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:56,712] DEBUG preCommitting offsets {test-1=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:56,712] WARN Failed to execute bulk request due to org.apache.http.ConnectionClosedException: Connection is closed. Retrying attempt (1/3) after backoff of 8 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:16:56,722] WARN Failed to execute bulk request due to java.net.ConnectException: Connection refused. Retrying attempt (2/3) after backoff of 30 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:16:56,753] ERROR Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 11:16:56,754] WARN Bulk request 2 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 11:16:57,233] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:57,233] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 3
	behavior.on.malformed.documents = ignore
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:33441/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:57,234] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:57,234] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:57,234] INFO Using unsecured connection to [http://localhost:33441]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:57,246] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:57,246] DEBUG Putting 6 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:57,246] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:57,250] DEBUG Encountered an illegal document error. Ignoring and will not index record. Please check DLQ topic for errors. (io.confluent.connect.elasticsearch.ElasticsearchClient:648)
[2025-10-07 11:16:57,253] DEBUG Encountered an illegal document error. Ignoring and will not index record. Please check DLQ topic for errors. (io.confluent.connect.elasticsearch.ElasticsearchClient:648)
[2025-10-07 11:16:57,254] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=6, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:57,254] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:57,254] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 3
	behavior.on.malformed.documents = fail
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:33441/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:57,254] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:57,255] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:57,255] INFO Using unsecured connection to [http://localhost:33441]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:57,266] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:57,266] DEBUG Putting 6 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:57,266] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:57,270] ERROR Encountered an illegal document error. Please check DLQ topic for errors. To ignore future records like this, change the configuration 'behavior.on.malformed.documents' to 'IGNORE'. (io.confluent.connect.elasticsearch.ElasticsearchClient:655)
[2025-10-07 11:16:57,273] DEBUG Tried to flush data to Elasticsearch, but BulkProcessor is already closed. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:137)
java.lang.IllegalStateException: bulk process already closed
	at org.elasticsearch.action.bulk.BulkProcessor.ensureOpen(BulkProcessor.java:454)
	at org.elasticsearch.action.bulk.BulkProcessor.flush(BulkProcessor.java:571)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.flush(ElasticsearchClient.java:303)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.preCommit(ElasticsearchSinkTask.java:135)
	at io.confluent.connect.elasticsearch.integration.ElasticsearchSinkTaskIT.testIndividualFailure(ElasticsearchSinkTaskIT.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at com.github.tomakehurst.wiremock.junit.WireMockRule$1.evaluate(WireMockRule.java:74)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[2025-10-07 11:16:57,274] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:57,287] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:57,287] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:36825/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 1
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 5
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:57,288] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:57,288] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:57,288] INFO Using unsecured connection to [http://localhost:36825]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:57,298] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:57,298] DEBUG Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:57,299] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:57,301] DEBUG preCommitting offsets {} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:57,302] WARN Failed to execute bulk request due to org.apache.http.ConnectionClosedException: Connection is closed. Retrying attempt (1/3) after backoff of 19 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:16:57,303] WARN Failed to execute bulk request due to java.net.ConnectException: Connection refused. Retrying attempt (1/3) after backoff of 9 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:16:57,314] WARN Failed to execute bulk request due to java.net.ConnectException: Connection refused. Retrying attempt (2/3) after backoff of 37 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:16:57,321] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:57,322] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = ignore
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:45371/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:57,322] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:57,322] WARN Failed to execute bulk request due to java.net.ConnectException: Connection refused. Retrying attempt (2/3) after backoff of 35 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:16:57,324] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:57,324] INFO Using unsecured connection to [http://localhost:45371]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:57,335] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:57,336] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:57,336] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:57,341] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=3, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:57,341] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:16:57,341] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = fail
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:45371/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:16:57,342] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:16:57,342] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:16:57,343] INFO Using unsecured connection to [http://localhost:45371]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:16:57,358] ERROR Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 11:16:57,359] WARN Bulk request 2 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 11:16:57,360] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:16:57,360] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:16:57,361] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:16:57,361] ERROR Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 11:16:57,362] WARN Bulk request 1 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 11:16:57,366] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:16:57,412] WARN Attempted to read Testcontainers configuration file at file:/home/semaphore/.testcontainers.properties but the file was not found. Exception message: FileNotFoundException: /home/semaphore/.testcontainers.properties (No such file or directory) (org.testcontainers.utility.TestcontainersConfiguration:338)
[2025-10-07 11:16:57,417] INFO Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor') (org.testcontainers.utility.ImageNameSubstitutor:50)
[2025-10-07 11:16:58,204] INFO Found Docker environment with local Unix socket (unix:///var/run/docker.sock) (org.testcontainers.dockerclient.DockerClientProviderStrategy:177)
[2025-10-07 11:16:58,208] INFO Docker host IP address is localhost (org.testcontainers.DockerClientFactory:190)
[2025-10-07 11:16:58,242] INFO Connected to docker: 
  Server Version: 28.1.1
  API Version: 1.49
  Operating System: Ubuntu 24.04.3 LTS
  Total Memory: 62976 MB (org.testcontainers.DockerClientFactory:206)
[2025-10-07 11:16:59,580] INFO Starting to pull image (org.testcontainers.DockerClientFactory:38)
[2025-10-07 11:16:59,601] INFO Pulling image layers:  0 pending,  0 downloaded,  0 extracted, (0 bytes/0 bytes) (org.testcontainers.DockerClientFactory:85)
[2025-10-07 11:16:59,917] INFO Pulling image layers:  2 pending,  1 downloaded,  0 extracted, (16 KB/? MB) (org.testcontainers.DockerClientFactory:85)
[2025-10-07 11:16:59,976] INFO Pulling image layers:  1 pending,  2 downloaded,  0 extracted, (48 KB/? MB) (org.testcontainers.DockerClientFactory:85)
[2025-10-07 11:17:00,002] INFO Pulling image layers:  0 pending,  3 downloaded,  0 extracted, (2 MB/5 MB) (org.testcontainers.DockerClientFactory:85)
[2025-10-07 11:17:00,112] INFO Pulling image layers:  0 pending,  3 downloaded,  1 extracted, (4 MB/5 MB) (org.testcontainers.DockerClientFactory:85)
[2025-10-07 11:17:00,194] INFO Pulling image layers:  0 pending,  3 downloaded,  2 extracted, (5 MB/5 MB) (org.testcontainers.DockerClientFactory:85)
[2025-10-07 11:17:00,230] INFO Pulling image layers:  0 pending,  3 downloaded,  3 extracted, (5 MB/5 MB) (org.testcontainers.DockerClientFactory:85)
[2025-10-07 11:17:00,964] INFO Ryuk started - will monitor and terminate Testcontainers containers on JVM exit (org.testcontainers.DockerClientFactory:224)
[2025-10-07 11:17:00,964] INFO Checking the system... (org.testcontainers.DockerClientFactory:235)
[2025-10-07 11:17:00,965] INFO âœ”ï¸Ž Docker server version should be at least 1.6.0 (org.testcontainers.DockerClientFactory:309)
[2025-10-07 11:17:01,025] INFO âœ”ï¸Ž Docker environment should have more than 2GB free disk space (org.testcontainers.DockerClientFactory:309)
[2025-10-07 11:17:01,036] INFO Pulling docker image: docker.elastic.co/elasticsearch/elasticsearch:8.2.2. Please be patient; this may take some time but only needs to be done once. (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:72)
[2025-10-07 11:17:01,939] INFO Starting to pull image (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:38)
[2025-10-07 11:17:01,940] INFO Pulling image layers:  0 pending,  0 downloaded,  0 extracted, (0 bytes/0 bytes) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:02,493] INFO Pulling image layers:  8 pending,  1 downloaded,  0 extracted, (4 KB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:02,584] INFO Pulling image layers:  7 pending,  2 downloaded,  0 extracted, (524 KB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:02,615] INFO Pulling image layers:  6 pending,  3 downloaded,  0 extracted, (524 KB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:02,821] INFO Pulling image layers:  5 pending,  4 downloaded,  0 extracted, (7 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:02,890] INFO Pulling image layers:  4 pending,  5 downloaded,  0 extracted, (27 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:03,081] INFO Pulling image layers:  3 pending,  6 downloaded,  0 extracted, (90 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:03,146] INFO Pulling image layers:  2 pending,  7 downloaded,  0 extracted, (97 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:03,331] INFO Pulling image layers:  1 pending,  8 downloaded,  0 extracted, (178 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:03,487] INFO Pulling image layers:  1 pending,  8 downloaded,  1 extracted, (256 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:03,911] INFO Pulling image layers:  1 pending,  8 downloaded,  2 extracted, (430 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:03,935] INFO Pulling image layers:  1 pending,  8 downloaded,  3 extracted, (430 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:04,305] INFO Pulling image layers:  0 pending,  9 downloaded,  3 extracted, (587 MB/607 MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:08,399] INFO Pulling image layers:  0 pending,  9 downloaded,  4 extracted, (607 MB/607 MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:08,415] INFO Pulling image layers:  0 pending,  9 downloaded,  5 extracted, (607 MB/607 MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:08,431] INFO Pulling image layers:  0 pending,  9 downloaded,  6 extracted, (607 MB/607 MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:08,454] INFO Pulling image layers:  0 pending,  9 downloaded,  7 extracted, (607 MB/607 MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:08,472] INFO Pulling image layers:  0 pending,  9 downloaded,  8 extracted, (607 MB/607 MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:08,489] INFO Pulling image layers:  0 pending,  9 downloaded,  9 extracted, (607 MB/607 MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 11:17:08,503] INFO Pull complete. 9 layers, pulled in 6s (downloaded 607 MB at 101 MB/s) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:106)
[2025-10-07 11:17:08,513] INFO Starting an elasticsearch container using [docker.elastic.co/elasticsearch/elasticsearch:8.2.2] (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:73)
[2025-10-07 11:17:08,536] INFO Setting up basic authentication in a Docker image (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:328)
[2025-10-07 11:17:08,565] INFO Transferred 545 bytes to Docker daemon (org.testcontainers.images.builder.ImageFromDockerfile:137)
[2025-10-07 11:17:17,175] INFO Creating container for image: localhost/testcontainers/xeuirdejdc8jreej:latest (ðŸ³ [localhost/testcontainers/xeuirdejdc8jreej:latest]:363)
[2025-10-07 11:17:17,224] INFO Container localhost/testcontainers/xeuirdejdc8jreej:latest is starting: bb4814347a586488bb5a7428fd60f4933adbd734fb6dd6d3826ccfcdf23f4690 (ðŸ³ [localhost/testcontainers/xeuirdejdc8jreej:latest]:424)
2025-10-07 11:17:17,776 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

Exception in thread "main" org.elasticsearch.common.settings.SettingsException: Failed to load settings from [elasticsearch.yml]
	at org.elasticsearch.common.settings.Settings$Builder.loadFromStream(Settings.java:1193)
	at org.elasticsearch.node.InternalSettingsPreparer.loadConfigWithSubstitutions(InternalSettingsPreparer.java:140)
	at org.elasticsearch.node.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:54)
	at org.elasticsearch.common.cli.EnvironmentAwareCommand.createEnv(EnvironmentAwareCommand.java:95)
	at org.elasticsearch.common.cli.EnvironmentAwareCommand.createEnv(EnvironmentAwareCommand.java:86)
	at org.elasticsearch.common.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:81)
	at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:112)
	at org.elasticsearch.cli.MultiCommand.execute(MultiCommand.java:95)
	at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:112)
	at org.elasticsearch.cli.Command.main(Command.java:77)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)
Caused by: org.elasticsearch.common.ParsingException: Failed to parse object: expecting token of type [START_OBJECT] but found [START_ARRAY]
	at org.elasticsearch.common.xcontent.XContentParserUtils.parsingException(XContentParserUtils.java:77)
	at org.elasticsearch.common.xcontent.XContentParserUtils.ensureExpectedToken(XContentParserUtils.java:70)
	at org.elasticsearch.common.settings.Settings.fromXContent(Settings.java:684)
	at org.elasticsearch.common.settings.Settings$Builder.loadFromStream(Settings.java:1189)
	... 10 more
[2025-10-07 11:22:17,477] ERROR Could not start container (ðŸ³ [localhost/testcontainers/xeuirdejdc8jreej:latest]:512)
java.lang.IllegalStateException: Container exited with code 1
	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:497)
	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:331)
	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:329)
	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:317)
	at io.confluent.connect.elasticsearch.helper.ElasticsearchContainer.start(ElasticsearchContainer.java:165)
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT.setupBeforeAll(ElasticsearchConnectorIT.java:80)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[2025-10-07 11:22:17,508] ERROR Log output from the failed container:
2025-10-07 11:17:17,776 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

Exception in thread "main" org.elasticsearch.common.settings.SettingsException: Failed to load settings from [elasticsearch.yml]
	at org.elasticsearch.common.settings.Settings$Builder.loadFromStream(Settings.java:1193)
	at org.elasticsearch.node.InternalSettingsPreparer.loadConfigWithSubstitutions(InternalSettingsPreparer.java:140)
	at org.elasticsearch.node.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:54)
	at org.elasticsearch.common.cli.EnvironmentAwareCommand.createEnv(EnvironmentAwareCommand.java:95)
	at org.elasticsearch.common.cli.EnvironmentAwareCommand.createEnv(EnvironmentAwareCommand.java:86)
	at org.elasticsearch.common.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:81)
	at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:112)
	at org.elasticsearch.cli.MultiCommand.execute(MultiCommand.java:95)
	at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:112)
	at org.elasticsearch.cli.Command.main(Command.java:77)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)
Caused by: org.elasticsearch.common.ParsingException: Failed to parse object: expecting token of type [START_OBJECT] but found [START_ARRAY]
	at org.elasticsearch.common.xcontent.XContentParserUtils.parsingException(XContentParserUtils.java:77)
	at org.elasticsearch.common.xcontent.XContentParserUtils.ensureExpectedToken(XContentParserUtils.java:70)
	at org.elasticsearch.common.settings.Settings.fromXContent(Settings.java:684)
	at org.elasticsearch.common.settings.Settings$Builder.loadFromStream(Settings.java:1189)
	... 10 more
 (ðŸ³ [localhost/testcontainers/xeuirdejdc8jreej:latest]:519)
[WARNING] Tests run: 14, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 331.754 s - in io.confluent.connect.elasticsearch.integration.ElasticsearchSinkTaskIT
[INFO] Running io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT
[2025-10-07 11:22:17,538] INFO Configuration: (org.apache.hadoop.minikdc.MiniKdc:225)
[2025-10-07 11:22:17,538] INFO --------------------------------------------------------------- (org.apache.hadoop.minikdc.MiniKdc:226)
[2025-10-07 11:22:17,538] INFO   debug: false (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:22:17,539] INFO   transport: TCP (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:22:17,539] INFO   max.ticket.lifetime: 86400000 (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:22:17,539] INFO   org.name: EXAMPLE (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:22:17,539] INFO   kdc.port: 0 (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:22:17,539] INFO   org.domain: COM (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:22:17,539] INFO   max.renewable.lifetime: 604800000 (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:22:17,539] INFO   instance: DefaultKrbServer (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:22:17,539] INFO   kdc.bind.address: localhost (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:22:17,539] INFO --------------------------------------------------------------- (org.apache.hadoop.minikdc.MiniKdc:230)
[2025-10-07 11:22:17,627] INFO MiniKdc started. (org.apache.hadoop.minikdc.MiniKdc:285)
[2025-10-07 11:22:17,637] INFO Starting an elasticsearch container using [docker.elastic.co/elasticsearch/elasticsearch:8.2.2] (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:73)
[2025-10-07 11:22:17,638] INFO Building Elasticsearch image with Kerberos configuration. (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:396)
[2025-10-07 11:22:17,638] INFO Creating Kerberized Elasticsearch image. (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:332)
[2025-10-07 11:22:17,641] INFO Transferred 1 KB to Docker daemon (org.testcontainers.images.builder.ImageFromDockerfile:137)
[2025-10-07 11:22:18,050] INFO Creating container for image: localhost/testcontainers/uerq7rnrhvyhpd5f:latest (ðŸ³ [localhost/testcontainers/uerq7rnrhvyhpd5f:latest]:363)
[2025-10-07 11:22:18,093] INFO Container localhost/testcontainers/uerq7rnrhvyhpd5f:latest is starting: 5b669e34b4533a2044a97cb05bb87a0fe8d2f7c561ced445ba8ff2147b56dcaa (ðŸ³ [localhost/testcontainers/uerq7rnrhvyhpd5f:latest]:424)
2025-10-07 11:22:18,803 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

2025-10-07 11:22:19,370 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.AutoConfigureNode.main(AutoConfigureNode.java:157)

Exception in thread "main" java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/sun.management.spi.PlatformMBeanProvider$PlatformComponent.getMBeans(PlatformMBeanProvider.java:195)
	at java.management/java.lang.management.ManagementFactory.getPlatformMXBean(ManagementFactory.java:689)
	at java.management/java.lang.management.ManagementFactory.getOperatingSystemMXBean(ManagementFactory.java:389)
	at org.elasticsearch.tools.launchers.DefaultSystemMemoryInfo.<init>(DefaultSystemMemoryInfo.java:29)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.jvmOptions(JvmOptionsParser.java:132)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:87)
[2025-10-07 11:27:18,335] ERROR Could not start container (ðŸ³ [localhost/testcontainers/uerq7rnrhvyhpd5f:latest]:512)
java.lang.IllegalStateException: Container exited with code 1
	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:497)
	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:331)
	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:329)
	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:317)
	at io.confluent.connect.elasticsearch.helper.ElasticsearchContainer.start(ElasticsearchContainer.java:165)
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT.setupBeforeAll(ElasticsearchConnectorKerberosIT.java:37)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[2025-10-07 11:27:18,362] ERROR Log output from the failed container:
2025-10-07 11:22:18,803 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

2025-10-07 11:22:19,370 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.AutoConfigureNode.main(AutoConfigureNode.java:157)

Exception in thread "main" java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/sun.management.spi.PlatformMBeanProvider$PlatformComponent.getMBeans(PlatformMBeanProvider.java:195)
	at java.management/java.lang.management.ManagementFactory.getPlatformMXBean(ManagementFactory.java:689)
	at java.management/java.lang.management.ManagementFactory.getOperatingSystemMXBean(ManagementFactory.java:389)
	at org.elasticsearch.tools.launchers.DefaultSystemMemoryInfo.<init>(DefaultSystemMemoryInfo.java:29)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.jvmOptions(JvmOptionsParser.java:132)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:87)
 (ðŸ³ [localhost/testcontainers/uerq7rnrhvyhpd5f:latest]:519)
[2025-10-07 11:27:18,381] INFO Default Internal kdc server stopped. (org.apache.kerby.kerberos.kerb.server.impl.DefaultInternalKdcServerImpl:102)
[2025-10-07 11:27:19,381] INFO MiniKdc stopped. (org.apache.hadoop.minikdc.MiniKdc:359)
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 301.841 s <<< FAILURE! - in io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT
[ERROR] io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT  Time elapsed: 0 s  <<< ERROR!
org.testcontainers.containers.ContainerLaunchException: Container startup failed
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT.setupBeforeAll(ElasticsearchConnectorIT.java:80)
Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT.setupBeforeAll(ElasticsearchConnectorIT.java:80)
Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT.setupBeforeAll(ElasticsearchConnectorIT.java:80)
Caused by: java.lang.IllegalStateException: Container exited with code 1
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT.setupBeforeAll(ElasticsearchConnectorIT.java:80)

[INFO] Running io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT
[2025-10-07 11:27:19,387] INFO Configuration: (org.apache.hadoop.minikdc.MiniKdc:225)
[2025-10-07 11:27:19,387] INFO --------------------------------------------------------------- (org.apache.hadoop.minikdc.MiniKdc:226)
[2025-10-07 11:27:19,387] INFO   debug: false (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:27:19,387] INFO   transport: TCP (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:27:19,387] INFO   max.ticket.lifetime: 86400000 (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:27:19,387] INFO   org.name: EXAMPLE (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:27:19,387] INFO   kdc.port: 0 (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:27:19,387] INFO   org.domain: COM (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:27:19,387] INFO   max.renewable.lifetime: 604800000 (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:27:19,388] INFO   instance: DefaultKrbServer (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:27:19,388] INFO   kdc.bind.address: localhost (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 11:27:19,388] INFO --------------------------------------------------------------- (org.apache.hadoop.minikdc.MiniKdc:230)
[2025-10-07 11:27:19,390] INFO MiniKdc started. (org.apache.hadoop.minikdc.MiniKdc:285)
[2025-10-07 11:27:19,393] INFO Starting an elasticsearch container using [docker.elastic.co/elasticsearch/elasticsearch:8.2.2] (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:73)
[2025-10-07 11:27:19,395] INFO Building Elasticsearch image with SSL configuration (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:377)
[2025-10-07 11:27:19,395] INFO Building Elasticsearch image with Kerberos configuration. (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:396)
[2025-10-07 11:27:19,395] INFO Creating Kerberized Elasticsearch image. (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:332)
[2025-10-07 11:27:19,395] INFO Extending Docker image to generate certs and enable SSL (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:336)
[2025-10-07 11:27:19,398] INFO Including test machine address 10.113.99.94 in Elasticsearch certs (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:418)
[2025-10-07 11:27:19,404] INFO Transferred 6 KB to Docker daemon (org.testcontainers.images.builder.ImageFromDockerfile:137)
[2025-10-07 11:27:38,856] INFO Creating container for image: localhost/testcontainers/d5xalcvoz5guouds:latest (ðŸ³ [localhost/testcontainers/d5xalcvoz5guouds:latest]:363)
[2025-10-07 11:27:38,881] INFO Container localhost/testcontainers/d5xalcvoz5guouds:latest is starting: 659851959ff16ac558a18ee72e2c878c559f807290d6e489aa1c23a7875f1eb0 (ðŸ³ [localhost/testcontainers/d5xalcvoz5guouds:latest]:424)

Replacing the ip address in the /usr/share/elasticsearch/config/ssl/instances.yml file with 10.113.99.94
Setting up Elasticsearch and generating certificates in /usr/share/elasticsearch
=== CREATE Keystore ===
Elastic password is: elastic
2025-10-07 11:27:39,423 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

Created elasticsearch keystore in /usr/share/elasticsearch/config/elasticsearch.keystore
Setting bootstrap.password...
2025-10-07 11:27:40,080 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

=== CREATE SSL CERTS ===
Creating cluster-ca.zip... (warnings are benign)
2025-10-07 11:27:41,006 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.CertificateTool.main(CertificateTool.java:149)

Unzip ca files...
Archive:  /usr/share/elasticsearch/config/ssl/cluster-ca.zip
   creating: /usr/share/elasticsearch/config/ssl/ca/
  inflating: /usr/share/elasticsearch/config/ssl/ca/ca.crt  
  inflating: /usr/share/elasticsearch/config/ssl/ca/ca.key  
Create cluster certs zipfile... (warnings are benign)
2025-10-07 11:27:42,094 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.CertificateTool.main(CertificateTool.java:149)

Unzipping cluster certs zipfile...
Archive:  /usr/share/elasticsearch/config/ssl/cluster.zip
   creating: /usr/share/elasticsearch/config/ssl/cluster/elasticsearch/
  inflating: /usr/share/elasticsearch/config/ssl/cluster/elasticsearch/elasticsearch.crt  
  inflating: /usr/share/elasticsearch/config/ssl/cluster/elasticsearch/elasticsearch.key  
   creating: /usr/share/elasticsearch/config/ssl/cluster/kibana/
  inflating: /usr/share/elasticsearch/config/ssl/cluster/kibana/kibana.crt  
  inflating: /usr/share/elasticsearch/config/ssl/cluster/kibana/kibana.key  
   creating: /usr/share/elasticsearch/config/ssl/cluster/logstash/
  inflating: /usr/share/elasticsearch/config/ssl/cluster/logstash/logstash.crt  
  inflating: /usr/share/elasticsearch/config/ssl/cluster/logstash/logstash.key  
Move elasticsearch certs to SSL config dir...
Generating truststore at /usr/share/elasticsearch/config/ssl/truststore.jks
Certificate was added to keystore
Generating keystore for client at /usr/share/elasticsearch/config/ssl/keystore.jks
Importing keystore /usr/share/elasticsearch/config/ssl/client.p12 to /usr/share/elasticsearch/config/ssl/keystore.jks...
Entry for alias clientkey successfully imported.
Import command completed:  1 entries successfully imported, 0 entries failed or cancelled

Warning:
The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format using "keytool -importkeystore -srckeystore /usr/share/elasticsearch/config/ssl/keystore.jks -destkeystore /usr/share/elasticsearch/config/ssl/keystore.jks -deststoretype pkcs12".
Elasticsearch Configuration
## Used by Docker images in our integration test
http.host: 0.0.0.0
network.host: 0.0.0.0
transport.host: 0.0.0.0

node.store.allow_mmap: false
cluster.routing.allocation.disk.threshold_enabled: false
discovery.type: single-node

xpack.license.self_generated.type: trial
xpack.security.enabled: true
xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.client_authentication: optional
xpack.security.http.ssl.verification_mode: certificate
xpack.security.http.ssl.key:  ssl/elasticsearch.key
xpack.security.http.ssl.certificate: ssl/elasticsearch.crt
xpack.security.http.ssl.certificate_authorities: [ "ssl/ca/ca.crt" ]

xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.key:  ssl/elasticsearch.key
xpack.security.transport.ssl.certificate: ssl/elasticsearch.crt
xpack.security.transport.ssl.certificate_authorities: [ "ssl/ca/ca.crt" ]

# enable anonymous connections since setting passwords requires running a command
xpack.security.authc:
  anonymous:
    username: connect_user
    roles: superuser
    authz_exception: true

# Kerberos realm
xpack.security.authc.realms.kerberos.kerb1:
  order: 3
  keytab.path: es.keytab
  remove_realm_name: false
Starting Elasticsearch with SSL and Kerberos enabled ...
2025-10-07 11:27:43,646 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

2025-10-07 11:27:44,210 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.AutoConfigureNode.main(AutoConfigureNode.java:157)

Exception in thread "main" java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/sun.management.spi.PlatformMBeanProvider$PlatformComponent.getMBeans(PlatformMBeanProvider.java:195)
	at java.management/java.lang.management.ManagementFactory.getPlatformMXBean(ManagementFactory.java:689)
	at java.management/java.lang.management.ManagementFactory.getOperatingSystemMXBean(ManagementFactory.java:389)
	at org.elasticsearch.tools.launchers.DefaultSystemMemoryInfo.<init>(DefaultSystemMemoryInfo.java:29)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.jvmOptions(JvmOptionsParser.java:132)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:87)
[2025-10-07 11:32:39,100] ERROR Could not start container (ðŸ³ [localhost/testcontainers/d5xalcvoz5guouds:latest]:512)
java.lang.IllegalStateException: Container exited with code 1
	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:497)
	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:331)
	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:329)
	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:317)
	at io.confluent.connect.elasticsearch.helper.ElasticsearchContainer.start(ElasticsearchContainer.java:165)
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT.setupBeforeAll(ElasticsearchConnectorKerberosWithSslIT.java:27)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[2025-10-07 11:32:39,126] ERROR Log output from the failed container:

Replacing the ip address in the /usr/share/elasticsearch/config/ssl/instances.yml file with 10.113.99.94
Setting up Elasticsearch and generating certificates in /usr/share/elasticsearch
=== CREATE Keystore ===
Elastic password is: elastic
2025-10-07 11:27:39,423 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

Created elasticsearch keystore in /usr/share/elasticsearch/config/elasticsearch.keystore
Setting bootstrap.password...
2025-10-07 11:27:40,080 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

=== CREATE SSL CERTS ===
Creating cluster-ca.zip... (warnings are benign)
2025-10-07 11:27:41,006 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.CertificateTool.main(CertificateTool.java:149)

Unzip ca files...
Archive:  /usr/share/elasticsearch/config/ssl/cluster-ca.zip
   creating: /usr/share/elasticsearch/config/ssl/ca/
  inflating: /usr/share/elasticsearch/config/ssl/ca/ca.crt  
  inflating: /usr/share/elasticsearch/config/ssl/ca/ca.key  
Create cluster certs zipfile... (warnings are benign)
2025-10-07 11:27:42,094 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.CertificateTool.main(CertificateTool.java:149)

Unzipping cluster certs zipfile...
Archive:  /usr/share/elasticsearch/config/ssl/cluster.zip
   creating: /usr/share/elasticsearch/config/ssl/cluster/elasticsearch/
  inflating: /usr/share/elasticsearch/config/ssl/cluster/elasticsearch/elasticsearch.crt  
  inflating: /usr/share/elasticsearch/config/ssl/cluster/elasticsearch/elasticsearch.key  
   creating: /usr/share/elasticsearch/config/ssl/cluster/kibana/
  inflating: /usr/share/elasticsearch/config/ssl/cluster/kibana/kibana.crt  
  inflating: /usr/share/elasticsearch/config/ssl/cluster/kibana/kibana.key  
   creating: /usr/share/elasticsearch/config/ssl/cluster/logstash/
  inflating: /usr/share/elasticsearch/config/ssl/cluster/logstash/logstash.crt  
  inflating: /usr/share/elasticsearch/config/ssl/cluster/logstash/logstash.key  
Move elasticsearch certs to SSL config dir...
Generating truststore at /usr/share/elasticsearch/config/ssl/truststore.jks
Certificate was added to keystore
Generating keystore for client at /usr/share/elasticsearch/config/ssl/keystore.jks
Importing keystore /usr/share/elasticsearch/config/ssl/client.p12 to /usr/share/elasticsearch/config/ssl/keystore.jks...
Entry for alias clientkey successfully imported.
Import command completed:  1 entries successfully imported, 0 entries failed or cancelled

Warning:
The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format using "keytool -importkeystore -srckeystore /usr/share/elasticsearch/config/ssl/keystore.jks -destkeystore /usr/share/elasticsearch/config/ssl/keystore.jks -deststoretype pkcs12".
Elasticsearch Configuration
## Used by Docker images in our integration test
http.host: 0.0.0.0
network.host: 0.0.0.0
transport.host: 0.0.0.0

node.store.allow_mmap: false
cluster.routing.allocation.disk.threshold_enabled: false
discovery.type: single-node

xpack.license.self_generated.type: trial
xpack.security.enabled: true
xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.client_authentication: optional
xpack.security.http.ssl.verification_mode: certificate
xpack.security.http.ssl.key:  ssl/elasticsearch.key
xpack.security.http.ssl.certificate: ssl/elasticsearch.crt
xpack.security.http.ssl.certificate_authorities: [ "ssl/ca/ca.crt" ]

xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.key:  ssl/elasticsearch.key
xpack.security.transport.ssl.certificate: ssl/elasticsearch.crt
xpack.security.transport.ssl.certificate_authorities: [ "ssl/ca/ca.crt" ]

# enable anonymous connections since setting passwords requires running a command
xpack.security.authc:
  anonymous:
    username: connect_user
    roles: superuser
    authz_exception: true

# Kerberos realm
xpack.security.authc.realms.kerberos.kerb1:
  order: 3
  keytab.path: es.keytab
  remove_realm_name: false
Starting Elasticsearch with SSL and Kerberos enabled ...
2025-10-07 11:27:43,646 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

2025-10-07 11:27:44,210 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.AutoConfigureNode.main(AutoConfigureNode.java:157)

Exception in thread "main" java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/sun.management.spi.PlatformMBeanProvider$PlatformComponent.getMBeans(PlatformMBeanProvider.java:195)
	at java.management/java.lang.management.ManagementFactory.getPlatformMXBean(ManagementFactory.java:689)
	at java.management/java.lang.management.ManagementFactory.getOperatingSystemMXBean(ManagementFactory.java:389)
	at org.elasticsearch.tools.launchers.DefaultSystemMemoryInfo.<init>(DefaultSystemMemoryInfo.java:29)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.jvmOptions(JvmOptionsParser.java:132)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:87)
 (ðŸ³ [localhost/testcontainers/d5xalcvoz5guouds:latest]:519)
[2025-10-07 11:32:39,142] INFO Default Internal kdc server stopped. (org.apache.kerby.kerberos.kerb.server.impl.DefaultInternalKdcServerImpl:102)
[2025-10-07 11:32:40,142] INFO MiniKdc stopped. (org.apache.hadoop.minikdc.MiniKdc:359)
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 320.751 s <<< FAILURE! - in io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT
[ERROR] io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT  Time elapsed: 0 s  <<< ERROR!
org.testcontainers.containers.ContainerLaunchException: Container startup failed
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT.setupBeforeAll(ElasticsearchConnectorKerberosIT.java:37)
Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT.setupBeforeAll(ElasticsearchConnectorKerberosIT.java:37)
Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT.setupBeforeAll(ElasticsearchConnectorKerberosIT.java:37)
Caused by: java.lang.IllegalStateException: Container exited with code 1
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT.setupBeforeAll(ElasticsearchConnectorKerberosIT.java:37)

[INFO] Running io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 s <<< FAILURE! - in io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT
[ERROR] io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT  Time elapsed: 0 s  <<< ERROR!
org.testcontainers.containers.ContainerLaunchException: Container startup failed
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT.setupBeforeAll(ElasticsearchConnectorKerberosWithSslIT.java:27)
Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT.setupBeforeAll(ElasticsearchConnectorKerberosWithSslIT.java:27)
Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT.setupBeforeAll(ElasticsearchConnectorKerberosWithSslIT.java:27)
Caused by: java.lang.IllegalStateException: Container exited with code 1
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT.setupBeforeAll(ElasticsearchConnectorKerberosWithSslIT.java:27)

[INFO] Running io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorNetworkIT
[2025-10-07 11:32:40,511] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$:31)
[2025-10-07 11:32:40,676] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster6906859873313648356
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:34237
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:32:40,777] INFO starting (kafka.server.KafkaServer:66)
[2025-10-07 11:32:40,777] INFO Connecting to zookeeper on 127.0.0.1:34237 (kafka.server.KafkaServer:66)
[2025-10-07 11:32:40,795] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:34237. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:32:40,817] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:32:40,849] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:32:40,942] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:32:40,959] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 11:32:40,959] INFO Cleared cache (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 11:32:40,982] INFO Cluster ID = oscwy5YTTly1dnbJDz_L-g (kafka.server.KafkaServer:66)
[2025-10-07 11:32:40,985] WARN No meta.properties file under dir /tmp/EmbeddedKafkaCluster6906859873313648356/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2025-10-07 11:32:41,031] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster6906859873313648356
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:34237
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:32:41,040] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster6906859873313648356
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:34237
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:32:41,077] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:41,079] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:41,081] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:41,084] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:41,118] INFO Loading logs from log dirs ArraySeq(/tmp/EmbeddedKafkaCluster6906859873313648356) (kafka.log.LogManager:66)
[2025-10-07 11:32:41,124] INFO Attempting recovery for all logs in /tmp/EmbeddedKafkaCluster6906859873313648356 since no clean shutdown file was found (kafka.log.LogManager:66)
[2025-10-07 11:32:41,127] INFO Loaded 0 logs in 0ms. (kafka.log.LogManager:66)
[2025-10-07 11:32:41,128] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:66)
[2025-10-07 11:32:41,129] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:66)
[2025-10-07 11:32:41,143] INFO Starting the log cleaner (kafka.log.LogCleaner:66)
[2025-10-07 11:32:41,196] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner:66)
[2025-10-07 11:32:41,550] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:41,671] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas:66)
[2025-10-07 11:32:41,674] INFO Awaiting socket connections on localhost:37365. (kafka.network.Acceptor:66)
[2025-10-07 11:32:41,702] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 11:32:41,709] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:41,738] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:41,739] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:41,741] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:41,742] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:41,759] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:32:41,788] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:32:41,809] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1759836761802,1759836761802,1,0,0,72057687946559488,204,0,25
 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:32:41,810] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:37365, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:32:41,879] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:32:41,891] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:41,896] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:32:41,899] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:41,901] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:41,988] INFO [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:41,993] INFO [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{}) (kafka.controller.KafkaController:66)
[2025-10-07 11:32:41,994] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 11:32:42,008] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 11:32:42,008] INFO [Controller id=0] Registering handlers (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,011] INFO [Controller id=0] Deleting log dir event notifications (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,011] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:42,013] INFO [Controller id=0] Deleting isr change notifications (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,014] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:42,015] INFO [Controller id=0] Initializing controller context (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,028] INFO [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25) (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,036] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:32:42,042] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:32:42,042] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:32:42,046] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread:66)
[2025-10-07 11:32:42,048] INFO [Controller id=0] Currently active brokers in the cluster: Set(0) (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,048] INFO [Controller id=0] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,049] INFO [Controller id=0] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,049] INFO [Controller id=0] Fetching topic deletions in progress (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,051] INFO [Controller id=0] List of topics to be deleted:  (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,051] INFO [Controller id=0] List of topics ineligible for deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,051] INFO [Controller id=0] Initializing topic deletion manager (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,052] INFO [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager:66)
[2025-10-07 11:32:42,052] INFO [Controller id=0] Sending update metadata request (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,055] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:42,062] INFO [ReplicaStateMachine controllerId=0] Initializing replica state (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:32:42,062] INFO [ReplicaStateMachine controllerId=0] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:32:42,071] INFO [ReplicaStateMachine controllerId=0] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:32:42,072] INFO [PartitionStateMachine controllerId=0] Initializing partition state (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:32:42,072] INFO [PartitionStateMachine controllerId=0] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:32:42,074] INFO [RequestSendThread controllerId=0] Controller 0 connected to localhost:37365 (id: 0 rack: null) for sending state change requests (kafka.controller.RequestSendThread:66)
[2025-10-07 11:32:42,075] INFO [Controller id=0] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,079] INFO [Controller id=0] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,079] INFO [Controller id=0] Partitions that completed preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,079] INFO [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,080] INFO [Controller id=0] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,082] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:42,082] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,092] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController:66)
[2025-10-07 11:32:42,103] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:32:42,110] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 11:32:42,113] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 11:32:42,114] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 11:32:42,116] INFO [KafkaServer id=0] started (kafka.server.KafkaServer:66)
[2025-10-07 11:32:42,378] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker localhost:37365 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:42,378] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:37365 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:42,397] INFO [Controller id=0] Acquired new producerId block ProducerIdsBlock{brokerId=0, producerIdStart=0, producerIdLen=1000} by writing to Zk with path version 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:45,684] INFO Creating topic connect-offset-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:32:45,708] INFO [Controller id=0] New topics: [Set(connect-offset-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-offset-topic-elasticsearch-it-connect-cluster,Some(txKxQ1DHQTiv4B9titFM4A),HashMap(connect-offset-topic-elasticsearch-it-connect-cluster-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:32:45,709] INFO [Controller id=0] New partition creation callback for connect-offset-topic-elasticsearch-it-connect-cluster-20,connect-offset-topic-elasticsearch-it-connect-cluster-7,connect-offset-topic-elasticsearch-it-connect-cluster-0,connect-offset-topic-elasticsearch-it-connect-cluster-12,connect-offset-topic-elasticsearch-it-connect-cluster-13,connect-offset-topic-elasticsearch-it-connect-cluster-5,connect-offset-topic-elasticsearch-it-connect-cluster-21,connect-offset-topic-elasticsearch-it-connect-cluster-14,connect-offset-topic-elasticsearch-it-connect-cluster-6,connect-offset-topic-elasticsearch-it-connect-cluster-1,connect-offset-topic-elasticsearch-it-connect-cluster-19,connect-offset-topic-elasticsearch-it-connect-cluster-17,connect-offset-topic-elasticsearch-it-connect-cluster-16,connect-offset-topic-elasticsearch-it-connect-cluster-23,connect-offset-topic-elasticsearch-it-connect-cluster-10,connect-offset-topic-elasticsearch-it-connect-cluster-3,connect-offset-topic-elasticsearch-it-connect-cluster-15,connect-offset-topic-elasticsearch-it-connect-cluster-22,connect-offset-topic-elasticsearch-it-connect-cluster-24,connect-offset-topic-elasticsearch-it-connect-cluster-18,connect-offset-topic-elasticsearch-it-connect-cluster-2,connect-offset-topic-elasticsearch-it-connect-cluster-4,connect-offset-topic-elasticsearch-it-connect-cluster-8,connect-offset-topic-elasticsearch-it-connect-cluster-9,connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:45,711] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,711] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,711] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,711] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,712] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:45,713] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:45,718] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:45,788] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,788] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,788] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,788] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,788] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,788] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,788] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,789] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:45,791] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 25 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:32:45,792] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 25 partitions (state.change.logger:66)
[2025-10-07 11:32:45,795] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:45,799] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 11:32:45,841] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-offset-topic-elasticsearch-it-connect-cluster-20, connect-offset-topic-elasticsearch-it-connect-cluster-7, connect-offset-topic-elasticsearch-it-connect-cluster-0, connect-offset-topic-elasticsearch-it-connect-cluster-12, connect-offset-topic-elasticsearch-it-connect-cluster-13, connect-offset-topic-elasticsearch-it-connect-cluster-5, connect-offset-topic-elasticsearch-it-connect-cluster-21, connect-offset-topic-elasticsearch-it-connect-cluster-14, connect-offset-topic-elasticsearch-it-connect-cluster-6, connect-offset-topic-elasticsearch-it-connect-cluster-1, connect-offset-topic-elasticsearch-it-connect-cluster-19, connect-offset-topic-elasticsearch-it-connect-cluster-17, connect-offset-topic-elasticsearch-it-connect-cluster-16, connect-offset-topic-elasticsearch-it-connect-cluster-23, connect-offset-topic-elasticsearch-it-connect-cluster-10, connect-offset-topic-elasticsearch-it-connect-cluster-3, connect-offset-topic-elasticsearch-it-connect-cluster-15, connect-offset-topic-elasticsearch-it-connect-cluster-22, connect-offset-topic-elasticsearch-it-connect-cluster-24, connect-offset-topic-elasticsearch-it-connect-cluster-18, connect-offset-topic-elasticsearch-it-connect-cluster-2, connect-offset-topic-elasticsearch-it-connect-cluster-4, connect-offset-topic-elasticsearch-it-connect-cluster-8, connect-offset-topic-elasticsearch-it-connect-cluster-9, connect-offset-topic-elasticsearch-it-connect-cluster-11) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:32:45,842] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 25 partitions (state.change.logger:66)
[2025-10-07 11:32:45,914] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-10, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:45,932] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-10 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:45,935] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 (kafka.cluster.Partition:66)
[2025-10-07 11:32:45,935] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:45,937] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:45,950] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-14, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:45,951] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-14 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:45,951] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 (kafka.cluster.Partition:66)
[2025-10-07 11:32:45,951] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:45,951] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:45,962] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-18, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:45,963] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-18 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:45,963] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 (kafka.cluster.Partition:66)
[2025-10-07 11:32:45,963] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:45,963] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:45,974] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:45,975] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:45,975] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 11:32:45,975] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:45,975] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:45,986] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-22, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:45,987] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-22 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:45,987] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 (kafka.cluster.Partition:66)
[2025-10-07 11:32:45,987] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:45,987] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:45,998] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-7, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:45,999] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-7 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:45,999] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 (kafka.cluster.Partition:66)
[2025-10-07 11:32:45,999] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:45,999] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,010] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-11, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,011] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-11 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,011] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,011] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,011] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,024] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-15, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,024] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-15 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,024] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,024] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,025] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,035] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,036] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,036] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,036] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,036] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,046] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-19, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,047] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-19 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,047] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,047] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,047] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,058] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,058] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,059] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,059] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,059] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,069] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-23, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,069] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-23 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,069] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,069] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,070] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,081] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-8, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,081] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-8 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,082] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,082] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,082] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,090] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-12, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,091] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-12 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,091] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,091] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,091] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,102] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-16, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,103] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-16 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,103] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,103] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,103] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 11:32:46 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Oct 07, 2025 11:32:46 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Oct 07, 2025 11:32:46 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Oct 07, 2025 11:32:46 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
[2025-10-07 11:32:46,115] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,116] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,116] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,116] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,116] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,127] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-20, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,127] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-20 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,127] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,127] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,127] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,138] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-5, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,139] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-5 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,140] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,140] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,140] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,150] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-24, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,151] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-24 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,151] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,151] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,151] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,164] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-9, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,165] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-9 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,165] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,165] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,165] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,176] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-13, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,176] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-13 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,176] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,176] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,176] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,187] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-17, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,187] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-17 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,187] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,188] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,188] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,198] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,199] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,199] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,199] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,199] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,212] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-21, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,213] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-21 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,213] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,213] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,213] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,224] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-6, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,224] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-offset-topic-elasticsearch-it-connect-cluster-6 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,224] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,224] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,224] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,241] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 11:32:46,249] INFO [Broker id=0] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger:66)
[2025-10-07 11:32:46,405] INFO Creating topic connect-storage-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:32:46,414] INFO [Controller id=0] New topics: [Set(connect-storage-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-storage-topic-elasticsearch-it-connect-cluster,Some(iBD_ixZkT9a8_7pu2_4nlw),HashMap(connect-storage-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:32:46,415] INFO [Controller id=0] New partition creation callback for connect-storage-topic-elasticsearch-it-connect-cluster-2,connect-storage-topic-elasticsearch-it-connect-cluster-0,connect-storage-topic-elasticsearch-it-connect-cluster-3,connect-storage-topic-elasticsearch-it-connect-cluster-4,connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:46,415] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,415] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,415] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,415] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,415] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,415] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:46,416] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:46,428] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,428] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,428] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,428] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,428] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,428] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:32:46,429] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions (state.change.logger:66)
[2025-10-07 11:32:46,429] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:46,430] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 11:32:46,434] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-storage-topic-elasticsearch-it-connect-cluster-2, connect-storage-topic-elasticsearch-it-connect-cluster-0, connect-storage-topic-elasticsearch-it-connect-cluster-3, connect-storage-topic-elasticsearch-it-connect-cluster-4, connect-storage-topic-elasticsearch-it-connect-cluster-1) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:32:46,435] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions (state.change.logger:66)
[2025-10-07 11:32:46,439] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,441] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-storage-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,441] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,442] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,442] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,452] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,453] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-storage-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,453] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,453] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,453] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,464] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,465] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-storage-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,465] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,465] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,465] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,476] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,477] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-storage-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,477] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,477] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,477] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,487] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,488] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-storage-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,488] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,488] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,488] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,498] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 11:32:46,500] INFO [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger:66)
[2025-10-07 11:32:46,538] INFO Creating topic connect-config-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:32:46,544] INFO [Controller id=0] New topics: [Set(connect-config-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-config-topic-elasticsearch-it-connect-cluster,Some(q6UwUW8TSluHaxHeDP_91g),Map(connect-config-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:32:46,544] INFO [Controller id=0] New partition creation callback for connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:46,544] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,544] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:46,544] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:46,548] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,548] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:32:46,548] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:46,549] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:46,549] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:46,550] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(connect-config-topic-elasticsearch-it-connect-cluster-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:32:46,550] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:46,552] INFO [LogLoader partition=connect-config-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,553] INFO Created log for partition connect-config-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster6906859873313648356/connect-config-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,553] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,554] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-config-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,554] INFO [Broker id=0] Leader connect-config-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,563] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 5 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:46,564] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6 (state.change.logger:66)
[2025-10-07 11:32:46,603] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
Oct 07, 2025 11:32:46 AM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

[2025-10-07 11:32:46,614] INFO [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(Q80FTDIIRAyPLrtSrg_8Xw),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:32:46,614] INFO [Controller id=0] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:46,615] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,615] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,615] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,615] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,615] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,615] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,615] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,615] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,615] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,616] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,618] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:46,620] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:46,620] INFO Creating topic test with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:32:46,659] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,660] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,661] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,662] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,662] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,662] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,662] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,662] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,662] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,662] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,662] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,662] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 50 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:32:46,662] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 50 partitions (state.change.logger:66)
[2025-10-07 11:32:46,663] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:46,664] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 11:32:46,666] INFO [Controller id=0] New topics: [HashSet(test)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(test,Some(j_6xoAvKRoGxLkNtmv4aQQ),Map(test-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:32:46,666] INFO [Controller id=0] New partition creation callback for test-0 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:46,666] INFO [Controller id=0 epoch=1] Changed partition test-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:46,666] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:46,666] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:46,669] INFO [Controller id=0 epoch=1] Changed partition test-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:46,669] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:32:46,670] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:46,670] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:46,676] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:32:46,676] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 0 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger:66)
[2025-10-07 11:32:46,679] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,679] INFO Created log for partition __consumer_offsets-3 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,680] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,680] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,680] INFO [Broker id=0] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,689] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,689] INFO Created log for partition __consumer_offsets-18 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,690] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,690] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,690] INFO [Broker id=0] Leader __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,700] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,701] INFO Created log for partition __consumer_offsets-41 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,701] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,701] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,701] INFO [Broker id=0] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,711] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,712] INFO Created log for partition __consumer_offsets-10 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,712] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,712] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,712] INFO [Broker id=0] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,722] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,723] INFO Created log for partition __consumer_offsets-33 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,723] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,723] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,723] INFO [Broker id=0] Leader __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,734] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,734] INFO Created log for partition __consumer_offsets-48 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,734] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,734] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,734] INFO [Broker id=0] Leader __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,745] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,745] INFO Created log for partition __consumer_offsets-19 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,745] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,745] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,745] INFO [Broker id=0] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,756] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,757] INFO Created log for partition __consumer_offsets-34 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,757] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,757] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,757] INFO [Broker id=0] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,768] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,768] INFO Created log for partition __consumer_offsets-4 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,768] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,768] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,768] INFO [Broker id=0] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,780] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,781] INFO Created log for partition __consumer_offsets-11 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,781] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,781] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,781] INFO [Broker id=0] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,792] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,793] INFO Created log for partition __consumer_offsets-26 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,793] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,793] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,793] INFO [Broker id=0] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,804] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,804] INFO Created log for partition __consumer_offsets-49 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,804] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,804] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,804] INFO [Broker id=0] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,814] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,815] INFO Created log for partition __consumer_offsets-39 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,815] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,815] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,815] INFO [Broker id=0] Leader __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,825] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,826] INFO Created log for partition __consumer_offsets-9 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,826] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,826] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,826] INFO [Broker id=0] Leader __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,837] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,838] INFO Created log for partition __consumer_offsets-24 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,838] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,838] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,838] INFO [Broker id=0] Leader __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,849] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,849] INFO Created log for partition __consumer_offsets-31 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,850] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,850] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,850] INFO [Broker id=0] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,861] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,861] INFO Created log for partition __consumer_offsets-46 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,861] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,861] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,861] INFO [Broker id=0] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,873] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,873] INFO Created log for partition __consumer_offsets-1 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,873] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,873] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,873] INFO [Broker id=0] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,886] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,887] INFO Created log for partition __consumer_offsets-16 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,887] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,887] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,887] INFO [Broker id=0] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,898] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,898] INFO Created log for partition __consumer_offsets-2 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,898] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,898] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,898] INFO [Broker id=0] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,911] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,911] INFO Created log for partition __consumer_offsets-25 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,911] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,912] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,912] INFO [Broker id=0] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,922] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,922] INFO Created log for partition __consumer_offsets-40 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,922] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,923] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,923] INFO [Broker id=0] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,934] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,934] INFO Created log for partition __consumer_offsets-47 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,934] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,934] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,934] INFO [Broker id=0] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,945] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,946] INFO Created log for partition __consumer_offsets-17 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,946] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,946] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,946] INFO [Broker id=0] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,957] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,957] INFO Created log for partition __consumer_offsets-32 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,957] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,957] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,957] INFO [Broker id=0] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,968] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,969] INFO Created log for partition __consumer_offsets-37 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,969] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,969] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,969] INFO [Broker id=0] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,980] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,980] INFO Created log for partition __consumer_offsets-7 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,980] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,980] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,980] INFO [Broker id=0] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:46,991] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:46,991] INFO Created log for partition __consumer_offsets-22 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:46,992] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,992] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:46,992] INFO [Broker id=0] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,003] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,003] INFO Created log for partition __consumer_offsets-29 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,003] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,003] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,003] INFO [Broker id=0] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,015] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,015] INFO Created log for partition __consumer_offsets-44 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,015] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,015] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,015] INFO [Broker id=0] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,026] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,026] INFO Created log for partition __consumer_offsets-14 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,026] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,026] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,027] INFO [Broker id=0] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,039] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,039] INFO Created log for partition __consumer_offsets-23 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,039] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,039] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,039] INFO [Broker id=0] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,050] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,051] INFO Created log for partition __consumer_offsets-38 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,051] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,051] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,051] INFO [Broker id=0] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,063] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,064] INFO Created log for partition __consumer_offsets-8 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,064] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,064] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,064] INFO [Broker id=0] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,074] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,075] INFO Created log for partition __consumer_offsets-45 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,075] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,075] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,075] INFO [Broker id=0] Leader __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,086] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,086] INFO Created log for partition __consumer_offsets-15 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,086] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,086] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,086] INFO [Broker id=0] Leader __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,094] INFO [Controller id=0] Processing automatic preferred replica leader election (kafka.controller.KafkaController:66)
[2025-10-07 11:32:47,098] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,098] INFO Created log for partition __consumer_offsets-30 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,098] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,098] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,098] INFO [Broker id=0] Leader __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,110] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,110] INFO Created log for partition __consumer_offsets-0 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,110] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,110] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,110] INFO [Broker id=0] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,121] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,121] INFO Created log for partition __consumer_offsets-35 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,121] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,121] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,121] INFO [Broker id=0] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,132] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,133] INFO Created log for partition __consumer_offsets-5 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,133] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,133] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,133] INFO [Broker id=0] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,143] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,144] INFO Created log for partition __consumer_offsets-20 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,144] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,144] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,144] INFO [Broker id=0] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,154] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,155] INFO Created log for partition __consumer_offsets-27 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,155] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,155] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,155] INFO [Broker id=0] Leader __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,166] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,166] INFO Created log for partition __consumer_offsets-42 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,166] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,166] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,166] INFO [Broker id=0] Leader __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,177] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,178] INFO Created log for partition __consumer_offsets-12 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,178] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,178] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,178] INFO [Broker id=0] Leader __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,189] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,189] INFO Created log for partition __consumer_offsets-21 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,189] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,189] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,189] INFO [Broker id=0] Leader __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,200] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,201] INFO Created log for partition __consumer_offsets-36 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,201] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,201] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,201] INFO [Broker id=0] Leader __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,212] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,212] INFO Created log for partition __consumer_offsets-6 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,212] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,212] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,212] INFO [Broker id=0] Leader __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,223] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,223] INFO Created log for partition __consumer_offsets-43 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,223] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,223] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,223] INFO [Broker id=0] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,233] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,234] INFO Created log for partition __consumer_offsets-13 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,234] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,234] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,234] INFO [Broker id=0] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,245] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,246] INFO Created log for partition __consumer_offsets-28 in /tmp/EmbeddedKafkaCluster6906859873313648356/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,246] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,246] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,246] INFO [Broker id=0] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,256] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,259] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,260] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,260] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,260] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,260] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,260] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,260] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,260] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,260] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,261] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,261] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,261] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,261] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,261] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,261] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,261] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,261] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,261] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,261] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,261] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,261] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,261] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,261] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,261] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,261] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,261] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,261] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,261] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,261] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,261] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,261] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,261] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,262] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,262] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,262] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,262] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,262] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,262] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,262] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,262] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,262] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,262] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,262] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,262] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,262] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,262] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,262] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,262] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,262] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,262] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,262] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,262] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,262] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,262] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,263] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,263] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,263] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,263] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,263] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,263] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,263] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,263] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,263] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,263] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,263] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,263] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,263] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,263] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,263] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,263] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,263] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,263] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,263] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,263] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,263] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,263] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,263] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,263] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,263] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,263] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,263] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,263] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,264] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,264] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,264] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,264] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,264] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,264] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,264] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,264] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,264] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,264] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,264] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,264] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,264] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,264] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,264] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,264] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,264] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,264] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 7 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 11:32:47,265] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 5 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,266] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,266] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,266] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,266] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,266] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,266] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,267] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,267] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,267] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,267] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,267] INFO [Broker id=0] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger:66)
[2025-10-07 11:32:47,267] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,268] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,268] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,268] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,268] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,268] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,269] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,269] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,269] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:47,269] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,269] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,270] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 8 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,270] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,270] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:32:47,270] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:47,270] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,270] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,270] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,270] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,270] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,271] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,271] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,271] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,271] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,271] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,271] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,271] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,271] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,272] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,272] INFO [LogLoader partition=test-0, dir=/tmp/EmbeddedKafkaCluster6906859873313648356] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:47,272] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,272] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,272] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,272] INFO Created log for partition test-0 in /tmp/EmbeddedKafkaCluster6906859873313648356/test-0 with properties {} (kafka.log.LogManager:66)
[2025-10-07 11:32:47,272] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,272] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,273] INFO [Partition test-0 broker=0] No checkpointed highwatermark is found for partition test-0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,273] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 9 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,273] INFO [Partition test-0 broker=0] Log loaded for partition test-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:47,273] INFO [Broker id=0] Leader test-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:47,273] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,273] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,273] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,273] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,273] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,273] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,274] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:47,281] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 9 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:47,283] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 10 (state.change.logger:66)
[2025-10-07 11:32:47,334] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-integration-test-elasticsearch-it-connect-cluster in Empty state. Created a new member id connect-1-8566bba8-3fc7-4728-8b82-3cdae3832958 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,352] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member connect-1-8566bba8-3fc7-4728-8b82-3cdae3832958 with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,360] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 1 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,392] INFO [GroupCoordinator 0]: Assignment received from leader connect-1-8566bba8-3fc7-4728-8b82-3cdae3832958 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,498] INFO ElasticsearchSinkConnectorConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:42445/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 600000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 600000
	max.buffered.records = 10
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 600000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 11:32:47,500] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:32:47,500] INFO Using unsecured connection to [http://localhost:42445]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:32:47,534] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 1 (__consumer_offsets-4) (reason: Leader connect-1-8566bba8-3fc7-4728-8b82-3cdae3832958 re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,535] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 2 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,542] INFO [GroupCoordinator 0]: Assignment received from leader connect-1-8566bba8-3fc7-4728-8b82-3cdae3832958 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 2. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,557] INFO [es-connector|worker] ElasticsearchSinkConnectorConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:42445/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 600000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 600000
	max.buffered.records = 10
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 600000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 11:32:47,606] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 2 (__consumer_offsets-4) (reason: Leader connect-1-8566bba8-3fc7-4728-8b82-3cdae3832958 re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,606] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 3 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,612] INFO [GroupCoordinator 0]: Assignment received from leader connect-1-8566bba8-3fc7-4728-8b82-3cdae3832958 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,613] ERROR Could not check connector state info. (io.confluent.connect.elasticsearch.integration.BaseConnectorIT:92)
[2025-10-07 11:32:47,636] INFO [es-connector|task-0] Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:32:47,637] INFO [es-connector|task-0] ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:42445/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 600000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 600000
	max.buffered.records = 10
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 600000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 0
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:32:47,637] INFO [es-connector|task-0] Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:32:47,638] DEBUG [es-connector|task-0] Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:32:47,638] INFO [es-connector|task-0] Using unsecured connection to [http://localhost:42445]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:32:47,650] INFO [es-connector|task-0] Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:32:47,669] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-es-connector in Empty state. Created a new member id connector-consumer-es-connector-0-8214547c-8c63-4f62-a72e-4b38c6d15b8c and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,672] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member connector-consumer-es-connector-0-8214547c-8c63-4f62-a72e-4b38c6d15b8c with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,675] INFO [GroupCoordinator 0]: Stabilized group connect-es-connector generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,686] INFO [GroupCoordinator 0]: Assignment received from leader connector-consumer-es-connector-0-8214547c-8c63-4f62-a72e-4b38c6d15b8c for group connect-es-connector for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:47,772] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,772] INFO [es-connector|task-0] Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:32:47,877] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,887] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,891] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,896] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,902] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,904] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,907] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,908] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,913] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,914] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,918] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,919] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,924] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,926] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,928] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,933] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,935] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,937] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,939] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,940] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,942] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,943] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,945] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,946] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,948] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,956] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,957] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,961] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,963] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,964] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,966] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,970] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,972] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,974] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,975] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,977] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,978] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,980] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,982] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,989] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,990] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,992] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,993] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,995] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,996] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:47,998] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,000] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,004] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,006] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,007] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,009] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,010] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,011] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,012] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,013] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,015] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,016] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,018] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,019] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,020] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,021] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,022] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,026] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,029] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,030] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,031] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,033] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,034] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,035] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,037] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,039] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,040] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,041] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,043] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,045] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,046] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,048] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,049] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,050] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,052] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,053] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,054] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,056] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,057] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,058] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,062] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,064] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,065] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,069] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,070] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,072] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,074] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,075] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,076] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,077] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,078] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,079] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,080] DEBUG [es-connector|task-0] Pausing all partitions (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:363)
[2025-10-07 11:32:48,181] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,281] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,382] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,482] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,583] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,683] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,784] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,884] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:48,985] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:49,085] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:49,186] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:49,286] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:49,387] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:49,488] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:49,588] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:49,689] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:49,789] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:49,890] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:49,990] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:50,091] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:50,191] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:50,291] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:50,392] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:50,492] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:50,593] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:50,693] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:50,794] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:50,894] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:50,894] DEBUG [es-connector|task-0] Resuming all partitions (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:348)
[2025-10-07 11:32:50,896] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:50,915] DEBUG [es-connector|task-0] Putting 500 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:51,085] DEBUG [es-connector|task-0] Putting 399 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:51,255] DEBUG [es-connector|task-0] preCommitting offsets {test-0=OffsetAndMetadata{offset=1001, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:32:51,271] DEBUG [es-connector|task-0] Stopping Elasticsearch client. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:146)
[2025-10-07 11:32:51,278] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 1 (__consumer_offsets-23) (reason: Removing member connector-consumer-es-connector-0-8214547c-8c63-4f62-a72e-4b38c6d15b8c on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:51,279] INFO [GroupCoordinator 0]: Group connect-es-connector with generation 2 is now empty (__consumer_offsets-23) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:51,281] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connector-consumer-es-connector-0-8214547c-8c63-4f62-a72e-4b38c6d15b8c, groupInstanceId=None, clientId=connector-consumer-es-connector-0, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group connect-es-connector through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:51,289] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 3 (__consumer_offsets-4) (reason: Removing member connect-1-8566bba8-3fc7-4728-8b82-3cdae3832958 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:51,289] INFO [GroupCoordinator 0]: Group connect-integration-test-elasticsearch-it-connect-cluster with generation 4 is now empty (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:51,290] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connect-1-8566bba8-3fc7-4728-8b82-3cdae3832958, groupInstanceId=None, clientId=connect-1, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(sessioned, compatible, default)) has left group connect-integration-test-elasticsearch-it-connect-cluster through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:51,303] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer:66)
[2025-10-07 11:32:51,304] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer:66)
[2025-10-07 11:32:51,307] INFO [Controller id=0] Shutting down broker 0 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:51,310] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:51,313] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer:66)
[2025-10-07 11:32:51,314] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:32:51,315] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:32:51,315] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:32:51,316] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 11:32:51,323] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 11:32:51,323] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 11:32:51,329] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 11:32:51,331] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:51,367] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:51,367] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:51,368] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis:66)
[2025-10-07 11:32:51,368] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:51,567] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:51,567] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:51,569] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:32:51,570] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager:66)
[2025-10-07 11:32:51,570] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:32:51,571] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:32:51,571] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:32:51,573] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:32:51,573] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:51,573] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:51,767] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:51,767] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:51,768] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:51,877] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:51,877] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:51,879] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:51,879] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager:66)
[2025-10-07 11:32:51,879] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:32:51,883] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:32:51,883] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:32:51,884] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:32:51,885] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:32:51,886] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 11:32:51,886] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 11:32:51,886] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:51,992] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:51,992] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:51,993] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:52,087] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:52,087] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:52,087] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:52,286] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:52,286] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:52,286] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:52,287] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:52,287] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:52,294] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager:66)
[2025-10-07 11:32:52,295] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:52,295] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:52,295] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:52,297] INFO Broker to controller channel manager for alterIsr shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 11:32:52,298] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:52,298] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:52,298] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:52,300] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 11:32:52,300] INFO Shutting down. (kafka.log.LogManager:66)
[2025-10-07 11:32:52,301] INFO Shutting down the log cleaner. (kafka.log.LogCleaner:66)
[2025-10-07 11:32:52,301] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner:66)
[2025-10-07 11:32:52,302] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner:66)
[2025-10-07 11:32:52,302] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner:66)
[2025-10-07 11:32:52,327] INFO [ProducerStateManager partition=test-0] Wrote producer snapshot at offset 1001 with 1 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:32:52,353] INFO [ProducerStateManager partition=__consumer_offsets-23] Wrote producer snapshot at offset 3 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:32:52,359] INFO [ProducerStateManager partition=connect-config-topic-elasticsearch-it-connect-cluster-0] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:32:52,366] INFO [ProducerStateManager partition=__consumer_offsets-4] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:32:52,374] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-2] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:32:52,384] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-3] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:32:52,403] INFO Shutdown complete. (kafka.log.LogManager:66)
[2025-10-07 11:32:52,404] INFO [ControllerEventThread controllerId=0] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:32:52,404] INFO [ControllerEventThread controllerId=0] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:32:52,404] INFO [ControllerEventThread controllerId=0] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:32:52,407] INFO [PartitionStateMachine controllerId=0] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:32:52,408] INFO [ReplicaStateMachine controllerId=0] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:32:52,408] INFO [RequestSendThread controllerId=0] Shutting down (kafka.controller.RequestSendThread:66)
[2025-10-07 11:32:52,408] INFO [RequestSendThread controllerId=0] Shutdown completed (kafka.controller.RequestSendThread:66)
[2025-10-07 11:32:52,408] INFO [RequestSendThread controllerId=0] Stopped (kafka.controller.RequestSendThread:66)
[2025-10-07 11:32:52,412] INFO [Controller id=0] Resigned (kafka.controller.KafkaController:66)
[2025-10-07 11:32:52,412] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:32:52,412] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:32:52,412] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:32:52,413] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:32:52,521] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:32:52,521] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:53,082] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:53,082] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:53,082] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:54,082] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:54,082] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:54,082] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:54,083] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:54,083] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:54,083] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:54,086] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:54,086] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:54,087] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer:66)
[2025-10-07 11:32:54,122] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer:66)
[2025-10-07 11:32:54,125] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats:66)
[2025-10-07 11:32:54,125] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer:66)
[2025-10-07 11:32:54,175] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster3324690897593769643
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:37067
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:32:54,178] INFO starting (kafka.server.KafkaServer:66)
[2025-10-07 11:32:54,178] INFO Connecting to zookeeper on 127.0.0.1:37067 (kafka.server.KafkaServer:66)
[2025-10-07 11:32:54,179] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:37067. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:32:54,181] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:32:54,183] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:32:54,203] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:32:54,206] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 11:32:54,206] INFO Cleared cache (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 11:32:54,211] INFO Cluster ID = 5toLzObuS2C7cVdcZHLDJw (kafka.server.KafkaServer:66)
[2025-10-07 11:32:54,211] WARN No meta.properties file under dir /tmp/EmbeddedKafkaCluster3324690897593769643/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2025-10-07 11:32:54,215] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster3324690897593769643
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:37067
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:32:54,217] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster3324690897593769643
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:37067
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:32:54,227] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:54,229] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:54,229] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:54,229] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:54,232] INFO Loading logs from log dirs ArraySeq(/tmp/EmbeddedKafkaCluster3324690897593769643) (kafka.log.LogManager:66)
[2025-10-07 11:32:54,232] INFO Attempting recovery for all logs in /tmp/EmbeddedKafkaCluster3324690897593769643 since no clean shutdown file was found (kafka.log.LogManager:66)
[2025-10-07 11:32:54,233] INFO Loaded 0 logs in 0ms. (kafka.log.LogManager:66)
[2025-10-07 11:32:54,233] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:66)
[2025-10-07 11:32:54,233] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:66)
[2025-10-07 11:32:54,234] INFO Starting the log cleaner (kafka.log.LogCleaner:66)
[2025-10-07 11:32:54,244] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner:66)
[2025-10-07 11:32:54,246] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:54,259] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas:66)
[2025-10-07 11:32:54,259] INFO Awaiting socket connections on localhost:40957. (kafka.network.Acceptor:66)
[2025-10-07 11:32:54,261] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 11:32:54,263] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:54,265] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:54,266] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:54,267] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:54,268] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:54,269] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:32:54,272] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:32:54,275] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1759836774273,1759836774273,1,0,0,72057688833589248,204,0,25
 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:32:54,275] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:40957, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:32:54,284] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:32:54,286] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:54,288] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:54,289] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:54,290] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:54,290] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:32:54,291] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:54,292] INFO [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,293] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:32:54,293] INFO [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{}) (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,294] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 11:32:54,294] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:32:54,295] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:32:54,296] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:54,297] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 11:32:54,297] INFO [Controller id=0] Registering handlers (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,298] INFO [Controller id=0] Deleting log dir event notifications (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,299] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:32:54,300] INFO [Controller id=0] Deleting isr change notifications (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,300] INFO [Controller id=0] Initializing controller context (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,302] INFO [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25) (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,303] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 11:32:54,304] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 11:32:54,305] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 11:32:54,305] INFO [KafkaServer id=0] started (kafka.server.KafkaServer:66)
[2025-10-07 11:32:54,306] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread:66)
[2025-10-07 11:32:54,306] INFO [Controller id=0] Currently active brokers in the cluster: Set(0) (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,306] INFO [Controller id=0] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,306] INFO [Controller id=0] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,306] INFO [Controller id=0] Fetching topic deletions in progress (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,307] INFO [Controller id=0] List of topics to be deleted:  (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,307] INFO [Controller id=0] List of topics ineligible for deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,307] INFO [Controller id=0] Initializing topic deletion manager (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,307] INFO [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager:66)
[2025-10-07 11:32:54,307] INFO [Controller id=0] Sending update metadata request (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,307] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:54,307] INFO [ReplicaStateMachine controllerId=0] Initializing replica state (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:32:54,308] INFO [ReplicaStateMachine controllerId=0] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:32:54,308] INFO [ReplicaStateMachine controllerId=0] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:32:54,308] INFO [PartitionStateMachine controllerId=0] Initializing partition state (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:32:54,309] INFO [PartitionStateMachine controllerId=0] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:32:54,309] INFO [Controller id=0] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,309] INFO [RequestSendThread controllerId=0] Controller 0 connected to localhost:40957 (id: 0 rack: null) for sending state change requests (kafka.controller.RequestSendThread:66)
[2025-10-07 11:32:54,311] INFO [Controller id=0] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,311] INFO [Controller id=0] Partitions that completed preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,311] INFO [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,311] INFO [Controller id=0] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,311] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,314] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,347] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker localhost:40957 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:54,363] INFO [Controller id=0] Acquired new producerId block ProducerIdsBlock{brokerId=0, producerIdStart=0, producerIdLen=1000} by writing to Zk with path version 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:54,364] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:40957 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:55,814] INFO Creating topic connect-offset-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:32:55,820] INFO [Controller id=0] New topics: [Set(connect-offset-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-offset-topic-elasticsearch-it-connect-cluster,Some(hKmxOBACTROdUsKKKbfs4g),HashMap(connect-offset-topic-elasticsearch-it-connect-cluster-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:32:55,820] INFO [Controller id=0] New partition creation callback for connect-offset-topic-elasticsearch-it-connect-cluster-20,connect-offset-topic-elasticsearch-it-connect-cluster-7,connect-offset-topic-elasticsearch-it-connect-cluster-0,connect-offset-topic-elasticsearch-it-connect-cluster-12,connect-offset-topic-elasticsearch-it-connect-cluster-13,connect-offset-topic-elasticsearch-it-connect-cluster-5,connect-offset-topic-elasticsearch-it-connect-cluster-21,connect-offset-topic-elasticsearch-it-connect-cluster-14,connect-offset-topic-elasticsearch-it-connect-cluster-6,connect-offset-topic-elasticsearch-it-connect-cluster-1,connect-offset-topic-elasticsearch-it-connect-cluster-19,connect-offset-topic-elasticsearch-it-connect-cluster-17,connect-offset-topic-elasticsearch-it-connect-cluster-16,connect-offset-topic-elasticsearch-it-connect-cluster-23,connect-offset-topic-elasticsearch-it-connect-cluster-10,connect-offset-topic-elasticsearch-it-connect-cluster-3,connect-offset-topic-elasticsearch-it-connect-cluster-15,connect-offset-topic-elasticsearch-it-connect-cluster-22,connect-offset-topic-elasticsearch-it-connect-cluster-24,connect-offset-topic-elasticsearch-it-connect-cluster-18,connect-offset-topic-elasticsearch-it-connect-cluster-2,connect-offset-topic-elasticsearch-it-connect-cluster-4,connect-offset-topic-elasticsearch-it-connect-cluster-8,connect-offset-topic-elasticsearch-it-connect-cluster-9,connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,821] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,822] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,822] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,822] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,822] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,822] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,822] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,822] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,822] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,822] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:55,822] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,839] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,840] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,840] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,840] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,840] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 25 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:32:55,840] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 25 partitions (state.change.logger:66)
[2025-10-07 11:32:55,841] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:55,841] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 11:32:55,845] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-offset-topic-elasticsearch-it-connect-cluster-20, connect-offset-topic-elasticsearch-it-connect-cluster-7, connect-offset-topic-elasticsearch-it-connect-cluster-0, connect-offset-topic-elasticsearch-it-connect-cluster-12, connect-offset-topic-elasticsearch-it-connect-cluster-13, connect-offset-topic-elasticsearch-it-connect-cluster-5, connect-offset-topic-elasticsearch-it-connect-cluster-21, connect-offset-topic-elasticsearch-it-connect-cluster-14, connect-offset-topic-elasticsearch-it-connect-cluster-6, connect-offset-topic-elasticsearch-it-connect-cluster-1, connect-offset-topic-elasticsearch-it-connect-cluster-19, connect-offset-topic-elasticsearch-it-connect-cluster-17, connect-offset-topic-elasticsearch-it-connect-cluster-16, connect-offset-topic-elasticsearch-it-connect-cluster-23, connect-offset-topic-elasticsearch-it-connect-cluster-10, connect-offset-topic-elasticsearch-it-connect-cluster-3, connect-offset-topic-elasticsearch-it-connect-cluster-15, connect-offset-topic-elasticsearch-it-connect-cluster-22, connect-offset-topic-elasticsearch-it-connect-cluster-24, connect-offset-topic-elasticsearch-it-connect-cluster-18, connect-offset-topic-elasticsearch-it-connect-cluster-2, connect-offset-topic-elasticsearch-it-connect-cluster-4, connect-offset-topic-elasticsearch-it-connect-cluster-8, connect-offset-topic-elasticsearch-it-connect-cluster-9, connect-offset-topic-elasticsearch-it-connect-cluster-11) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:32:55,845] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 25 partitions (state.change.logger:66)
Oct 07, 2025 11:32:55 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Oct 07, 2025 11:32:55 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Oct 07, 2025 11:32:55 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Oct 07, 2025 11:32:55 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
[2025-10-07 11:32:55,848] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-10, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:55,849] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-10 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:55,849] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,849] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,849] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:55,862] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-14, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:55,862] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-14 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:55,862] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,862] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,863] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:55,873] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-18, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:55,873] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-18 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:55,874] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,874] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,874] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:55,885] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:55,886] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:55,886] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,886] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,886] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 11:32:55 AM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

[2025-10-07 11:32:55,897] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-22, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:55,897] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-22 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:55,898] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,898] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,898] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:55,899] INFO Creating topic test with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:32:55,905] INFO [Controller id=0] New topics: [Set(test)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(test,Some(KATSanxSQ9ifceEpeYX3iQ),Map(test-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:32:55,905] INFO [Controller id=0] New partition creation callback for test-0 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:55,905] INFO [Controller id=0 epoch=1] Changed partition test-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:55,905] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:55,905] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:55,908] INFO [Controller id=0 epoch=1] Changed partition test-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:55,908] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:32:55,908] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-7, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:55,908] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:55,908] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-7 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:55,909] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:55,909] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,909] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,909] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:55,919] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-11, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:55,920] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-11 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:55,920] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,920] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,920] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:55,931] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-15, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:55,931] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-15 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:55,931] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,932] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,932] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:55,943] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:55,943] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:55,943] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,943] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,943] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:55,954] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-19, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:55,955] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-19 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:55,955] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,955] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,955] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:55,966] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:55,966] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:55,966] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,967] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,967] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:55,978] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-23, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:55,978] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-23 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:55,978] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,978] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,978] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:55,989] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-8, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:55,989] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-8 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:55,989] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,989] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:55,990] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,000] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-12, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,295] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-12 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,295] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,295] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,295] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,304] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-16, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,304] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-16 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,304] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,304] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,304] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,315] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,315] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,315] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,315] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,315] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,327] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-20, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,327] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-20 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,328] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,328] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,328] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,338] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-5, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,339] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-5 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,339] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,339] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,339] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,350] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-24, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,350] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-24 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,350] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,350] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,350] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,358] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-9, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,358] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-9 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,358] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,358] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,358] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,369] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-13, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,370] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-13 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,370] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,370] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,370] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,381] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-17, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,381] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-17 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,381] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,381] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,381] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,392] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,393] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,393] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,393] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,393] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,403] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-21, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,404] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-21 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,404] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,404] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,404] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,415] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-6, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,415] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-offset-topic-elasticsearch-it-connect-cluster-6 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,415] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,415] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,415] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,425] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 11:32:56,426] INFO [Broker id=0] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger:66)
[2025-10-07 11:32:56,428] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:56,429] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:32:56,429] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:56,431] INFO [LogLoader partition=test-0, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,432] INFO Created log for partition test-0 in /tmp/EmbeddedKafkaCluster3324690897593769643/test-0 with properties {} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,433] INFO [Partition test-0 broker=0] No checkpointed highwatermark is found for partition test-0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,433] INFO [Partition test-0 broker=0] Log loaded for partition test-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,433] INFO [Broker id=0] Leader test-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,441] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:56,442] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger:66)
[2025-10-07 11:32:56,457] INFO Creating topic connect-storage-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:32:56,466] INFO [Controller id=0] New topics: [Set(connect-storage-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-storage-topic-elasticsearch-it-connect-cluster,Some(50OOgLt3QzO-GYoSfvIq_Q),HashMap(connect-storage-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:32:56,466] INFO [Controller id=0] New partition creation callback for connect-storage-topic-elasticsearch-it-connect-cluster-2,connect-storage-topic-elasticsearch-it-connect-cluster-0,connect-storage-topic-elasticsearch-it-connect-cluster-3,connect-storage-topic-elasticsearch-it-connect-cluster-4,connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:56,467] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,467] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,467] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,467] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,467] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,467] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:56,467] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:56,471] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,471] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,471] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,471] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,471] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,472] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:32:56,472] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions (state.change.logger:66)
[2025-10-07 11:32:56,472] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:56,472] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 11:32:56,475] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-storage-topic-elasticsearch-it-connect-cluster-2, connect-storage-topic-elasticsearch-it-connect-cluster-0, connect-storage-topic-elasticsearch-it-connect-cluster-3, connect-storage-topic-elasticsearch-it-connect-cluster-4, connect-storage-topic-elasticsearch-it-connect-cluster-1) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:32:56,475] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions (state.change.logger:66)
[2025-10-07 11:32:56,489] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,489] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-storage-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,489] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,490] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,490] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,500] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,500] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-storage-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,500] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,500] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,500] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,512] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,512] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-storage-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,512] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,512] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,512] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,523] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,523] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-storage-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,523] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,523] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,523] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,535] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,535] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-storage-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,535] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,535] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,535] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,544] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 11:32:56,545] INFO [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6 (state.change.logger:66)
[2025-10-07 11:32:56,562] INFO Creating topic connect-config-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:32:56,565] INFO [Controller id=0] New topics: [Set(connect-config-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-config-topic-elasticsearch-it-connect-cluster,Some(6nnNEREDQqantHo_f2AwuA),Map(connect-config-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:32:56,566] INFO [Controller id=0] New partition creation callback for connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:56,566] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,566] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:56,566] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:56,568] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,568] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:32:56,569] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:56,569] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:56,569] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:56,569] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(connect-config-topic-elasticsearch-it-connect-cluster-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:32:56,569] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:56,571] INFO [LogLoader partition=connect-config-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,572] INFO Created log for partition connect-config-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster3324690897593769643/connect-config-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,572] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,572] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-config-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,572] INFO [Broker id=0] Leader connect-config-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,582] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:32:56,583] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger:66)
[2025-10-07 11:32:56,610] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:32:56,616] INFO [Controller id=0] New topics: [HashSet(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(DmM-paIkRtm4Y-gA6fXixw),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:32:56,616] INFO [Controller id=0] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:56,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,617] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,618] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,619] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,619] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,619] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,619] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,619] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,619] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,619] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,619] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,619] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,619] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,619] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,619] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:32:56,619] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:56,620] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:56,642] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,643] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 50 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:32:56,644] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 50 partitions (state.change.logger:66)
[2025-10-07 11:32:56,645] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 11:32:56,647] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:56,652] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:32:56,652] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 0 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger:66)
[2025-10-07 11:32:56,654] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,654] INFO Created log for partition __consumer_offsets-3 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,654] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,654] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,654] INFO [Broker id=0] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,665] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,665] INFO Created log for partition __consumer_offsets-18 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,665] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,665] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,665] INFO [Broker id=0] Leader __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,676] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,676] INFO Created log for partition __consumer_offsets-41 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,676] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,676] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,676] INFO [Broker id=0] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,687] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,687] INFO Created log for partition __consumer_offsets-10 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,687] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,687] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,687] INFO [Broker id=0] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,698] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,698] INFO Created log for partition __consumer_offsets-33 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,698] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,698] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,698] INFO [Broker id=0] Leader __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,709] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,709] INFO Created log for partition __consumer_offsets-48 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,709] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,709] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,709] INFO [Broker id=0] Leader __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,720] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,720] INFO Created log for partition __consumer_offsets-19 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,720] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,720] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,720] INFO [Broker id=0] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,731] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,731] INFO Created log for partition __consumer_offsets-34 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,731] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,731] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,731] INFO [Broker id=0] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,742] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,742] INFO Created log for partition __consumer_offsets-4 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,742] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,742] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,742] INFO [Broker id=0] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,753] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,753] INFO Created log for partition __consumer_offsets-11 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,753] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,753] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,753] INFO [Broker id=0] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,764] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,764] INFO Created log for partition __consumer_offsets-26 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,764] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,764] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,764] INFO [Broker id=0] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,774] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,774] INFO Created log for partition __consumer_offsets-49 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,774] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,774] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,774] INFO [Broker id=0] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,785] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,785] INFO Created log for partition __consumer_offsets-39 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,785] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,785] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,785] INFO [Broker id=0] Leader __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,796] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,796] INFO Created log for partition __consumer_offsets-9 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,797] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,797] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,798] INFO [Broker id=0] Leader __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,806] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,806] INFO Created log for partition __consumer_offsets-24 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,806] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,806] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,806] INFO [Broker id=0] Leader __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,816] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,817] INFO Created log for partition __consumer_offsets-31 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,817] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,817] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,817] INFO [Broker id=0] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,824] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,825] INFO Created log for partition __consumer_offsets-46 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,825] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,825] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,825] INFO [Broker id=0] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,832] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,832] INFO Created log for partition __consumer_offsets-1 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,832] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,832] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,833] INFO [Broker id=0] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,843] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,843] INFO Created log for partition __consumer_offsets-16 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,843] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,843] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,843] INFO [Broker id=0] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,853] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,853] INFO Created log for partition __consumer_offsets-2 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,854] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,854] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,854] INFO [Broker id=0] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,864] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,864] INFO Created log for partition __consumer_offsets-25 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,864] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,864] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,864] INFO [Broker id=0] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,875] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,875] INFO Created log for partition __consumer_offsets-40 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,875] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,875] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,875] INFO [Broker id=0] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,887] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,887] INFO Created log for partition __consumer_offsets-47 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,887] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,887] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,887] INFO [Broker id=0] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,898] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,898] INFO Created log for partition __consumer_offsets-17 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,898] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,898] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,898] INFO [Broker id=0] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,909] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,909] INFO Created log for partition __consumer_offsets-32 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,909] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,909] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,909] INFO [Broker id=0] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,919] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,920] INFO Created log for partition __consumer_offsets-37 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,920] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,920] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,920] INFO [Broker id=0] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,930] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,930] INFO Created log for partition __consumer_offsets-7 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,930] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,931] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,931] INFO [Broker id=0] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,941] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,942] INFO Created log for partition __consumer_offsets-22 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,942] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,942] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,942] INFO [Broker id=0] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,953] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,953] INFO Created log for partition __consumer_offsets-29 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,953] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,953] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,953] INFO [Broker id=0] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,960] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,960] INFO Created log for partition __consumer_offsets-44 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,960] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,960] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,960] INFO [Broker id=0] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,971] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,971] INFO Created log for partition __consumer_offsets-14 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,971] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,971] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,971] INFO [Broker id=0] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,981] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,982] INFO Created log for partition __consumer_offsets-23 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,982] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,982] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,982] INFO [Broker id=0] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:56,992] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:56,993] INFO Created log for partition __consumer_offsets-38 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:56,993] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,993] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:56,993] INFO [Broker id=0] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,004] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,004] INFO Created log for partition __consumer_offsets-8 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,004] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,004] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,004] INFO [Broker id=0] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,015] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,015] INFO Created log for partition __consumer_offsets-45 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,015] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,015] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,015] INFO [Broker id=0] Leader __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,026] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,026] INFO Created log for partition __consumer_offsets-15 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,026] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,026] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,026] INFO [Broker id=0] Leader __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,037] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,037] INFO Created log for partition __consumer_offsets-30 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,037] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,037] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,037] INFO [Broker id=0] Leader __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,048] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,049] INFO Created log for partition __consumer_offsets-0 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,049] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,049] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,049] INFO [Broker id=0] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,060] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,060] INFO Created log for partition __consumer_offsets-35 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,060] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,060] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,060] INFO [Broker id=0] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,069] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,069] INFO Created log for partition __consumer_offsets-5 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,070] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,070] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,070] INFO [Broker id=0] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,080] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,080] INFO Created log for partition __consumer_offsets-20 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,080] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,080] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,080] INFO [Broker id=0] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,091] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,091] INFO Created log for partition __consumer_offsets-27 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,091] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,091] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,091] INFO [Broker id=0] Leader __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,101] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,101] INFO Created log for partition __consumer_offsets-42 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,101] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,102] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,102] INFO [Broker id=0] Leader __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,112] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,113] INFO Created log for partition __consumer_offsets-12 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,113] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,113] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,113] INFO [Broker id=0] Leader __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,123] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,124] INFO Created log for partition __consumer_offsets-21 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,124] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,124] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,124] INFO [Broker id=0] Leader __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,134] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,135] INFO Created log for partition __consumer_offsets-36 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,135] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,135] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,135] INFO [Broker id=0] Leader __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,146] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,146] INFO Created log for partition __consumer_offsets-6 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,146] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,146] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,146] INFO [Broker id=0] Leader __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,157] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,157] INFO Created log for partition __consumer_offsets-43 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,157] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,157] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,157] INFO [Broker id=0] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,168] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,168] INFO Created log for partition __consumer_offsets-13 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,169] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,169] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,169] INFO [Broker id=0] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,179] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/EmbeddedKafkaCluster3324690897593769643] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:32:57,180] INFO Created log for partition __consumer_offsets-28 in /tmp/EmbeddedKafkaCluster3324690897593769643/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:32:57,180] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,180] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:32:57,180] INFO [Broker id=0] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:32:57,190] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,190] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,191] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,191] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,191] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,191] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,191] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,191] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,191] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,191] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,191] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,191] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,191] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,191] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,192] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,192] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,192] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,192] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,192] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,192] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,192] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,192] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,192] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,192] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,192] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,192] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,193] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,193] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,193] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,193] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,193] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,193] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,193] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,193] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,193] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,193] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,193] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,193] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,193] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,193] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,193] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,193] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,193] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,193] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,193] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,194] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,194] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,194] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,194] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,194] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,194] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,194] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,194] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,194] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,194] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,194] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,194] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,194] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,194] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,194] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,194] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,194] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,194] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,194] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,194] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,194] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,195] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,195] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,195] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,195] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,195] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,195] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,195] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,195] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,195] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,195] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,195] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,195] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,195] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,195] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,195] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,195] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,195] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,195] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,195] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,195] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,195] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,196] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,196] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,195] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,196] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,196] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,196] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,196] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,196] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,196] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,196] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,196] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,196] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,196] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,196] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,196] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,196] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,196] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,196] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,196] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,196] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,196] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,197] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,197] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,197] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,197] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,197] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,197] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,197] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,197] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,197] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,197] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,197] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,197] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,197] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,197] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,197] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,197] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,197] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,197] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,197] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,198] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,198] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,198] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,198] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,197] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,198] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 11:32:57,198] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,198] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,199] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,199] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,199] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,199] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,199] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,200] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 4 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,200] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,200] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,200] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,200] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,200] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,200] INFO [Broker id=0] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 10 (state.change.logger:66)
[2025-10-07 11:32:57,201] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 4 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,201] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,201] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,201] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,201] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:32:57,217] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-integration-test-elasticsearch-it-connect-cluster in Empty state. Created a new member id connect-2-69c628cd-ea26-4b90-b79c-86dc7fc7003d and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,220] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member connect-2-69c628cd-ea26-4b90-b79c-86dc7fc7003d with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,221] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 1 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,232] INFO [GroupCoordinator 0]: Assignment received from leader connect-2-69c628cd-ea26-4b90-b79c-86dc7fc7003d for group connect-integration-test-elasticsearch-it-connect-cluster for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,245] INFO ElasticsearchSinkConnectorConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:40157/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 11:32:57,246] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:32:57,246] INFO Using unsecured connection to [http://localhost:40157]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:32:57,274] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 1 (__consumer_offsets-4) (reason: Leader connect-2-69c628cd-ea26-4b90-b79c-86dc7fc7003d re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,276] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 2 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,279] INFO [GroupCoordinator 0]: Assignment received from leader connect-2-69c628cd-ea26-4b90-b79c-86dc7fc7003d for group connect-integration-test-elasticsearch-it-connect-cluster for generation 2. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,285] INFO [es-connector|worker] ElasticsearchSinkConnectorConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:40157/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 11:32:57,307] ERROR Could not check connector state info. (io.confluent.connect.elasticsearch.integration.BaseConnectorIT:92)
[2025-10-07 11:32:57,310] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 2 (__consumer_offsets-4) (reason: Leader connect-2-69c628cd-ea26-4b90-b79c-86dc7fc7003d re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,311] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 3 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,313] INFO [GroupCoordinator 0]: Assignment received from leader connect-2-69c628cd-ea26-4b90-b79c-86dc7fc7003d for group connect-integration-test-elasticsearch-it-connect-cluster for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,319] INFO [es-connector|task-0] Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:32:57,319] INFO [es-connector|task-0] ElasticsearchSinkTaskConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:40157/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 0
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:32:57,319] INFO [es-connector|task-0] Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:32:57,320] DEBUG [es-connector|task-0] Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:32:57,320] INFO [es-connector|task-0] Using unsecured connection to [http://localhost:40157]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:32:57,328] INFO [es-connector|task-0] Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:32:57,337] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-es-connector in Empty state. Created a new member id connector-consumer-es-connector-0-8f7e0732-717c-4f1d-b1c9-d9b88b87332f and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,340] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member connector-consumer-es-connector-0-8f7e0732-717c-4f1d-b1c9-d9b88b87332f with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,342] INFO [GroupCoordinator 0]: Stabilized group connect-es-connector generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,349] INFO [GroupCoordinator 0]: Assignment received from leader connector-consumer-es-connector-0-8f7e0732-717c-4f1d-b1c9-d9b88b87332f for group connect-es-connector for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,425] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:57,426] INFO [es-connector|task-0] Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:32:57,429] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:57,430] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:32:57,435] WARN [es-connector|task-0] Failed to execute bulk request due to org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:40157], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
{
  "name" : "KafkaESClusterNodeold_1",
  "cluster_name" : "KafkaESCluster",
  "cluster_uuid" : "83EJmDNrRVirBWcZDgs9ew",
  "tagline" : "You Know, for Search",
  "version" : {
    "number" : "7.16.3",
    "build_hash" : "83EJmDNrRVirBWcZDgs9ew",
    "build_date" : "2018-04-12T16:25:14.838Z",
    "build_snapshot" : "false",
    "lucene_version" : "6.6.1",
    "minimum_wire_compatibility_version" : "1.1.1",
    "minimum_index_compatibility_version" : "2.2.2"
  }
}. Retrying attempt (1/3) after backoff of 8 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:32:57,544] DEBUG [es-connector|task-0] preCommitting offsets {test-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:32:57,547] DEBUG [es-connector|task-0] Stopping Elasticsearch client. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:146)
[2025-10-07 11:32:57,553] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 1 (__consumer_offsets-23) (reason: Removing member connector-consumer-es-connector-0-8f7e0732-717c-4f1d-b1c9-d9b88b87332f on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,554] INFO [GroupCoordinator 0]: Group connect-es-connector with generation 2 is now empty (__consumer_offsets-23) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,555] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connector-consumer-es-connector-0-8f7e0732-717c-4f1d-b1c9-d9b88b87332f, groupInstanceId=None, clientId=connector-consumer-es-connector-0, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group connect-es-connector through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,559] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 3 (__consumer_offsets-4) (reason: Removing member connect-2-69c628cd-ea26-4b90-b79c-86dc7fc7003d on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,559] INFO [GroupCoordinator 0]: Group connect-integration-test-elasticsearch-it-connect-cluster with generation 4 is now empty (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,561] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connect-2-69c628cd-ea26-4b90-b79c-86dc7fc7003d, groupInstanceId=None, clientId=connect-2, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(sessioned, compatible, default)) has left group connect-integration-test-elasticsearch-it-connect-cluster through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,569] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer:66)
[2025-10-07 11:32:57,569] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer:66)
[2025-10-07 11:32:57,570] INFO [Controller id=0] Shutting down broker 0 (kafka.controller.KafkaController:66)
[2025-10-07 11:32:57,572] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:32:57,573] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer:66)
[2025-10-07 11:32:57,574] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:32:57,574] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:32:57,574] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:32:57,574] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 11:32:57,577] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 11:32:57,578] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 11:32:57,583] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 11:32:57,584] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:57,695] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:57,695] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:57,696] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis:66)
[2025-10-07 11:32:57,696] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:57,895] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:57,895] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:57,896] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:32:57,896] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager:66)
[2025-10-07 11:32:57,896] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:32:57,896] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:32:57,896] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:32:57,898] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:32:57,898] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:57,898] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,095] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,095] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,096] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,141] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,141] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,142] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:32:58,142] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager:66)
[2025-10-07 11:32:58,142] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:32:58,142] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:32:58,142] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:32:58,142] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:32:58,142] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:32:58,142] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 11:32:58,142] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 11:32:58,142] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,166] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,166] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,166] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,295] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,295] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,296] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,496] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,496] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,496] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,696] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,696] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:32:58,702] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager:66)
[2025-10-07 11:32:58,702] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:58,703] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:58,703] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:58,705] INFO Broker to controller channel manager for alterIsr shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 11:32:58,705] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:58,705] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:58,705] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:32:58,707] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 11:32:58,707] INFO Shutting down. (kafka.log.LogManager:66)
[2025-10-07 11:32:58,707] INFO Shutting down the log cleaner. (kafka.log.LogCleaner:66)
[2025-10-07 11:32:58,707] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner:66)
[2025-10-07 11:32:58,707] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner:66)
[2025-10-07 11:32:58,707] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner:66)
[2025-10-07 11:32:58,715] INFO [ProducerStateManager partition=test-0] Wrote producer snapshot at offset 4 with 1 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:32:58,736] INFO [ProducerStateManager partition=__consumer_offsets-23] Wrote producer snapshot at offset 3 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:32:58,743] INFO [ProducerStateManager partition=connect-config-topic-elasticsearch-it-connect-cluster-0] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:32:58,750] INFO [ProducerStateManager partition=__consumer_offsets-4] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:32:58,758] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-2] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:32:58,767] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-3] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:32:58,780] INFO Shutdown complete. (kafka.log.LogManager:66)
[2025-10-07 11:32:58,781] INFO [ControllerEventThread controllerId=0] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:32:58,781] INFO [ControllerEventThread controllerId=0] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:32:58,781] INFO [ControllerEventThread controllerId=0] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:32:58,782] INFO [PartitionStateMachine controllerId=0] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:32:58,782] INFO [ReplicaStateMachine controllerId=0] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:32:58,782] INFO [RequestSendThread controllerId=0] Shutting down (kafka.controller.RequestSendThread:66)
[2025-10-07 11:32:58,782] INFO [RequestSendThread controllerId=0] Stopped (kafka.controller.RequestSendThread:66)
[2025-10-07 11:32:58,782] INFO [RequestSendThread controllerId=0] Shutdown completed (kafka.controller.RequestSendThread:66)
[2025-10-07 11:32:58,783] INFO [Controller id=0] Resigned (kafka.controller.KafkaController:66)
[2025-10-07 11:32:58,783] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:32:58,784] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:32:58,784] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:32:58,784] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:32:58,888] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:32:58,888] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:59,291] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:59,291] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:32:59,291] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:00,291] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:00,291] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:00,291] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:01,291] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:01,291] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:01,291] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:02,291] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:02,291] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:02,292] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer:66)
[2025-10-07 11:33:02,313] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer:66)
[2025-10-07 11:33:02,314] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats:66)
[2025-10-07 11:33:02,314] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer:66)
[2025-10-07 11:33:02,346] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster6047051280357722445
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:38629
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:33:02,348] INFO starting (kafka.server.KafkaServer:66)
[2025-10-07 11:33:02,349] INFO Connecting to zookeeper on 127.0.0.1:38629 (kafka.server.KafkaServer:66)
[2025-10-07 11:33:02,349] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:38629. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:02,349] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:02,353] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:02,369] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:02,371] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 11:33:02,371] INFO Cleared cache (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 11:33:02,374] INFO Cluster ID = Y-mH1a5OSo-69Lj7Qh8prQ (kafka.server.KafkaServer:66)
[2025-10-07 11:33:02,375] WARN No meta.properties file under dir /tmp/EmbeddedKafkaCluster6047051280357722445/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2025-10-07 11:33:02,377] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster6047051280357722445
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:38629
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:33:02,379] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster6047051280357722445
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:38629
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:33:02,386] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:02,388] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:02,392] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:02,392] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:02,393] INFO Loading logs from log dirs ArraySeq(/tmp/EmbeddedKafkaCluster6047051280357722445) (kafka.log.LogManager:66)
[2025-10-07 11:33:02,394] INFO Attempting recovery for all logs in /tmp/EmbeddedKafkaCluster6047051280357722445 since no clean shutdown file was found (kafka.log.LogManager:66)
[2025-10-07 11:33:02,394] INFO Loaded 0 logs in 0ms. (kafka.log.LogManager:66)
[2025-10-07 11:33:02,394] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:66)
[2025-10-07 11:33:02,394] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:66)
[2025-10-07 11:33:02,395] INFO Starting the log cleaner (kafka.log.LogCleaner:66)
[2025-10-07 11:33:02,405] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner:66)
[2025-10-07 11:33:02,407] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:02,415] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas:66)
[2025-10-07 11:33:02,415] INFO Awaiting socket connections on localhost:34965. (kafka.network.Acceptor:66)
[2025-10-07 11:33:02,417] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 11:33:02,418] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:02,419] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:02,420] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:02,421] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:02,424] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:02,426] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:02,428] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:02,429] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1759836782428,1759836782428,1,0,0,72057689369149440,204,0,25
 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:02,429] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:34965, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:02,439] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:02,440] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:02,441] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:02,442] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:02,442] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:02,443] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:02,444] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:02,444] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:02,445] INFO [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,445] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:02,446] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:02,446] INFO [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{}) (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,447] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:02,448] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 11:33:02,450] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:02,450] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 11:33:02,450] INFO [Controller id=0] Registering handlers (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,451] INFO [Controller id=0] Deleting log dir event notifications (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,452] INFO [Controller id=0] Deleting isr change notifications (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,452] INFO [Controller id=0] Initializing controller context (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,452] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:02,453] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 11:33:02,453] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:02,454] INFO [KafkaServer id=0] started (kafka.server.KafkaServer:66)
[2025-10-07 11:33:02,454] INFO [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25) (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,457] INFO [Controller id=0] Currently active brokers in the cluster: Set(0) (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,457] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:02,457] INFO [Controller id=0] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,457] INFO [Controller id=0] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,457] INFO [Controller id=0] Fetching topic deletions in progress (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,458] INFO [Controller id=0] List of topics to be deleted:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,458] INFO [Controller id=0] List of topics ineligible for deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,458] INFO [Controller id=0] Initializing topic deletion manager (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,458] INFO [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager:66)
[2025-10-07 11:33:02,459] INFO [Controller id=0] Sending update metadata request (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,459] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:02,460] INFO [ReplicaStateMachine controllerId=0] Initializing replica state (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:02,460] INFO [ReplicaStateMachine controllerId=0] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:02,460] INFO [ReplicaStateMachine controllerId=0] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:02,460] INFO [PartitionStateMachine controllerId=0] Initializing partition state (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:33:02,460] INFO [RequestSendThread controllerId=0] Controller 0 connected to localhost:34965 (id: 0 rack: null) for sending state change requests (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:02,460] INFO [PartitionStateMachine controllerId=0] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:33:02,460] INFO [Controller id=0] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,461] INFO [Controller id=0] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,461] INFO [Controller id=0] Partitions that completed preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,461] INFO [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,461] INFO [Controller id=0] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,461] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,462] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController:66)
[2025-10-07 11:33:02,507] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker localhost:34965 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:02,519] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:34965 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:02,524] INFO [Controller id=0] Acquired new producerId block ProducerIdsBlock{brokerId=0, producerIdStart=0, producerIdLen=1000} by writing to Zk with path version 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:04,180] INFO Creating topic connect-offset-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:04,184] INFO [Controller id=0] New topics: [Set(connect-offset-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-offset-topic-elasticsearch-it-connect-cluster,Some(q6oGxsT-SYO2JA6-dnT1Cg),HashMap(connect-offset-topic-elasticsearch-it-connect-cluster-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:04,184] INFO [Controller id=0] New partition creation callback for connect-offset-topic-elasticsearch-it-connect-cluster-20,connect-offset-topic-elasticsearch-it-connect-cluster-7,connect-offset-topic-elasticsearch-it-connect-cluster-0,connect-offset-topic-elasticsearch-it-connect-cluster-12,connect-offset-topic-elasticsearch-it-connect-cluster-13,connect-offset-topic-elasticsearch-it-connect-cluster-5,connect-offset-topic-elasticsearch-it-connect-cluster-21,connect-offset-topic-elasticsearch-it-connect-cluster-14,connect-offset-topic-elasticsearch-it-connect-cluster-6,connect-offset-topic-elasticsearch-it-connect-cluster-1,connect-offset-topic-elasticsearch-it-connect-cluster-19,connect-offset-topic-elasticsearch-it-connect-cluster-17,connect-offset-topic-elasticsearch-it-connect-cluster-16,connect-offset-topic-elasticsearch-it-connect-cluster-23,connect-offset-topic-elasticsearch-it-connect-cluster-10,connect-offset-topic-elasticsearch-it-connect-cluster-3,connect-offset-topic-elasticsearch-it-connect-cluster-15,connect-offset-topic-elasticsearch-it-connect-cluster-22,connect-offset-topic-elasticsearch-it-connect-cluster-24,connect-offset-topic-elasticsearch-it-connect-cluster-18,connect-offset-topic-elasticsearch-it-connect-cluster-2,connect-offset-topic-elasticsearch-it-connect-cluster-4,connect-offset-topic-elasticsearch-it-connect-cluster-8,connect-offset-topic-elasticsearch-it-connect-cluster-9,connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:04,184] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,184] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,184] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,184] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,184] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,184] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,184] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,184] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,184] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,185] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:04,186] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,200] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,201] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 25 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:04,201] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 25 partitions (state.change.logger:66)
[2025-10-07 11:33:04,201] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:04,202] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
Oct 07, 2025 11:33:04 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
[2025-10-07 11:33:04,204] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-offset-topic-elasticsearch-it-connect-cluster-20, connect-offset-topic-elasticsearch-it-connect-cluster-7, connect-offset-topic-elasticsearch-it-connect-cluster-0, connect-offset-topic-elasticsearch-it-connect-cluster-12, connect-offset-topic-elasticsearch-it-connect-cluster-13, connect-offset-topic-elasticsearch-it-connect-cluster-5, connect-offset-topic-elasticsearch-it-connect-cluster-21, connect-offset-topic-elasticsearch-it-connect-cluster-14, connect-offset-topic-elasticsearch-it-connect-cluster-6, connect-offset-topic-elasticsearch-it-connect-cluster-1, connect-offset-topic-elasticsearch-it-connect-cluster-19, connect-offset-topic-elasticsearch-it-connect-cluster-17, connect-offset-topic-elasticsearch-it-connect-cluster-16, connect-offset-topic-elasticsearch-it-connect-cluster-23, connect-offset-topic-elasticsearch-it-connect-cluster-10, connect-offset-topic-elasticsearch-it-connect-cluster-3, connect-offset-topic-elasticsearch-it-connect-cluster-15, connect-offset-topic-elasticsearch-it-connect-cluster-22, connect-offset-topic-elasticsearch-it-connect-cluster-24, connect-offset-topic-elasticsearch-it-connect-cluster-18, connect-offset-topic-elasticsearch-it-connect-cluster-2, connect-offset-topic-elasticsearch-it-connect-cluster-4, connect-offset-topic-elasticsearch-it-connect-cluster-8, connect-offset-topic-elasticsearch-it-connect-cluster-9, connect-offset-topic-elasticsearch-it-connect-cluster-11) (kafka.server.ReplicaFetcherManager:66)
Oct 07, 2025 11:33:04 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
[2025-10-07 11:33:04,204] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 25 partitions (state.change.logger:66)
Oct 07, 2025 11:33:04 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Oct 07, 2025 11:33:04 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
[2025-10-07 11:33:04,207] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-10, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,208] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-10 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,208] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,208] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,208] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,220] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-14, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,221] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-14 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,221] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,221] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,221] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,232] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-18, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,232] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-18 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,232] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,232] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,232] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 11:33:04 AM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

[2025-10-07 11:33:04,243] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,244] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,244] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,244] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,244] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,246] INFO Creating topic test with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:04,253] INFO [Controller id=0] New topics: [Set(test)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(test,Some(kE6m_BsxSU-mo_NUtB3o4w),Map(test-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:04,253] INFO [Controller id=0] New partition creation callback for test-0 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:04,253] INFO [Controller id=0 epoch=1] Changed partition test-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,253] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:04,254] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:04,254] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-22, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,254] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-22 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,254] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,254] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,254] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,257] INFO [Controller id=0 epoch=1] Changed partition test-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,257] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:04,257] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:04,258] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:04,266] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-7, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,266] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-7 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,266] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,266] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,266] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,277] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-11, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,277] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-11 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,277] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,278] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,278] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,289] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-15, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,289] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-15 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,289] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,289] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,289] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,299] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,299] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,299] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,299] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,299] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,310] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-19, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,310] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-19 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,310] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,310] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,310] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,321] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,321] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,321] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,321] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,321] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,331] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-23, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,332] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-23 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,332] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,332] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,332] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,343] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-8, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,343] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-8 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,343] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,343] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,343] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,354] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-12, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,354] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-12 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,354] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,354] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,354] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,361] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-16, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,361] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-16 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,362] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,362] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,362] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,372] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,372] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,372] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,372] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,372] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,383] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-20, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,383] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-20 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,383] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,383] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,383] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,393] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-5, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,393] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-5 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,393] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,393] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,394] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,404] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-24, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,404] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-24 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,404] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,404] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,404] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,415] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-9, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,415] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-9 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,415] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,415] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,415] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,426] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-13, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,426] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-13 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,426] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,426] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,426] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,433] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-17, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,433] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-17 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,433] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,433] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,433] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,444] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,444] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,444] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,444] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,444] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,455] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-21, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,455] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-21 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,455] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,455] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,455] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,465] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-6, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,466] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-offset-topic-elasticsearch-it-connect-cluster-6 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,466] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,466] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,466] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,475] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 11:33:04,476] INFO [Broker id=0] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger:66)
[2025-10-07 11:33:04,478] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:04,479] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:04,479] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:04,482] INFO [LogLoader partition=test-0, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,483] INFO Created log for partition test-0 in /tmp/EmbeddedKafkaCluster6047051280357722445/test-0 with properties {} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,483] INFO [Partition test-0 broker=0] No checkpointed highwatermark is found for partition test-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,483] INFO [Partition test-0 broker=0] Log loaded for partition test-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,483] INFO [Broker id=0] Leader test-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,492] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:04,497] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger:66)
[2025-10-07 11:33:04,500] INFO Creating topic connect-storage-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:04,503] INFO [Controller id=0] New topics: [Set(connect-storage-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-storage-topic-elasticsearch-it-connect-cluster,Some(K2Cqek6pSke1G_-y7u1T5Q),HashMap(connect-storage-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:04,507] INFO [Controller id=0] New partition creation callback for connect-storage-topic-elasticsearch-it-connect-cluster-2,connect-storage-topic-elasticsearch-it-connect-cluster-0,connect-storage-topic-elasticsearch-it-connect-cluster-3,connect-storage-topic-elasticsearch-it-connect-cluster-4,connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:04,507] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,507] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,507] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,508] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,508] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,508] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:04,508] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:04,526] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,527] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,527] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,527] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,527] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,527] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:04,527] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:04,527] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:04,527] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:04,529] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-storage-topic-elasticsearch-it-connect-cluster-2, connect-storage-topic-elasticsearch-it-connect-cluster-0, connect-storage-topic-elasticsearch-it-connect-cluster-3, connect-storage-topic-elasticsearch-it-connect-cluster-4, connect-storage-topic-elasticsearch-it-connect-cluster-1) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:04,529] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:04,530] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,531] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-storage-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,531] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,531] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,531] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,542] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,543] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-storage-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,543] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,543] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,543] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,554] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,554] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-storage-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,554] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,554] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,554] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,565] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,565] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-storage-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,565] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,565] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,565] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,576] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,576] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-storage-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,576] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,576] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,576] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,586] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:04,587] INFO [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6 (state.change.logger:66)
[2025-10-07 11:33:04,603] INFO Creating topic connect-config-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:04,607] INFO [Controller id=0] New topics: [Set(connect-config-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-config-topic-elasticsearch-it-connect-cluster,Some(FGe1_jvORf-efNJbbHLWcg),Map(connect-config-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:04,607] INFO [Controller id=0] New partition creation callback for connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:04,607] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,607] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:04,607] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:04,609] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,609] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:04,609] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:04,609] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:04,610] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:04,610] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(connect-config-topic-elasticsearch-it-connect-cluster-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:04,610] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:04,612] INFO [LogLoader partition=connect-config-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,612] INFO Created log for partition connect-config-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster6047051280357722445/connect-config-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,612] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,613] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-config-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,613] INFO [Broker id=0] Leader connect-config-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,622] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:04,623] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger:66)
[2025-10-07 11:33:04,650] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:04,654] INFO [Controller id=0] New topics: [HashSet(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(yYbe5dKpSXm60oIvRxUGHg),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:04,654] INFO [Controller id=0] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:04,654] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,655] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:04,656] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,669] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:04,670] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 50 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:04,671] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:04,671] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:04,671] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:04,676] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:04,676] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 0 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:04,679] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,679] INFO Created log for partition __consumer_offsets-3 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,679] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,679] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,679] INFO [Broker id=0] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,690] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,691] INFO Created log for partition __consumer_offsets-18 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,691] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,691] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,691] INFO [Broker id=0] Leader __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,702] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,702] INFO Created log for partition __consumer_offsets-41 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,702] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,702] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,703] INFO [Broker id=0] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,713] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,713] INFO Created log for partition __consumer_offsets-10 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,713] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,713] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,713] INFO [Broker id=0] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,720] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,720] INFO Created log for partition __consumer_offsets-33 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,720] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,720] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,720] INFO [Broker id=0] Leader __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,731] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,731] INFO Created log for partition __consumer_offsets-48 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,731] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,731] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,731] INFO [Broker id=0] Leader __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,742] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,743] INFO Created log for partition __consumer_offsets-19 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,743] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,743] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,743] INFO [Broker id=0] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,753] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,753] INFO Created log for partition __consumer_offsets-34 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,753] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,753] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,753] INFO [Broker id=0] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,764] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,764] INFO Created log for partition __consumer_offsets-4 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,764] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,764] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,764] INFO [Broker id=0] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,775] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,775] INFO Created log for partition __consumer_offsets-11 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,775] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,775] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,775] INFO [Broker id=0] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,786] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,786] INFO Created log for partition __consumer_offsets-26 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,786] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,786] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,786] INFO [Broker id=0] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,797] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,797] INFO Created log for partition __consumer_offsets-49 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,797] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,797] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,797] INFO [Broker id=0] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,808] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,808] INFO Created log for partition __consumer_offsets-39 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,808] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,808] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,808] INFO [Broker id=0] Leader __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,818] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,819] INFO Created log for partition __consumer_offsets-9 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,819] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,819] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,819] INFO [Broker id=0] Leader __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,830] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,831] INFO Created log for partition __consumer_offsets-24 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,831] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,831] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,831] INFO [Broker id=0] Leader __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,842] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,843] INFO Created log for partition __consumer_offsets-31 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,843] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,843] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,843] INFO [Broker id=0] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,853] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,853] INFO Created log for partition __consumer_offsets-46 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,854] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,854] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,854] INFO [Broker id=0] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,865] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,865] INFO Created log for partition __consumer_offsets-1 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,865] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,865] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,865] INFO [Broker id=0] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,876] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,876] INFO Created log for partition __consumer_offsets-16 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,876] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,876] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,876] INFO [Broker id=0] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,883] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,883] INFO Created log for partition __consumer_offsets-2 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,883] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,883] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,883] INFO [Broker id=0] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,894] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,894] INFO Created log for partition __consumer_offsets-25 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,894] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,894] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,895] INFO [Broker id=0] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,906] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,906] INFO Created log for partition __consumer_offsets-40 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,906] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,906] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,906] INFO [Broker id=0] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,914] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,914] INFO Created log for partition __consumer_offsets-47 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,914] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,914] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,914] INFO [Broker id=0] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,921] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,921] INFO Created log for partition __consumer_offsets-17 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,921] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,921] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,921] INFO [Broker id=0] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,932] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,932] INFO Created log for partition __consumer_offsets-32 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,932] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,932] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,932] INFO [Broker id=0] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,943] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,943] INFO Created log for partition __consumer_offsets-37 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,943] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,943] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,943] INFO [Broker id=0] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,954] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,955] INFO Created log for partition __consumer_offsets-7 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,955] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,955] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,955] INFO [Broker id=0] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,965] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,966] INFO Created log for partition __consumer_offsets-22 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,966] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,966] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,966] INFO [Broker id=0] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,977] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,977] INFO Created log for partition __consumer_offsets-29 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,977] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,977] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,977] INFO [Broker id=0] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:04,989] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:04,989] INFO Created log for partition __consumer_offsets-44 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:04,989] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,989] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:04,989] INFO [Broker id=0] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,000] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,000] INFO Created log for partition __consumer_offsets-14 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,000] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,000] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,000] INFO [Broker id=0] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,010] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,011] INFO Created log for partition __consumer_offsets-23 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,011] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,011] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,011] INFO [Broker id=0] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,022] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,022] INFO Created log for partition __consumer_offsets-38 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,022] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,022] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,022] INFO [Broker id=0] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,032] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,033] INFO Created log for partition __consumer_offsets-8 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,033] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,033] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,033] INFO [Broker id=0] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,043] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,043] INFO Created log for partition __consumer_offsets-45 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,043] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,043] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,043] INFO [Broker id=0] Leader __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,053] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,054] INFO Created log for partition __consumer_offsets-15 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,054] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,054] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,054] INFO [Broker id=0] Leader __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,064] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,065] INFO Created log for partition __consumer_offsets-30 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,065] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,065] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,065] INFO [Broker id=0] Leader __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,075] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,075] INFO Created log for partition __consumer_offsets-0 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,075] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,075] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,075] INFO [Broker id=0] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,086] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,086] INFO Created log for partition __consumer_offsets-35 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,086] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,086] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,086] INFO [Broker id=0] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,096] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,097] INFO Created log for partition __consumer_offsets-5 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,097] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,097] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,097] INFO [Broker id=0] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,108] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,108] INFO Created log for partition __consumer_offsets-20 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,108] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,108] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,108] INFO [Broker id=0] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,119] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,119] INFO Created log for partition __consumer_offsets-27 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,119] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,119] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,119] INFO [Broker id=0] Leader __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,129] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,129] INFO Created log for partition __consumer_offsets-42 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,129] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,129] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,129] INFO [Broker id=0] Leader __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,139] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,140] INFO Created log for partition __consumer_offsets-12 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,140] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,140] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,140] INFO [Broker id=0] Leader __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,151] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,151] INFO Created log for partition __consumer_offsets-21 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,151] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,151] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,151] INFO [Broker id=0] Leader __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,162] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,162] INFO Created log for partition __consumer_offsets-36 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,162] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,162] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,162] INFO [Broker id=0] Leader __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,172] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,172] INFO Created log for partition __consumer_offsets-6 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,172] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,172] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,173] INFO [Broker id=0] Leader __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,184] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,184] INFO Created log for partition __consumer_offsets-43 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,184] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,184] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,184] INFO [Broker id=0] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,195] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,195] INFO Created log for partition __consumer_offsets-13 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,195] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,195] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,195] INFO [Broker id=0] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,206] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/EmbeddedKafkaCluster6047051280357722445] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:05,206] INFO Created log for partition __consumer_offsets-28 in /tmp/EmbeddedKafkaCluster6047051280357722445/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:05,206] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,206] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:05,206] INFO [Broker id=0] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:05,216] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,216] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,217] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,217] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,217] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,218] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,218] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,218] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,218] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,218] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,218] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,218] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,218] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,218] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,218] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,218] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,218] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,218] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,218] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,218] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,218] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,218] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,218] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,218] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,218] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,219] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,220] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,221] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,221] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,221] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,221] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,221] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,221] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,219] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,221] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,221] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,221] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,221] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,221] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,221] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,221] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,221] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:05,222] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,222] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,222] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,222] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,222] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,222] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,223] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,223] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,223] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,223] INFO [Broker id=0] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 10 (state.change.logger:66)
[2025-10-07 11:33:05,223] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,223] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,223] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,223] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,223] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,223] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 4 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,225] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 4 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,225] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,225] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,225] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,225] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,225] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,225] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,225] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:05,260] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-integration-test-elasticsearch-it-connect-cluster in Empty state. Created a new member id connect-3-936d3574-a55e-4e5b-8548-9b0c25605d00 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,266] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member connect-3-936d3574-a55e-4e5b-8548-9b0c25605d00 with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,268] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 1 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,272] INFO [GroupCoordinator 0]: Assignment received from leader connect-3-936d3574-a55e-4e5b-8548-9b0c25605d00 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,282] INFO ElasticsearchSinkConnectorConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:42937/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 11:33:05,283] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:33:05,283] INFO Using unsecured connection to [http://localhost:42937]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:33:05,301] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 1 (__consumer_offsets-4) (reason: Leader connect-3-936d3574-a55e-4e5b-8548-9b0c25605d00 re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,301] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 2 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,304] INFO [GroupCoordinator 0]: Assignment received from leader connect-3-936d3574-a55e-4e5b-8548-9b0c25605d00 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 2. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,309] INFO [es-connector|worker] ElasticsearchSinkConnectorConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:42937/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 11:33:05,323] ERROR Could not check connector state info. (io.confluent.connect.elasticsearch.integration.BaseConnectorIT:92)
[2025-10-07 11:33:05,328] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 2 (__consumer_offsets-4) (reason: Leader connect-3-936d3574-a55e-4e5b-8548-9b0c25605d00 re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,328] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 3 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,331] INFO [GroupCoordinator 0]: Assignment received from leader connect-3-936d3574-a55e-4e5b-8548-9b0c25605d00 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,336] INFO [es-connector|task-0] Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:33:05,337] INFO [es-connector|task-0] ElasticsearchSinkTaskConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:42937/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 0
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:33:05,337] INFO [es-connector|task-0] Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:33:05,337] DEBUG [es-connector|task-0] Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:33:05,337] INFO [es-connector|task-0] Using unsecured connection to [http://localhost:42937]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:33:05,346] INFO [es-connector|task-0] Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:33:05,352] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-es-connector in Empty state. Created a new member id connector-consumer-es-connector-0-d5c77c81-f37c-4873-b93d-c3a4f03df4b2 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,353] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member connector-consumer-es-connector-0-d5c77c81-f37c-4873-b93d-c3a4f03df4b2 with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,354] INFO [GroupCoordinator 0]: Stabilized group connect-es-connector generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,359] INFO [GroupCoordinator 0]: Assignment received from leader connector-consumer-es-connector-0-d5c77c81-f37c-4873-b93d-c3a4f03df4b2 for group connect-es-connector for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,440] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:05,440] INFO [es-connector|task-0] Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:33:05,443] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:05,443] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:05,445] WARN [es-connector|task-0] Failed to execute bulk request due to null. Retrying attempt (1/3) after backoff of 0 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:33:05,446] WARN [es-connector|task-0] Failed to execute bulk request due to null. Retrying attempt (2/3) after backoff of 12 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:33:05,459] ERROR [es-connector|task-0] Failed to execute bulk request due to 'ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]
	at org.elasticsearch.rest.BytesRestResponse.errorFromXContent(BytesRestResponse.java:178)
	at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2484)
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
	Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:42937], URI [/_bulk?timeout=1m], status line [HTTP/1.1 429 Too Many Requests]
{
  "error": {
    "type": "circuit_breaking_exception",
    "reason": "Data too large",
    "bytes_wanted": 123848638,
    "bytes_limit": 123273216,
    "durability": "TRANSIENT"
  },
  "status": 429
}
		at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
		at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
		at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
		... 13 more
[2025-10-07 11:33:05,460] WARN [es-connector|task-0] Bulk request 1 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]
	at org.elasticsearch.rest.BytesRestResponse.errorFromXContent(BytesRestResponse.java:178)
	at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2484)
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:42937], URI [/_bulk?timeout=1m], status line [HTTP/1.1 429 Too Many Requests]
{
  "error": {
    "type": "circuit_breaking_exception",
    "reason": "Data too large",
    "bytes_wanted": 123848638,
    "bytes_limit": 123273216,
    "durability": "TRANSIENT"
  },
  "status": 429
}
		at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
		at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
		at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
		... 13 more
[2025-10-07 11:33:05,461] ERROR [es-connector|task-0] WorkerSinkTask{id=es-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Bulk request failed (org.apache.kafka.connect.runtime.WorkerSinkTask:629)
org.apache.kafka.connect.errors.ConnectException: Bulk request failed
	at io.confluent.connect.elasticsearch.ElasticsearchClient$2.afterBulk(ElasticsearchClient.java:455)
	at org.elasticsearch.action.bulk.BulkRequestHandler$1.onFailure(BulkRequestHandler.java:64)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.ActionListener$RunAfterActionListener.onFailure(ActionListener.java:350)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onFailure(Retry.java:123)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:227)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	... 5 more
Caused by: ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]
	at org.elasticsearch.rest.BytesRestResponse.errorFromXContent(BytesRestResponse.java:178)
	at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2484)
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:42937], URI [/_bulk?timeout=1m], status line [HTTP/1.1 429 Too Many Requests]
{
  "error": {
    "type": "circuit_breaking_exception",
    "reason": "Data too large",
    "bytes_wanted": 123848638,
    "bytes_limit": 123273216,
    "durability": "TRANSIENT"
  },
  "status": 429
}
		at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
		at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
		at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
		... 13 more
[2025-10-07 11:33:05,463] DEBUG [es-connector|task-0] Tried to flush data to Elasticsearch, but BulkProcessor is already closed. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:137)
java.lang.IllegalStateException: bulk process already closed
	at org.elasticsearch.action.bulk.BulkProcessor.ensureOpen(BulkProcessor.java:454)
	at org.elasticsearch.action.bulk.BulkProcessor.flush(BulkProcessor.java:571)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.flush(ElasticsearchClient.java:303)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.preCommit(ElasticsearchSinkTask.java:135)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:404)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.closePartitions(WorkerSinkTask.java:666)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.closeAllPartitions(WorkerSinkTask.java:661)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:204)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[2025-10-07 11:33:05,463] DEBUG [es-connector|task-0] preCommitting offsets {} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:33:05,463] ERROR [es-connector|task-0] WorkerSinkTask{id=es-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:195)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:631)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:333)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:234)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:203)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.kafka.connect.errors.ConnectException: Bulk request failed
	at io.confluent.connect.elasticsearch.ElasticsearchClient$2.afterBulk(ElasticsearchClient.java:455)
	at org.elasticsearch.action.bulk.BulkRequestHandler$1.onFailure(BulkRequestHandler.java:64)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.ActionListener$RunAfterActionListener.onFailure(ActionListener.java:350)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onFailure(Retry.java:123)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:227)
	... 5 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	... 5 more
Caused by: ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]
	at org.elasticsearch.rest.BytesRestResponse.errorFromXContent(BytesRestResponse.java:178)
	at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2484)
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:42937], URI [/_bulk?timeout=1m], status line [HTTP/1.1 429 Too Many Requests]
{
  "error": {
    "type": "circuit_breaking_exception",
    "reason": "Data too large",
    "bytes_wanted": 123848638,
    "bytes_limit": 123273216,
    "durability": "TRANSIENT"
  },
  "status": 429
}
		at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
		at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
		at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
		... 13 more
[2025-10-07 11:33:05,463] DEBUG [es-connector|task-0] Stopping Elasticsearch client. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:146)
[2025-10-07 11:33:05,464] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 1 (__consumer_offsets-23) (reason: Removing member connector-consumer-es-connector-0-d5c77c81-f37c-4873-b93d-c3a4f03df4b2 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,465] INFO [GroupCoordinator 0]: Group connect-es-connector with generation 2 is now empty (__consumer_offsets-23) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,466] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connector-consumer-es-connector-0-d5c77c81-f37c-4873-b93d-c3a4f03df4b2, groupInstanceId=None, clientId=connector-consumer-es-connector-0, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group connect-es-connector through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,578] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 3 (__consumer_offsets-4) (reason: Removing member connect-3-936d3574-a55e-4e5b-8548-9b0c25605d00 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,579] INFO [GroupCoordinator 0]: Group connect-integration-test-elasticsearch-it-connect-cluster with generation 4 is now empty (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,583] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connect-3-936d3574-a55e-4e5b-8548-9b0c25605d00, groupInstanceId=None, clientId=connect-3, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(sessioned, compatible, default)) has left group connect-integration-test-elasticsearch-it-connect-cluster through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,591] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer:66)
[2025-10-07 11:33:05,592] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer:66)
[2025-10-07 11:33:05,593] INFO [Controller id=0] Shutting down broker 0 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:05,594] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:05,594] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer:66)
[2025-10-07 11:33:05,595] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:05,595] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:05,595] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:05,595] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:05,596] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:05,596] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 11:33:05,596] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 11:33:05,597] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:05,749] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:05,749] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:05,749] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis:66)
[2025-10-07 11:33:05,750] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:05,949] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:05,949] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:05,950] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:05,950] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager:66)
[2025-10-07 11:33:05,950] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:05,950] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:05,950] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:05,952] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:05,952] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:05,952] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,150] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,150] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,150] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,154] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,154] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,154] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:06,154] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager:66)
[2025-10-07 11:33:06,154] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:06,155] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:06,155] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:06,155] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:06,155] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:06,155] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 11:33:06,155] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 11:33:06,155] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,208] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,208] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,208] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,352] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,352] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,352] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,552] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,552] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,553] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,553] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,553] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:06,560] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager:66)
[2025-10-07 11:33:06,560] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:06,560] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:06,560] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:06,563] INFO Broker to controller channel manager for alterIsr shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 11:33:06,563] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:06,563] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:06,563] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:06,565] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 11:33:06,565] INFO Shutting down. (kafka.log.LogManager:66)
[2025-10-07 11:33:06,565] INFO Shutting down the log cleaner. (kafka.log.LogCleaner:66)
[2025-10-07 11:33:06,565] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner:66)
[2025-10-07 11:33:06,565] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner:66)
[2025-10-07 11:33:06,565] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner:66)
[2025-10-07 11:33:06,573] INFO [ProducerStateManager partition=test-0] Wrote producer snapshot at offset 5 with 1 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:06,582] INFO [ProducerStateManager partition=__consumer_offsets-23] Wrote producer snapshot at offset 2 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:06,589] INFO [ProducerStateManager partition=connect-config-topic-elasticsearch-it-connect-cluster-0] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:06,596] INFO [ProducerStateManager partition=__consumer_offsets-4] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:06,609] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-2] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:06,618] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-3] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:06,631] INFO Shutdown complete. (kafka.log.LogManager:66)
[2025-10-07 11:33:06,632] INFO [ControllerEventThread controllerId=0] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:06,632] INFO [ControllerEventThread controllerId=0] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:06,632] INFO [ControllerEventThread controllerId=0] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:06,633] INFO [PartitionStateMachine controllerId=0] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:33:06,633] INFO [ReplicaStateMachine controllerId=0] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:06,633] INFO [RequestSendThread controllerId=0] Shutting down (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:06,633] INFO [RequestSendThread controllerId=0] Stopped (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:06,633] INFO [RequestSendThread controllerId=0] Shutdown completed (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:06,640] INFO [Controller id=0] Resigned (kafka.controller.KafkaController:66)
[2025-10-07 11:33:06,640] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:06,640] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:06,640] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:06,640] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:06,742] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:06,742] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:07,551] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:07,551] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:07,551] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:08,551] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:08,551] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:08,551] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:09,551] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:09,551] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:09,551] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:10,551] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:10,551] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:10,552] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer:66)
[2025-10-07 11:33:10,557] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer:66)
[2025-10-07 11:33:10,558] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats:66)
[2025-10-07 11:33:10,558] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer:66)
[2025-10-07 11:33:10,586] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster4752165901678924258
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:33697
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:33:10,587] INFO starting (kafka.server.KafkaServer:66)
[2025-10-07 11:33:10,588] INFO Connecting to zookeeper on 127.0.0.1:33697 (kafka.server.KafkaServer:66)
[2025-10-07 11:33:10,588] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:33697. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:10,588] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:10,592] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:10,606] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:10,608] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 11:33:10,608] INFO Cleared cache (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 11:33:10,611] INFO Cluster ID = 8SnJCXF3SCW82DUL7jH2Pg (kafka.server.KafkaServer:66)
[2025-10-07 11:33:10,611] WARN No meta.properties file under dir /tmp/EmbeddedKafkaCluster4752165901678924258/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2025-10-07 11:33:10,613] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster4752165901678924258
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:33697
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:33:10,615] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster4752165901678924258
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:33697
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:33:10,622] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:10,622] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:10,624] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:10,624] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:10,625] INFO Loading logs from log dirs ArraySeq(/tmp/EmbeddedKafkaCluster4752165901678924258) (kafka.log.LogManager:66)
[2025-10-07 11:33:10,625] INFO Attempting recovery for all logs in /tmp/EmbeddedKafkaCluster4752165901678924258 since no clean shutdown file was found (kafka.log.LogManager:66)
[2025-10-07 11:33:10,625] INFO Loaded 0 logs in 0ms. (kafka.log.LogManager:66)
[2025-10-07 11:33:10,625] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:66)
[2025-10-07 11:33:10,625] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:66)
[2025-10-07 11:33:10,626] INFO Starting the log cleaner (kafka.log.LogCleaner:66)
[2025-10-07 11:33:10,637] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner:66)
[2025-10-07 11:33:10,639] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:10,645] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas:66)
[2025-10-07 11:33:10,645] INFO Awaiting socket connections on localhost:42101. (kafka.network.Acceptor:66)
[2025-10-07 11:33:10,647] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 11:33:10,649] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:10,650] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:10,651] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:10,652] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:10,652] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:10,654] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:10,656] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:10,657] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1759836790656,1759836790656,1,0,0,72057689909166080,204,0,25
 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:10,657] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:42101, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:10,667] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:10,668] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:10,669] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:10,670] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:10,670] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:10,671] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:10,671] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:10,671] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:10,672] INFO [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,672] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:10,673] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:10,673] INFO [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{}) (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,674] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:10,675] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 11:33:10,676] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:10,676] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 11:33:10,676] INFO [Controller id=0] Registering handlers (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,678] INFO [Controller id=0] Deleting log dir event notifications (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,678] INFO [Controller id=0] Deleting isr change notifications (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,679] INFO [Controller id=0] Initializing controller context (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,679] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:10,680] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 11:33:10,680] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:10,680] INFO [KafkaServer id=0] started (kafka.server.KafkaServer:66)
[2025-10-07 11:33:10,680] INFO [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25) (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,683] INFO [Controller id=0] Currently active brokers in the cluster: Set(0) (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,683] INFO [Controller id=0] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,684] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:10,684] INFO [Controller id=0] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,684] INFO [Controller id=0] Fetching topic deletions in progress (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,685] INFO [Controller id=0] List of topics to be deleted:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,685] INFO [Controller id=0] List of topics ineligible for deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,685] INFO [Controller id=0] Initializing topic deletion manager (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,685] INFO [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager:66)
[2025-10-07 11:33:10,685] INFO [Controller id=0] Sending update metadata request (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,685] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:10,685] INFO [ReplicaStateMachine controllerId=0] Initializing replica state (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:10,685] INFO [ReplicaStateMachine controllerId=0] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:10,685] INFO [ReplicaStateMachine controllerId=0] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:10,686] INFO [PartitionStateMachine controllerId=0] Initializing partition state (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:33:10,686] INFO [RequestSendThread controllerId=0] Controller 0 connected to localhost:42101 (id: 0 rack: null) for sending state change requests (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:10,686] INFO [PartitionStateMachine controllerId=0] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:33:10,686] INFO [Controller id=0] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,687] INFO [Controller id=0] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,687] INFO [Controller id=0] Partitions that completed preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,687] INFO [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,687] INFO [Controller id=0] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,687] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,688] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController:66)
[2025-10-07 11:33:10,739] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker localhost:42101 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:10,749] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:42101 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:10,752] INFO [Controller id=0] Acquired new producerId block ProducerIdsBlock{brokerId=0, producerIdStart=0, producerIdLen=1000} by writing to Zk with path version 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:12,577] INFO Creating topic connect-offset-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:12,583] INFO [Controller id=0] New topics: [Set(connect-offset-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-offset-topic-elasticsearch-it-connect-cluster,Some(dUinr9cgSBa6v5y0Y6EjtA),HashMap(connect-offset-topic-elasticsearch-it-connect-cluster-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:12,583] INFO [Controller id=0] New partition creation callback for connect-offset-topic-elasticsearch-it-connect-cluster-20,connect-offset-topic-elasticsearch-it-connect-cluster-7,connect-offset-topic-elasticsearch-it-connect-cluster-0,connect-offset-topic-elasticsearch-it-connect-cluster-12,connect-offset-topic-elasticsearch-it-connect-cluster-13,connect-offset-topic-elasticsearch-it-connect-cluster-5,connect-offset-topic-elasticsearch-it-connect-cluster-21,connect-offset-topic-elasticsearch-it-connect-cluster-14,connect-offset-topic-elasticsearch-it-connect-cluster-6,connect-offset-topic-elasticsearch-it-connect-cluster-1,connect-offset-topic-elasticsearch-it-connect-cluster-19,connect-offset-topic-elasticsearch-it-connect-cluster-17,connect-offset-topic-elasticsearch-it-connect-cluster-16,connect-offset-topic-elasticsearch-it-connect-cluster-23,connect-offset-topic-elasticsearch-it-connect-cluster-10,connect-offset-topic-elasticsearch-it-connect-cluster-3,connect-offset-topic-elasticsearch-it-connect-cluster-15,connect-offset-topic-elasticsearch-it-connect-cluster-22,connect-offset-topic-elasticsearch-it-connect-cluster-24,connect-offset-topic-elasticsearch-it-connect-cluster-18,connect-offset-topic-elasticsearch-it-connect-cluster-2,connect-offset-topic-elasticsearch-it-connect-cluster-4,connect-offset-topic-elasticsearch-it-connect-cluster-8,connect-offset-topic-elasticsearch-it-connect-cluster-9,connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:12,583] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,583] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,583] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,583] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,583] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,583] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,583] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,584] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:12,585] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:12,596] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,596] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,596] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,596] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,597] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,598] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 25 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:12,598] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 25 partitions (state.change.logger:66)
[2025-10-07 11:33:12,598] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:12,598] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 11:33:12,601] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-offset-topic-elasticsearch-it-connect-cluster-20, connect-offset-topic-elasticsearch-it-connect-cluster-7, connect-offset-topic-elasticsearch-it-connect-cluster-0, connect-offset-topic-elasticsearch-it-connect-cluster-12, connect-offset-topic-elasticsearch-it-connect-cluster-13, connect-offset-topic-elasticsearch-it-connect-cluster-5, connect-offset-topic-elasticsearch-it-connect-cluster-21, connect-offset-topic-elasticsearch-it-connect-cluster-14, connect-offset-topic-elasticsearch-it-connect-cluster-6, connect-offset-topic-elasticsearch-it-connect-cluster-1, connect-offset-topic-elasticsearch-it-connect-cluster-19, connect-offset-topic-elasticsearch-it-connect-cluster-17, connect-offset-topic-elasticsearch-it-connect-cluster-16, connect-offset-topic-elasticsearch-it-connect-cluster-23, connect-offset-topic-elasticsearch-it-connect-cluster-10, connect-offset-topic-elasticsearch-it-connect-cluster-3, connect-offset-topic-elasticsearch-it-connect-cluster-15, connect-offset-topic-elasticsearch-it-connect-cluster-22, connect-offset-topic-elasticsearch-it-connect-cluster-24, connect-offset-topic-elasticsearch-it-connect-cluster-18, connect-offset-topic-elasticsearch-it-connect-cluster-2, connect-offset-topic-elasticsearch-it-connect-cluster-4, connect-offset-topic-elasticsearch-it-connect-cluster-8, connect-offset-topic-elasticsearch-it-connect-cluster-9, connect-offset-topic-elasticsearch-it-connect-cluster-11) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:12,601] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 25 partitions (state.change.logger:66)
[2025-10-07 11:33:12,603] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-10, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,604] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-10 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,605] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,605] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,605] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 11:33:12 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Oct 07, 2025 11:33:12 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Oct 07, 2025 11:33:12 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Oct 07, 2025 11:33:12 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
[2025-10-07 11:33:12,617] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-14, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,617] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-14 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,617] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,617] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,617] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,627] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-18, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,628] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-18 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,628] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,628] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,628] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,638] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,638] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,639] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,639] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,639] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 11:33:12 AM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

[2025-10-07 11:33:12,647] INFO Creating topic test with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:12,649] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-22, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,650] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-22 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,650] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,650] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,650] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,651] INFO [Controller id=0] New topics: [Set(test)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(test,Some(oIXX-gstTEGBSAnQN5j9KQ),Map(test-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:12,651] INFO [Controller id=0] New partition creation callback for test-0 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:12,651] INFO [Controller id=0 epoch=1] Changed partition test-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,651] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:12,652] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:12,655] INFO [Controller id=0 epoch=1] Changed partition test-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,655] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:12,655] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:12,655] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:12,660] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-7, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,660] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-7 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,660] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,660] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,660] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,670] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-11, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,671] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-11 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,671] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,671] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,671] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,681] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-15, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,682] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-15 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,682] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,682] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,682] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,692] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,693] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,693] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,693] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,693] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,703] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-19, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,704] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-19 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,704] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,704] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,704] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,714] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,714] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,715] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,715] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,715] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,726] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-23, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,726] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-23 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,726] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,726] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,726] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,737] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-8, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,737] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-8 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,737] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,737] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,737] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,747] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-12, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,747] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-12 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,747] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,747] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,747] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,758] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-16, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,758] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-16 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,758] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,758] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,758] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,769] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,769] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,769] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,769] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,769] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,780] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-20, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,780] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-20 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,780] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,780] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,780] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,791] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-5, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,791] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-5 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,791] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,791] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,791] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,801] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-24, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,801] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-24 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,801] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,802] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,802] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,812] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-9, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,812] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-9 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,812] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,812] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,812] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,823] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-13, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,823] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-13 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,823] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,823] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,823] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,834] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-17, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,834] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-17 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,834] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,834] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,834] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,845] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,845] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,845] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,845] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,845] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,856] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-21, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,856] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-21 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,856] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,856] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,857] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,867] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-6, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,867] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-offset-topic-elasticsearch-it-connect-cluster-6 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,867] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,867] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,867] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,877] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 11:33:12,880] INFO [Broker id=0] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger:66)
[2025-10-07 11:33:12,881] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:12,882] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:12,882] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:12,885] INFO [LogLoader partition=test-0, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,885] INFO Created log for partition test-0 in /tmp/EmbeddedKafkaCluster4752165901678924258/test-0 with properties {} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,885] INFO [Partition test-0 broker=0] No checkpointed highwatermark is found for partition test-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,885] INFO [Partition test-0 broker=0] Log loaded for partition test-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,885] INFO [Broker id=0] Leader test-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,896] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:12,899] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger:66)
[2025-10-07 11:33:12,900] INFO Creating topic connect-storage-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:12,903] INFO [Controller id=0] New topics: [Set(connect-storage-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-storage-topic-elasticsearch-it-connect-cluster,Some(ijk5j4uQQvqs3w9Ot3ADXQ),HashMap(connect-storage-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:12,903] INFO [Controller id=0] New partition creation callback for connect-storage-topic-elasticsearch-it-connect-cluster-2,connect-storage-topic-elasticsearch-it-connect-cluster-0,connect-storage-topic-elasticsearch-it-connect-cluster-3,connect-storage-topic-elasticsearch-it-connect-cluster-4,connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:12,903] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,903] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,904] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,904] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,904] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:12,904] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:12,904] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:12,921] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,921] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,921] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,921] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,921] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:12,921] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:12,921] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:12,923] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:12,923] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:12,931] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-storage-topic-elasticsearch-it-connect-cluster-2, connect-storage-topic-elasticsearch-it-connect-cluster-0, connect-storage-topic-elasticsearch-it-connect-cluster-3, connect-storage-topic-elasticsearch-it-connect-cluster-4, connect-storage-topic-elasticsearch-it-connect-cluster-1) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:12,931] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:12,943] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,944] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-storage-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,944] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,944] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,944] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,954] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,954] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-storage-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,954] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,955] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,955] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,965] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,965] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-storage-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,965] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,965] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,965] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,976] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,976] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-storage-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,976] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,976] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,976] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,987] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:12,987] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-storage-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:12,987] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,987] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:12,987] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:12,997] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:12,998] INFO [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6 (state.change.logger:66)
[2025-10-07 11:33:13,012] INFO Creating topic connect-config-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:13,015] INFO [Controller id=0] New topics: [Set(connect-config-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-config-topic-elasticsearch-it-connect-cluster,Some(lgHhq-MDSo-ZEfklp80F3A),Map(connect-config-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:13,016] INFO [Controller id=0] New partition creation callback for connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:13,016] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,016] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:13,016] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:13,018] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,018] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:13,018] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:13,018] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:13,018] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:13,018] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(connect-config-topic-elasticsearch-it-connect-cluster-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:13,018] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:13,020] INFO [LogLoader partition=connect-config-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,020] INFO Created log for partition connect-config-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster4752165901678924258/connect-config-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,020] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,020] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-config-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,020] INFO [Broker id=0] Leader connect-config-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,030] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:13,031] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger:66)
[2025-10-07 11:33:13,048] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:13,051] INFO [Controller id=0] New topics: [HashSet(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(8hV_6MOARQC9DM21Ag_moQ),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:13,051] INFO [Controller id=0] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,052] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:13,053] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:13,054] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:13,065] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,066] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 50 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:13,067] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:13,068] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:13,071] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:13,072] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 0 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:13,073] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,073] INFO Created log for partition __consumer_offsets-3 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,074] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,074] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,074] INFO [Broker id=0] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,084] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,084] INFO Created log for partition __consumer_offsets-18 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,084] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,084] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,084] INFO [Broker id=0] Leader __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,094] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,095] INFO Created log for partition __consumer_offsets-41 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,095] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,095] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,095] INFO [Broker id=0] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,105] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,105] INFO Created log for partition __consumer_offsets-10 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,105] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,105] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,105] INFO [Broker id=0] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,116] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,116] INFO Created log for partition __consumer_offsets-33 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,116] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,116] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,116] INFO [Broker id=0] Leader __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,127] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,127] INFO Created log for partition __consumer_offsets-48 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,127] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,127] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,127] INFO [Broker id=0] Leader __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,138] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,138] INFO Created log for partition __consumer_offsets-19 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,138] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,138] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,138] INFO [Broker id=0] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,148] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,148] INFO Created log for partition __consumer_offsets-34 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,148] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,148] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,149] INFO [Broker id=0] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,160] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,160] INFO Created log for partition __consumer_offsets-4 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,160] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,160] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,160] INFO [Broker id=0] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,170] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,171] INFO Created log for partition __consumer_offsets-11 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,171] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,171] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,171] INFO [Broker id=0] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,181] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,181] INFO Created log for partition __consumer_offsets-26 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,181] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,181] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,181] INFO [Broker id=0] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,192] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,192] INFO Created log for partition __consumer_offsets-49 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,192] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,192] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,192] INFO [Broker id=0] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,199] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,199] INFO Created log for partition __consumer_offsets-39 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,199] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,199] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,200] INFO [Broker id=0] Leader __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,210] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,210] INFO Created log for partition __consumer_offsets-9 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,210] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,210] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,210] INFO [Broker id=0] Leader __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,221] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,221] INFO Created log for partition __consumer_offsets-24 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,221] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,221] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,222] INFO [Broker id=0] Leader __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,232] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,232] INFO Created log for partition __consumer_offsets-31 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,232] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,232] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,232] INFO [Broker id=0] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,243] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,243] INFO Created log for partition __consumer_offsets-46 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,243] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,243] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,243] INFO [Broker id=0] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,253] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,253] INFO Created log for partition __consumer_offsets-1 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,253] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,253] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,253] INFO [Broker id=0] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,263] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,263] INFO Created log for partition __consumer_offsets-16 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,263] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,263] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,264] INFO [Broker id=0] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,274] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,274] INFO Created log for partition __consumer_offsets-2 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,274] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,274] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,274] INFO [Broker id=0] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,285] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,285] INFO Created log for partition __consumer_offsets-25 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,285] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,285] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,285] INFO [Broker id=0] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,296] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,296] INFO Created log for partition __consumer_offsets-40 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,296] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,296] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,296] INFO [Broker id=0] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,307] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,307] INFO Created log for partition __consumer_offsets-47 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,307] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,307] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,307] INFO [Broker id=0] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,318] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,318] INFO Created log for partition __consumer_offsets-17 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,318] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,318] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,318] INFO [Broker id=0] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,329] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,329] INFO Created log for partition __consumer_offsets-32 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,329] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,329] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,329] INFO [Broker id=0] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,340] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,340] INFO Created log for partition __consumer_offsets-37 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,340] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,340] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,340] INFO [Broker id=0] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,351] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,351] INFO Created log for partition __consumer_offsets-7 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,351] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,351] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,351] INFO [Broker id=0] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,362] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,362] INFO Created log for partition __consumer_offsets-22 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,362] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,362] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,362] INFO [Broker id=0] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,373] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,373] INFO Created log for partition __consumer_offsets-29 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,373] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,373] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,373] INFO [Broker id=0] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,383] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,383] INFO Created log for partition __consumer_offsets-44 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,383] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,383] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,383] INFO [Broker id=0] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,394] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,394] INFO Created log for partition __consumer_offsets-14 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,394] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,394] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,394] INFO [Broker id=0] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,406] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,406] INFO Created log for partition __consumer_offsets-23 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,406] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,406] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,406] INFO [Broker id=0] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,416] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,416] INFO Created log for partition __consumer_offsets-38 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,416] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,416] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,416] INFO [Broker id=0] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,427] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,427] INFO Created log for partition __consumer_offsets-8 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,427] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,427] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,427] INFO [Broker id=0] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,438] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,438] INFO Created log for partition __consumer_offsets-45 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,438] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,438] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,438] INFO [Broker id=0] Leader __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,449] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,449] INFO Created log for partition __consumer_offsets-15 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,449] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,449] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,449] INFO [Broker id=0] Leader __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,460] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,460] INFO Created log for partition __consumer_offsets-30 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,460] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,460] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,460] INFO [Broker id=0] Leader __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,471] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,471] INFO Created log for partition __consumer_offsets-0 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,471] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,471] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,471] INFO [Broker id=0] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,482] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,482] INFO Created log for partition __consumer_offsets-35 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,482] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,482] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,482] INFO [Broker id=0] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,493] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,493] INFO Created log for partition __consumer_offsets-5 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,493] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,493] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,493] INFO [Broker id=0] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,504] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,504] INFO Created log for partition __consumer_offsets-20 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,504] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,504] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,504] INFO [Broker id=0] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,514] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,514] INFO Created log for partition __consumer_offsets-27 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,514] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,514] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,514] INFO [Broker id=0] Leader __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,525] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,525] INFO Created log for partition __consumer_offsets-42 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,525] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,525] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,525] INFO [Broker id=0] Leader __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,536] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,536] INFO Created log for partition __consumer_offsets-12 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,536] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,536] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,536] INFO [Broker id=0] Leader __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,546] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,546] INFO Created log for partition __consumer_offsets-21 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,546] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,546] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,546] INFO [Broker id=0] Leader __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,555] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,555] INFO Created log for partition __consumer_offsets-36 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,555] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,555] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,555] INFO [Broker id=0] Leader __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,566] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,566] INFO Created log for partition __consumer_offsets-6 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,566] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,566] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,566] INFO [Broker id=0] Leader __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,577] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,577] INFO Created log for partition __consumer_offsets-43 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,577] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,577] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,577] INFO [Broker id=0] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,588] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,588] INFO Created log for partition __consumer_offsets-13 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,588] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,588] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,588] INFO [Broker id=0] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,598] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/EmbeddedKafkaCluster4752165901678924258] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:13,599] INFO Created log for partition __consumer_offsets-28 in /tmp/EmbeddedKafkaCluster4752165901678924258/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:13,599] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,599] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:13,599] INFO [Broker id=0] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:13,608] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,608] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,609] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,609] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,609] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,609] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,609] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,609] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,609] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,609] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,609] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,609] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,609] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,609] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,610] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,611] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,612] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,612] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,612] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,612] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,612] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,612] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,612] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,612] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,612] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,612] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,612] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,612] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,612] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,612] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,612] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,612] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,612] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,612] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,612] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,612] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,612] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,612] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,612] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,612] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,612] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,612] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,612] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:13,613] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,613] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,613] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,613] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,613] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,613] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,613] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,613] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,613] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,614] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,614] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,614] INFO [Broker id=0] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 10 (state.change.logger:66)
[2025-10-07 11:33:13,614] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,614] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,614] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,614] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,614] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,614] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,614] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,614] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,615] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,615] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,615] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,615] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,615] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,615] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,615] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,615] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,615] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,615] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,615] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,616] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 4 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,616] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,616] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,616] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,616] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,616] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,616] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,616] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,616] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,616] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,616] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,617] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,617] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:13,656] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-integration-test-elasticsearch-it-connect-cluster in Empty state. Created a new member id connect-4-103031ae-5f4c-48cb-b612-c4e55282234d and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,656] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member connect-4-103031ae-5f4c-48cb-b612-c4e55282234d with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,658] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 1 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,660] INFO [GroupCoordinator 0]: Assignment received from leader connect-4-103031ae-5f4c-48cb-b612-c4e55282234d for group connect-integration-test-elasticsearch-it-connect-cluster for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,669] INFO ElasticsearchSinkConnectorConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:44365/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 11:33:13,670] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:33:13,670] INFO Using unsecured connection to [http://localhost:44365]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:33:13,686] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 1 (__consumer_offsets-4) (reason: Leader connect-4-103031ae-5f4c-48cb-b612-c4e55282234d re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,686] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 2 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,688] INFO [GroupCoordinator 0]: Assignment received from leader connect-4-103031ae-5f4c-48cb-b612-c4e55282234d for group connect-integration-test-elasticsearch-it-connect-cluster for generation 2. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,690] INFO [es-connector|worker] ElasticsearchSinkConnectorConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:44365/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 11:33:13,705] ERROR Could not check connector state info. (io.confluent.connect.elasticsearch.integration.BaseConnectorIT:92)
[2025-10-07 11:33:13,706] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 2 (__consumer_offsets-4) (reason: Leader connect-4-103031ae-5f4c-48cb-b612-c4e55282234d re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,707] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 3 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,709] INFO [GroupCoordinator 0]: Assignment received from leader connect-4-103031ae-5f4c-48cb-b612-c4e55282234d for group connect-integration-test-elasticsearch-it-connect-cluster for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,714] INFO [es-connector|task-0] Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:33:13,714] INFO [es-connector|task-0] ElasticsearchSinkTaskConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:44365/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 0
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:33:13,715] INFO [es-connector|task-0] Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:33:13,715] DEBUG [es-connector|task-0] Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:33:13,715] INFO [es-connector|task-0] Using unsecured connection to [http://localhost:44365]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:33:13,723] INFO [es-connector|task-0] Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:33:13,728] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-es-connector in Empty state. Created a new member id connector-consumer-es-connector-0-100c303c-e448-4b7f-8ad8-e4cd8f457002 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,729] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member connector-consumer-es-connector-0-100c303c-e448-4b7f-8ad8-e4cd8f457002 with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,730] INFO [GroupCoordinator 0]: Stabilized group connect-es-connector generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,736] INFO [GroupCoordinator 0]: Assignment received from leader connector-consumer-es-connector-0-100c303c-e448-4b7f-8ad8-e4cd8f457002 for group connect-es-connector for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,819] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:13,819] INFO [es-connector|task-0] Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:33:13,822] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:13,822] DEBUG [es-connector|task-0] Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:13,824] WARN [es-connector|task-0] Failed to execute bulk request due to org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:44365], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
. Retrying attempt (1/3) after backoff of 7 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:33:13,832] WARN [es-connector|task-0] Failed to execute bulk request due to org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:44365], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
. Retrying attempt (2/3) after backoff of 35 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 11:33:13,868] ERROR [es-connector|task-0] Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:44365], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:44365], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:44365], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 11:33:13,869] WARN [es-connector|task-0] Bulk request 1 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:44365], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:44365], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:44365], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 11:33:13,870] ERROR [es-connector|task-0] WorkerSinkTask{id=es-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Bulk request failed (org.apache.kafka.connect.runtime.WorkerSinkTask:629)
org.apache.kafka.connect.errors.ConnectException: Bulk request failed
	at io.confluent.connect.elasticsearch.ElasticsearchClient$2.afterBulk(ElasticsearchClient.java:455)
	at org.elasticsearch.action.bulk.BulkRequestHandler$1.onFailure(BulkRequestHandler.java:64)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.ActionListener$RunAfterActionListener.onFailure(ActionListener.java:350)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onFailure(Retry.java:123)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:227)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:44365], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	... 5 more
Caused by: ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:44365], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:44365], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 11:33:13,872] DEBUG [es-connector|task-0] Tried to flush data to Elasticsearch, but BulkProcessor is already closed. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:137)
java.lang.IllegalStateException: bulk process already closed
	at org.elasticsearch.action.bulk.BulkProcessor.ensureOpen(BulkProcessor.java:454)
	at org.elasticsearch.action.bulk.BulkProcessor.flush(BulkProcessor.java:571)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.flush(ElasticsearchClient.java:303)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.preCommit(ElasticsearchSinkTask.java:135)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:404)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.closePartitions(WorkerSinkTask.java:666)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.closeAllPartitions(WorkerSinkTask.java:661)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:204)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[2025-10-07 11:33:13,872] DEBUG [es-connector|task-0] preCommitting offsets {} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:33:13,872] ERROR [es-connector|task-0] WorkerSinkTask{id=es-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:195)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:631)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:333)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:234)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:203)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.kafka.connect.errors.ConnectException: Bulk request failed
	at io.confluent.connect.elasticsearch.ElasticsearchClient$2.afterBulk(ElasticsearchClient.java:455)
	at org.elasticsearch.action.bulk.BulkRequestHandler$1.onFailure(BulkRequestHandler.java:64)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.ActionListener$RunAfterActionListener.onFailure(ActionListener.java:350)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onFailure(Retry.java:123)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:227)
	... 5 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:44365], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	... 5 more
Caused by: ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:44365], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:44365], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 11:33:13,873] DEBUG [es-connector|task-0] Stopping Elasticsearch client. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:146)
[2025-10-07 11:33:13,873] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 1 (__consumer_offsets-23) (reason: Removing member connector-consumer-es-connector-0-100c303c-e448-4b7f-8ad8-e4cd8f457002 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,874] INFO [GroupCoordinator 0]: Group connect-es-connector with generation 2 is now empty (__consumer_offsets-23) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,874] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connector-consumer-es-connector-0-100c303c-e448-4b7f-8ad8-e4cd8f457002, groupInstanceId=None, clientId=connector-consumer-es-connector-0, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group connect-es-connector through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,950] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 3 (__consumer_offsets-4) (reason: Removing member connect-4-103031ae-5f4c-48cb-b612-c4e55282234d on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,950] INFO [GroupCoordinator 0]: Group connect-integration-test-elasticsearch-it-connect-cluster with generation 4 is now empty (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,951] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connect-4-103031ae-5f4c-48cb-b612-c4e55282234d, groupInstanceId=None, clientId=connect-4, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(sessioned, compatible, default)) has left group connect-integration-test-elasticsearch-it-connect-cluster through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:13,962] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer:66)
[2025-10-07 11:33:13,962] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer:66)
[2025-10-07 11:33:13,963] INFO [Controller id=0] Shutting down broker 0 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:13,964] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:13,964] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer:66)
[2025-10-07 11:33:13,965] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:13,965] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:13,965] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:13,965] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:13,966] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:13,966] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 11:33:13,967] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 11:33:13,967] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,053] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,053] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,053] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis:66)
[2025-10-07 11:33:14,053] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,253] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,253] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,253] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:14,254] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager:66)
[2025-10-07 11:33:14,254] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:14,254] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:14,254] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:14,254] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:14,254] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:14,254] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,453] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,453] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,454] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,529] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,529] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,530] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:14,530] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager:66)
[2025-10-07 11:33:14,530] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:14,530] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:14,530] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:14,530] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:14,530] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:14,531] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 11:33:14,531] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 11:33:14,531] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,608] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,608] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,608] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,653] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,653] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,654] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,853] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,853] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:14,854] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:15,053] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:15,053] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:15,060] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager:66)
[2025-10-07 11:33:15,060] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:15,061] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:15,061] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:15,061] INFO Broker to controller channel manager for alterIsr shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 11:33:15,061] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:15,061] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:15,061] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:15,061] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 11:33:15,061] INFO Shutting down. (kafka.log.LogManager:66)
[2025-10-07 11:33:15,062] INFO Shutting down the log cleaner. (kafka.log.LogCleaner:66)
[2025-10-07 11:33:15,062] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner:66)
[2025-10-07 11:33:15,062] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner:66)
[2025-10-07 11:33:15,062] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner:66)
[2025-10-07 11:33:15,069] INFO [ProducerStateManager partition=test-0] Wrote producer snapshot at offset 5 with 1 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:15,078] INFO [ProducerStateManager partition=__consumer_offsets-23] Wrote producer snapshot at offset 2 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:15,087] INFO [ProducerStateManager partition=connect-config-topic-elasticsearch-it-connect-cluster-0] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:15,094] INFO [ProducerStateManager partition=__consumer_offsets-4] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:15,108] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-2] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:15,117] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-3] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:15,130] INFO Shutdown complete. (kafka.log.LogManager:66)
[2025-10-07 11:33:15,131] INFO [ControllerEventThread controllerId=0] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:15,131] INFO [ControllerEventThread controllerId=0] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:15,131] INFO [ControllerEventThread controllerId=0] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:15,132] INFO [PartitionStateMachine controllerId=0] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:33:15,132] INFO [ReplicaStateMachine controllerId=0] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:15,132] INFO [RequestSendThread controllerId=0] Shutting down (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:15,132] INFO [RequestSendThread controllerId=0] Stopped (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:15,132] INFO [RequestSendThread controllerId=0] Shutdown completed (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:15,133] INFO [Controller id=0] Resigned (kafka.controller.KafkaController:66)
[2025-10-07 11:33:15,133] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:15,133] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:15,133] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:15,133] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:15,236] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:15,237] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:15,638] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:15,638] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:15,639] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:16,639] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:16,639] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:16,639] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:17,639] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:17,639] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:17,639] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:18,639] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:18,639] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:18,639] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer:66)
[2025-10-07 11:33:18,644] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer:66)
[2025-10-07 11:33:18,644] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats:66)
[2025-10-07 11:33:18,644] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer:66)
[2025-10-07 11:33:18,672] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster432596061218484029
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:45047
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:33:18,674] INFO starting (kafka.server.KafkaServer:66)
[2025-10-07 11:33:18,674] INFO Connecting to zookeeper on 127.0.0.1:45047 (kafka.server.KafkaServer:66)
[2025-10-07 11:33:18,674] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:45047. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:18,674] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:18,678] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:18,691] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:18,693] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 11:33:18,693] INFO Cleared cache (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 11:33:18,695] INFO Cluster ID = BDqKm7ahQUSSsevaocDo3w (kafka.server.KafkaServer:66)
[2025-10-07 11:33:18,696] WARN No meta.properties file under dir /tmp/EmbeddedKafkaCluster432596061218484029/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2025-10-07 11:33:18,698] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster432596061218484029
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:45047
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:33:18,699] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster432596061218484029
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:45047
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:33:18,705] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:18,706] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:18,710] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:18,710] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:18,711] INFO Loading logs from log dirs ArraySeq(/tmp/EmbeddedKafkaCluster432596061218484029) (kafka.log.LogManager:66)
[2025-10-07 11:33:18,711] INFO Attempting recovery for all logs in /tmp/EmbeddedKafkaCluster432596061218484029 since no clean shutdown file was found (kafka.log.LogManager:66)
[2025-10-07 11:33:18,711] INFO Loaded 0 logs in 0ms. (kafka.log.LogManager:66)
[2025-10-07 11:33:18,711] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:66)
[2025-10-07 11:33:18,711] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:66)
[2025-10-07 11:33:18,712] INFO Starting the log cleaner (kafka.log.LogCleaner:66)
[2025-10-07 11:33:18,722] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner:66)
[2025-10-07 11:33:18,723] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:18,730] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas:66)
[2025-10-07 11:33:18,731] INFO Awaiting socket connections on localhost:35547. (kafka.network.Acceptor:66)
[2025-10-07 11:33:18,732] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 11:33:18,734] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:18,734] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:18,735] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:18,736] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:18,737] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:18,738] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:18,739] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:18,740] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1759836798740,1759836798740,1,0,0,72057690439090176,204,0,25
 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:18,740] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:35547, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:18,749] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:18,750] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:18,751] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:18,754] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:18,755] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:18,756] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:18,756] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:18,756] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:18,757] INFO [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,757] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:18,758] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:18,758] INFO [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{}) (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,758] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:18,758] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 11:33:18,760] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:18,760] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 11:33:18,760] INFO [Controller id=0] Registering handlers (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,761] INFO [Controller id=0] Deleting log dir event notifications (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,761] INFO [Controller id=0] Deleting isr change notifications (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,762] INFO [Controller id=0] Initializing controller context (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,762] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:18,763] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 11:33:18,763] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:18,763] INFO [KafkaServer id=0] started (kafka.server.KafkaServer:66)
[2025-10-07 11:33:18,763] INFO [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25) (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,765] INFO [Controller id=0] Currently active brokers in the cluster: Set(0) (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,765] INFO [Controller id=0] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,766] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:18,766] INFO [Controller id=0] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,766] INFO [Controller id=0] Fetching topic deletions in progress (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,768] INFO [Controller id=0] List of topics to be deleted:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,768] INFO [Controller id=0] List of topics ineligible for deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,768] INFO [Controller id=0] Initializing topic deletion manager (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,768] INFO [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager:66)
[2025-10-07 11:33:18,768] INFO [Controller id=0] Sending update metadata request (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,768] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:18,768] INFO [ReplicaStateMachine controllerId=0] Initializing replica state (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:18,768] INFO [ReplicaStateMachine controllerId=0] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:18,768] INFO [ReplicaStateMachine controllerId=0] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:18,768] INFO [PartitionStateMachine controllerId=0] Initializing partition state (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:33:18,768] INFO [PartitionStateMachine controllerId=0] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:33:18,768] INFO [RequestSendThread controllerId=0] Controller 0 connected to localhost:35547 (id: 0 rack: null) for sending state change requests (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:18,768] INFO [Controller id=0] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,769] INFO [Controller id=0] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,769] INFO [Controller id=0] Partitions that completed preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,769] INFO [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,769] INFO [Controller id=0] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,769] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,771] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,824] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker localhost:35547 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:18,828] INFO [Controller id=0] Acquired new producerId block ProducerIdsBlock{brokerId=0, producerIdStart=0, producerIdLen=1000} by writing to Zk with path version 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:18,834] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:35547 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:20,653] INFO Creating topic connect-offset-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:20,657] INFO [Controller id=0] New topics: [Set(connect-offset-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-offset-topic-elasticsearch-it-connect-cluster,Some(vB9ljydMSPawFWF46JS13A),HashMap(connect-offset-topic-elasticsearch-it-connect-cluster-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:20,657] INFO [Controller id=0] New partition creation callback for connect-offset-topic-elasticsearch-it-connect-cluster-20,connect-offset-topic-elasticsearch-it-connect-cluster-7,connect-offset-topic-elasticsearch-it-connect-cluster-0,connect-offset-topic-elasticsearch-it-connect-cluster-12,connect-offset-topic-elasticsearch-it-connect-cluster-13,connect-offset-topic-elasticsearch-it-connect-cluster-5,connect-offset-topic-elasticsearch-it-connect-cluster-21,connect-offset-topic-elasticsearch-it-connect-cluster-14,connect-offset-topic-elasticsearch-it-connect-cluster-6,connect-offset-topic-elasticsearch-it-connect-cluster-1,connect-offset-topic-elasticsearch-it-connect-cluster-19,connect-offset-topic-elasticsearch-it-connect-cluster-17,connect-offset-topic-elasticsearch-it-connect-cluster-16,connect-offset-topic-elasticsearch-it-connect-cluster-23,connect-offset-topic-elasticsearch-it-connect-cluster-10,connect-offset-topic-elasticsearch-it-connect-cluster-3,connect-offset-topic-elasticsearch-it-connect-cluster-15,connect-offset-topic-elasticsearch-it-connect-cluster-22,connect-offset-topic-elasticsearch-it-connect-cluster-24,connect-offset-topic-elasticsearch-it-connect-cluster-18,connect-offset-topic-elasticsearch-it-connect-cluster-2,connect-offset-topic-elasticsearch-it-connect-cluster-4,connect-offset-topic-elasticsearch-it-connect-cluster-8,connect-offset-topic-elasticsearch-it-connect-cluster-9,connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:20,657] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,657] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,657] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,657] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,657] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,657] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,657] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,657] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,657] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,657] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,657] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,657] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,658] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,658] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,658] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,658] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,658] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,658] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,658] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,658] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,658] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,658] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,658] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,658] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,658] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,658] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:20,659] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:20,668] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,668] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,668] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,668] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,669] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 25 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:20,670] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 25 partitions (state.change.logger:66)
[2025-10-07 11:33:20,670] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:20,670] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 11:33:20,673] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-offset-topic-elasticsearch-it-connect-cluster-20, connect-offset-topic-elasticsearch-it-connect-cluster-7, connect-offset-topic-elasticsearch-it-connect-cluster-0, connect-offset-topic-elasticsearch-it-connect-cluster-12, connect-offset-topic-elasticsearch-it-connect-cluster-13, connect-offset-topic-elasticsearch-it-connect-cluster-5, connect-offset-topic-elasticsearch-it-connect-cluster-21, connect-offset-topic-elasticsearch-it-connect-cluster-14, connect-offset-topic-elasticsearch-it-connect-cluster-6, connect-offset-topic-elasticsearch-it-connect-cluster-1, connect-offset-topic-elasticsearch-it-connect-cluster-19, connect-offset-topic-elasticsearch-it-connect-cluster-17, connect-offset-topic-elasticsearch-it-connect-cluster-16, connect-offset-topic-elasticsearch-it-connect-cluster-23, connect-offset-topic-elasticsearch-it-connect-cluster-10, connect-offset-topic-elasticsearch-it-connect-cluster-3, connect-offset-topic-elasticsearch-it-connect-cluster-15, connect-offset-topic-elasticsearch-it-connect-cluster-22, connect-offset-topic-elasticsearch-it-connect-cluster-24, connect-offset-topic-elasticsearch-it-connect-cluster-18, connect-offset-topic-elasticsearch-it-connect-cluster-2, connect-offset-topic-elasticsearch-it-connect-cluster-4, connect-offset-topic-elasticsearch-it-connect-cluster-8, connect-offset-topic-elasticsearch-it-connect-cluster-9, connect-offset-topic-elasticsearch-it-connect-cluster-11) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:20,673] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 25 partitions (state.change.logger:66)
[2025-10-07 11:33:20,675] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-10, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,677] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-10 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,677] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,677] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,677] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 11:33:20 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Oct 07, 2025 11:33:20 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Oct 07, 2025 11:33:20 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Oct 07, 2025 11:33:20 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
[2025-10-07 11:33:20,689] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-14, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,689] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-14 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,690] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,690] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,690] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,701] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-18, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,701] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-18 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,701] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,701] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,701] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 11:33:20 AM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

[2025-10-07 11:33:20,712] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,712] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,712] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,712] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,712] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,717] INFO Creating topic test with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:20,720] INFO [Controller id=0] New topics: [Set(test)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(test,Some(6cP_m7UvRzOGwlc67_zVbQ),Map(test-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:20,720] INFO [Controller id=0] New partition creation callback for test-0 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:20,720] INFO [Controller id=0 epoch=1] Changed partition test-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,721] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:20,721] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:20,723] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-22, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,723] INFO [Controller id=0 epoch=1] Changed partition test-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,723] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:20,723] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-22 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,723] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:20,723] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,723] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,723] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,723] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:20,734] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-7, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,734] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-7 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,734] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,734] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,734] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,744] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-11, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,745] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-11 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,745] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,745] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,745] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,755] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-15, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,755] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-15 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,756] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,756] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,756] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,766] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,766] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,766] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,766] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,766] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,777] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-19, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,777] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-19 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,777] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,777] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,777] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,787] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,787] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,787] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,788] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,788] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,798] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-23, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,798] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-23 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,798] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,798] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,798] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,805] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-8, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,806] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-8 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,806] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,806] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,806] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,816] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-12, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,816] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-12 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,816] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,816] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,816] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,826] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-16, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,827] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-16 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,827] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,827] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,827] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,837] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,837] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,837] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,837] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,837] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,848] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-20, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,848] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-20 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,848] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,848] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,848] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,858] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-5, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,858] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-5 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,858] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,859] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,859] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,869] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-24, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,869] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-24 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,869] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,869] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,869] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,880] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-9, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,880] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-9 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,880] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,880] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,880] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,890] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-13, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,891] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-13 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,891] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,891] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,891] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,902] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-17, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,902] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-17 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,902] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,902] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,902] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,909] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,909] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,909] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,909] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,909] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,919] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-21, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,920] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-21 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,920] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,920] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,920] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,930] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-6, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,930] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-offset-topic-elasticsearch-it-connect-cluster-6 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,930] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,930] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,930] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,940] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 11:33:20,941] INFO [Broker id=0] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger:66)
[2025-10-07 11:33:20,943] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:20,944] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:20,944] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:20,946] INFO [LogLoader partition=test-0, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,947] INFO Created log for partition test-0 in /tmp/EmbeddedKafkaCluster432596061218484029/test-0 with properties {} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,947] INFO [Partition test-0 broker=0] No checkpointed highwatermark is found for partition test-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,947] INFO [Partition test-0 broker=0] Log loaded for partition test-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,947] INFO [Broker id=0] Leader test-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:20,957] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:20,960] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger:66)
[2025-10-07 11:33:20,961] INFO Creating topic connect-storage-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:20,964] INFO [Controller id=0] New topics: [Set(connect-storage-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-storage-topic-elasticsearch-it-connect-cluster,Some(RTxZzmECRrK5HJQyBN_31A),HashMap(connect-storage-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:20,964] INFO [Controller id=0] New partition creation callback for connect-storage-topic-elasticsearch-it-connect-cluster-2,connect-storage-topic-elasticsearch-it-connect-cluster-0,connect-storage-topic-elasticsearch-it-connect-cluster-3,connect-storage-topic-elasticsearch-it-connect-cluster-4,connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:20,964] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,964] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,964] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,964] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,964] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:20,964] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:20,965] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:20,975] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,976] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,976] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,976] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,976] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:20,976] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:20,976] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:20,976] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:20,977] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:20,981] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-storage-topic-elasticsearch-it-connect-cluster-2, connect-storage-topic-elasticsearch-it-connect-cluster-0, connect-storage-topic-elasticsearch-it-connect-cluster-3, connect-storage-topic-elasticsearch-it-connect-cluster-4, connect-storage-topic-elasticsearch-it-connect-cluster-1) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:20,981] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:20,992] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:20,992] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-storage-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:20,993] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,993] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:20,993] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,003] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,003] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-storage-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,004] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,004] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,004] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,014] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,014] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-storage-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,014] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,014] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,014] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,025] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,025] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-storage-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,025] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,025] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,025] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,035] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,035] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-storage-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,035] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,035] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,035] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,045] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:21,046] INFO [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6 (state.change.logger:66)
[2025-10-07 11:33:21,060] INFO Creating topic connect-config-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:21,063] INFO [Controller id=0] New topics: [Set(connect-config-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-config-topic-elasticsearch-it-connect-cluster,Some(V8JNArtWRB6RKYwOTmmThA),Map(connect-config-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:21,063] INFO [Controller id=0] New partition creation callback for connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:21,063] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,064] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:21,064] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:21,066] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,066] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:21,066] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:21,066] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:21,066] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:21,067] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(connect-config-topic-elasticsearch-it-connect-cluster-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:21,067] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:21,068] INFO [LogLoader partition=connect-config-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,068] INFO Created log for partition connect-config-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster432596061218484029/connect-config-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,068] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,069] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-config-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,069] INFO [Broker id=0] Leader connect-config-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,078] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:21,078] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger:66)
[2025-10-07 11:33:21,099] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:21,103] INFO [Controller id=0] New topics: [HashSet(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(7O5D2ThCQjyUJuHBz_HoVQ),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:21,103] INFO [Controller id=0] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:21,103] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,103] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,103] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,103] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,103] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,103] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,103] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,103] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,103] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,103] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,103] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,103] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,104] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:21,105] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:21,106] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,119] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,120] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:21,121] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 50 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:21,121] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:21,122] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:21,122] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:21,125] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:21,125] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 0 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:21,127] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,127] INFO Created log for partition __consumer_offsets-3 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,127] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,127] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,127] INFO [Broker id=0] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,137] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,138] INFO Created log for partition __consumer_offsets-18 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,138] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,138] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,138] INFO [Broker id=0] Leader __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,148] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,148] INFO Created log for partition __consumer_offsets-41 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,148] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,148] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,148] INFO [Broker id=0] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,158] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,158] INFO Created log for partition __consumer_offsets-10 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,158] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,158] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,158] INFO [Broker id=0] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,165] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,165] INFO Created log for partition __consumer_offsets-33 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,165] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,165] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,165] INFO [Broker id=0] Leader __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,176] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,176] INFO Created log for partition __consumer_offsets-48 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,176] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,176] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,176] INFO [Broker id=0] Leader __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,186] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,186] INFO Created log for partition __consumer_offsets-19 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,186] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,187] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,187] INFO [Broker id=0] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,197] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,197] INFO Created log for partition __consumer_offsets-34 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,197] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,197] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,197] INFO [Broker id=0] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,207] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,207] INFO Created log for partition __consumer_offsets-4 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,207] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,207] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,207] INFO [Broker id=0] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,217] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,218] INFO Created log for partition __consumer_offsets-11 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,218] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,218] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,218] INFO [Broker id=0] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,228] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,228] INFO Created log for partition __consumer_offsets-26 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,228] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,228] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,228] INFO [Broker id=0] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,238] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,238] INFO Created log for partition __consumer_offsets-49 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,238] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,238] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,238] INFO [Broker id=0] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,248] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,249] INFO Created log for partition __consumer_offsets-39 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,249] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,249] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,249] INFO [Broker id=0] Leader __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,259] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,259] INFO Created log for partition __consumer_offsets-9 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,259] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,259] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,259] INFO [Broker id=0] Leader __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,270] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,270] INFO Created log for partition __consumer_offsets-24 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,270] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,270] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,270] INFO [Broker id=0] Leader __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,280] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,280] INFO Created log for partition __consumer_offsets-31 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,280] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,280] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,280] INFO [Broker id=0] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,290] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,290] INFO Created log for partition __consumer_offsets-46 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,291] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,291] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,291] INFO [Broker id=0] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,301] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,301] INFO Created log for partition __consumer_offsets-1 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,301] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,301] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,301] INFO [Broker id=0] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,311] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,311] INFO Created log for partition __consumer_offsets-16 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,311] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,311] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,311] INFO [Broker id=0] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,321] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,322] INFO Created log for partition __consumer_offsets-2 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,322] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,322] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,322] INFO [Broker id=0] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,332] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,332] INFO Created log for partition __consumer_offsets-25 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,332] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,332] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,332] INFO [Broker id=0] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,342] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,342] INFO Created log for partition __consumer_offsets-40 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,342] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,342] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,342] INFO [Broker id=0] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,352] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,352] INFO Created log for partition __consumer_offsets-47 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,352] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,353] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,353] INFO [Broker id=0] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,364] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,364] INFO Created log for partition __consumer_offsets-17 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,364] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,365] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,365] INFO [Broker id=0] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,375] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,375] INFO Created log for partition __consumer_offsets-32 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,375] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,375] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,375] INFO [Broker id=0] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,385] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,386] INFO Created log for partition __consumer_offsets-37 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,386] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,386] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,386] INFO [Broker id=0] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,396] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,396] INFO Created log for partition __consumer_offsets-7 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,396] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,396] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,396] INFO [Broker id=0] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,406] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,406] INFO Created log for partition __consumer_offsets-22 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,406] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,407] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,407] INFO [Broker id=0] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,417] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,417] INFO Created log for partition __consumer_offsets-29 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,417] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,417] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,417] INFO [Broker id=0] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,427] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,428] INFO Created log for partition __consumer_offsets-44 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,428] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,428] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,428] INFO [Broker id=0] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,438] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,438] INFO Created log for partition __consumer_offsets-14 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,438] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,438] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,438] INFO [Broker id=0] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,448] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,448] INFO Created log for partition __consumer_offsets-23 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,448] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,448] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,448] INFO [Broker id=0] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,458] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,459] INFO Created log for partition __consumer_offsets-38 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,459] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,459] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,459] INFO [Broker id=0] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,469] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,469] INFO Created log for partition __consumer_offsets-8 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,469] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,469] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,469] INFO [Broker id=0] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,479] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,479] INFO Created log for partition __consumer_offsets-45 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,479] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,479] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,480] INFO [Broker id=0] Leader __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,490] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,490] INFO Created log for partition __consumer_offsets-15 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,490] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,490] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,490] INFO [Broker id=0] Leader __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,500] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,500] INFO Created log for partition __consumer_offsets-30 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,500] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,500] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,501] INFO [Broker id=0] Leader __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,510] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,510] INFO Created log for partition __consumer_offsets-0 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,511] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,511] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,511] INFO [Broker id=0] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,521] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,521] INFO Created log for partition __consumer_offsets-35 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,521] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,521] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,521] INFO [Broker id=0] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,531] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,531] INFO Created log for partition __consumer_offsets-5 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,531] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,531] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,531] INFO [Broker id=0] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,542] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,542] INFO Created log for partition __consumer_offsets-20 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,542] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,542] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,542] INFO [Broker id=0] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,552] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,552] INFO Created log for partition __consumer_offsets-27 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,552] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,553] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,553] INFO [Broker id=0] Leader __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,562] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,562] INFO Created log for partition __consumer_offsets-42 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,563] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,563] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,563] INFO [Broker id=0] Leader __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,573] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,573] INFO Created log for partition __consumer_offsets-12 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,573] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,573] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,573] INFO [Broker id=0] Leader __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,579] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,580] INFO Created log for partition __consumer_offsets-21 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,580] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,580] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,580] INFO [Broker id=0] Leader __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,590] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,590] INFO Created log for partition __consumer_offsets-36 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,590] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,591] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,591] INFO [Broker id=0] Leader __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,599] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,599] INFO Created log for partition __consumer_offsets-6 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,599] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,599] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,599] INFO [Broker id=0] Leader __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,609] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,609] INFO Created log for partition __consumer_offsets-43 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,609] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,610] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,610] INFO [Broker id=0] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,620] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,620] INFO Created log for partition __consumer_offsets-13 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,620] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,620] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,620] INFO [Broker id=0] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,630] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/EmbeddedKafkaCluster432596061218484029] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:21,631] INFO Created log for partition __consumer_offsets-28 in /tmp/EmbeddedKafkaCluster432596061218484029/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:21,631] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,631] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:21,631] INFO [Broker id=0] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:21,640] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,640] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,641] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,641] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,641] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,641] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,641] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,641] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,641] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,641] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,641] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,641] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,642] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,642] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,643] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,643] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,644] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,644] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,644] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,641] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,644] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,644] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,644] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,644] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,644] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,644] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,644] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,644] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,644] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,644] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,644] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,644] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,644] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,644] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,644] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,644] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,644] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,644] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:21,644] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,645] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,645] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,645] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,645] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,645] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,645] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,645] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,645] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,646] INFO [Broker id=0] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 10 (state.change.logger:66)
[2025-10-07 11:33:21,646] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,649] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,649] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:21,708] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-integration-test-elasticsearch-it-connect-cluster in Empty state. Created a new member id connect-5-43cd7727-d611-43b9-8869-2b5df081ad42 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,709] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member connect-5-43cd7727-d611-43b9-8869-2b5df081ad42 with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,710] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 1 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,713] INFO [GroupCoordinator 0]: Assignment received from leader connect-5-43cd7727-d611-43b9-8869-2b5df081ad42 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,721] INFO ElasticsearchSinkConnectorConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:46483/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 60000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 11:33:21,722] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:33:21,723] INFO Using unsecured connection to [http://localhost:46483]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:33:21,739] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 1 (__consumer_offsets-4) (reason: Leader connect-5-43cd7727-d611-43b9-8869-2b5df081ad42 re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,740] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 2 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,742] INFO [GroupCoordinator 0]: Assignment received from leader connect-5-43cd7727-d611-43b9-8869-2b5df081ad42 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 2. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,744] INFO [es-connector|worker] ElasticsearchSinkConnectorConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:46483/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 60000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 11:33:21,752] ERROR Could not check connector state info. (io.confluent.connect.elasticsearch.integration.BaseConnectorIT:92)
[2025-10-07 11:33:21,756] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 2 (__consumer_offsets-4) (reason: Leader connect-5-43cd7727-d611-43b9-8869-2b5df081ad42 re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,757] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 3 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,758] INFO [GroupCoordinator 0]: Assignment received from leader connect-5-43cd7727-d611-43b9-8869-2b5df081ad42 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,769] INFO [es-connector|task-0] Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:33:21,769] INFO [es-connector|task-0] ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:46483/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 60000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 0
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:33:21,769] INFO [es-connector|task-0] Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:33:21,770] DEBUG [es-connector|task-0] Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:33:21,770] INFO [es-connector|task-0] Using unsecured connection to [http://localhost:46483]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:33:21,779] INFO [es-connector|task-0] Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:33:21,783] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-es-connector in Empty state. Created a new member id connector-consumer-es-connector-0-b4931254-87e1-460a-b530-3255a380fe42 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,784] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member connector-consumer-es-connector-0-b4931254-87e1-460a-b530-3255a380fe42 with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,785] INFO [GroupCoordinator 0]: Stabilized group connect-es-connector generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,792] INFO [GroupCoordinator 0]: Assignment received from leader connector-consumer-es-connector-0-b4931254-87e1-460a-b530-3255a380fe42 for group connect-es-connector for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:21,871] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:21,871] INFO [es-connector|task-0] Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:33:21,874] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:21,875] DEBUG [es-connector|task-0] Putting 6 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:21,984] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:22,083] DEBUG [es-connector|task-0] preCommitting offsets {test-0=OffsetAndMetadata{offset=10, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:33:22,089] DEBUG [es-connector|task-0] Stopping Elasticsearch client. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:146)
[2025-10-07 11:33:22,091] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 1 (__consumer_offsets-23) (reason: Removing member connector-consumer-es-connector-0-b4931254-87e1-460a-b530-3255a380fe42 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:22,091] INFO [GroupCoordinator 0]: Group connect-es-connector with generation 2 is now empty (__consumer_offsets-23) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:22,091] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connector-consumer-es-connector-0-b4931254-87e1-460a-b530-3255a380fe42, groupInstanceId=None, clientId=connector-consumer-es-connector-0, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group connect-es-connector through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:22,096] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 3 (__consumer_offsets-4) (reason: Removing member connect-5-43cd7727-d611-43b9-8869-2b5df081ad42 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:22,096] INFO [GroupCoordinator 0]: Group connect-integration-test-elasticsearch-it-connect-cluster with generation 4 is now empty (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:22,097] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connect-5-43cd7727-d611-43b9-8869-2b5df081ad42, groupInstanceId=None, clientId=connect-5, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(sessioned, compatible, default)) has left group connect-integration-test-elasticsearch-it-connect-cluster through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:22,110] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer:66)
[2025-10-07 11:33:22,110] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer:66)
[2025-10-07 11:33:22,111] INFO [Controller id=0] Shutting down broker 0 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:22,112] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:22,112] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer:66)
[2025-10-07 11:33:22,113] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:22,113] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:22,113] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:22,113] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:22,114] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:22,114] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 11:33:22,114] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 11:33:22,115] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,214] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,214] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,214] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis:66)
[2025-10-07 11:33:22,214] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,414] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,414] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,414] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:22,414] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager:66)
[2025-10-07 11:33:22,415] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:22,415] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:22,415] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:22,415] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:22,415] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:22,415] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,614] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,614] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,614] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,785] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,785] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,785] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:22,786] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager:66)
[2025-10-07 11:33:22,786] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:22,786] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:22,786] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:22,786] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:22,787] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:22,787] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 11:33:22,787] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 11:33:22,787] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,797] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,797] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,797] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,814] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,814] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:22,814] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:23,014] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:23,014] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:23,014] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:23,214] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:23,214] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:23,221] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager:66)
[2025-10-07 11:33:23,221] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:23,221] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:23,221] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:23,222] INFO Broker to controller channel manager for alterIsr shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 11:33:23,222] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:23,222] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:23,222] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:23,222] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 11:33:23,223] INFO Shutting down. (kafka.log.LogManager:66)
[2025-10-07 11:33:23,223] INFO Shutting down the log cleaner. (kafka.log.LogCleaner:66)
[2025-10-07 11:33:23,223] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner:66)
[2025-10-07 11:33:23,223] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner:66)
[2025-10-07 11:33:23,223] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner:66)
[2025-10-07 11:33:23,231] INFO [ProducerStateManager partition=test-0] Wrote producer snapshot at offset 10 with 1 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:23,242] INFO [ProducerStateManager partition=__consumer_offsets-23] Wrote producer snapshot at offset 3 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:23,249] INFO [ProducerStateManager partition=connect-config-topic-elasticsearch-it-connect-cluster-0] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:23,257] INFO [ProducerStateManager partition=__consumer_offsets-4] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:23,264] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-2] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:23,274] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-3] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:23,287] INFO Shutdown complete. (kafka.log.LogManager:66)
[2025-10-07 11:33:23,287] INFO [ControllerEventThread controllerId=0] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:23,287] INFO [ControllerEventThread controllerId=0] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:23,287] INFO [ControllerEventThread controllerId=0] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:23,288] INFO [PartitionStateMachine controllerId=0] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:33:23,289] INFO [ReplicaStateMachine controllerId=0] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:23,289] INFO [RequestSendThread controllerId=0] Shutting down (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:23,289] INFO [RequestSendThread controllerId=0] Stopped (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:23,289] INFO [RequestSendThread controllerId=0] Shutdown completed (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:23,289] INFO [Controller id=0] Resigned (kafka.controller.KafkaController:66)
[2025-10-07 11:33:23,289] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:23,290] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:23,290] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:23,290] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:23,392] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:23,392] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:23,707] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:23,707] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:23,707] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:24,707] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:24,707] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:24,708] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:24,711] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:24,711] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:24,712] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:25,711] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:25,711] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:25,711] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer:66)
[2025-10-07 11:33:25,718] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer:66)
[2025-10-07 11:33:25,719] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats:66)
[2025-10-07 11:33:25,719] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer:66)
[2025-10-07 11:33:25,747] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster6102101804247647423
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:36437
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:33:25,749] INFO starting (kafka.server.KafkaServer:66)
[2025-10-07 11:33:25,749] INFO Connecting to zookeeper on 127.0.0.1:36437 (kafka.server.KafkaServer:66)
[2025-10-07 11:33:25,749] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:36437. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:25,750] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:25,753] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:25,767] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:25,769] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 11:33:25,769] INFO Cleared cache (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 11:33:25,772] INFO Cluster ID = WNcBEss6RRmRbVMR58FPTg (kafka.server.KafkaServer:66)
[2025-10-07 11:33:25,772] WARN No meta.properties file under dir /tmp/EmbeddedKafkaCluster6102101804247647423/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2025-10-07 11:33:25,774] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster6102101804247647423
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:36437
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:33:25,776] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster6102101804247647423
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:36437
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 11:33:25,784] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:25,785] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:25,786] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:25,786] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:25,788] INFO Loading logs from log dirs ArraySeq(/tmp/EmbeddedKafkaCluster6102101804247647423) (kafka.log.LogManager:66)
[2025-10-07 11:33:25,788] INFO Attempting recovery for all logs in /tmp/EmbeddedKafkaCluster6102101804247647423 since no clean shutdown file was found (kafka.log.LogManager:66)
[2025-10-07 11:33:25,788] INFO Loaded 0 logs in 0ms. (kafka.log.LogManager:66)
[2025-10-07 11:33:25,788] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:66)
[2025-10-07 11:33:25,788] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:66)
[2025-10-07 11:33:25,789] INFO Starting the log cleaner (kafka.log.LogCleaner:66)
[2025-10-07 11:33:25,824] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner:66)
[2025-10-07 11:33:25,826] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:25,833] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas:66)
[2025-10-07 11:33:25,834] INFO Awaiting socket connections on localhost:44027. (kafka.network.Acceptor:66)
[2025-10-07 11:33:25,836] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 11:33:25,837] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:25,838] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:25,838] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:25,838] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:25,839] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:25,840] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:25,842] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:25,843] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1759836805842,1759836805842,1,0,0,72057690902757376,204,0,25
 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:25,843] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:44027, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:25,852] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:25,852] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:25,853] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:25,853] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:25,853] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:25,854] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:25,854] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient:66)
[2025-10-07 11:33:25,854] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:25,855] INFO [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,855] INFO [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{}) (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,855] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:25,856] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:25,856] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 11:33:25,856] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:25,857] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 11:33:25,857] INFO [Controller id=0] Registering handlers (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,858] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:25,859] INFO [Controller id=0] Deleting log dir event notifications (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,859] INFO [Controller id=0] Deleting isr change notifications (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,860] INFO [Controller id=0] Initializing controller context (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,860] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:25,861] INFO [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25) (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,861] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 11:33:25,861] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:25,862] INFO [KafkaServer id=0] started (kafka.server.KafkaServer:66)
[2025-10-07 11:33:25,862] INFO [Controller id=0] Currently active brokers in the cluster: Set(0) (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,863] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:25,863] INFO [Controller id=0] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,863] INFO [Controller id=0] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,863] INFO [Controller id=0] Fetching topic deletions in progress (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,864] INFO [Controller id=0] List of topics to be deleted:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,864] INFO [Controller id=0] List of topics ineligible for deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,864] INFO [Controller id=0] Initializing topic deletion manager (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,864] INFO [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager:66)
[2025-10-07 11:33:25,864] INFO [Controller id=0] Sending update metadata request (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,864] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:25,865] INFO [ReplicaStateMachine controllerId=0] Initializing replica state (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:25,865] INFO [ReplicaStateMachine controllerId=0] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:25,865] INFO [ReplicaStateMachine controllerId=0] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:25,865] INFO [PartitionStateMachine controllerId=0] Initializing partition state (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:33:25,865] INFO [PartitionStateMachine controllerId=0] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:33:25,865] INFO [RequestSendThread controllerId=0] Controller 0 connected to localhost:44027 (id: 0 rack: null) for sending state change requests (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:25,865] INFO [Controller id=0] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,867] INFO [Controller id=0] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,867] INFO [Controller id=0] Partitions that completed preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,867] INFO [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,867] INFO [Controller id=0] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,867] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,868] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController:66)
[2025-10-07 11:33:25,926] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker localhost:44027 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:25,938] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:44027 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:25,941] INFO [Controller id=0] Acquired new producerId block ProducerIdsBlock{brokerId=0, producerIdStart=0, producerIdLen=1000} by writing to Zk with path version 1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:27,500] INFO Creating topic connect-offset-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:27,503] INFO [Controller id=0] New topics: [Set(connect-offset-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-offset-topic-elasticsearch-it-connect-cluster,Some(ls7BpVHTQd-DXVBQQLPVyw),HashMap(connect-offset-topic-elasticsearch-it-connect-cluster-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0] New partition creation callback for connect-offset-topic-elasticsearch-it-connect-cluster-20,connect-offset-topic-elasticsearch-it-connect-cluster-7,connect-offset-topic-elasticsearch-it-connect-cluster-0,connect-offset-topic-elasticsearch-it-connect-cluster-12,connect-offset-topic-elasticsearch-it-connect-cluster-13,connect-offset-topic-elasticsearch-it-connect-cluster-5,connect-offset-topic-elasticsearch-it-connect-cluster-21,connect-offset-topic-elasticsearch-it-connect-cluster-14,connect-offset-topic-elasticsearch-it-connect-cluster-6,connect-offset-topic-elasticsearch-it-connect-cluster-1,connect-offset-topic-elasticsearch-it-connect-cluster-19,connect-offset-topic-elasticsearch-it-connect-cluster-17,connect-offset-topic-elasticsearch-it-connect-cluster-16,connect-offset-topic-elasticsearch-it-connect-cluster-23,connect-offset-topic-elasticsearch-it-connect-cluster-10,connect-offset-topic-elasticsearch-it-connect-cluster-3,connect-offset-topic-elasticsearch-it-connect-cluster-15,connect-offset-topic-elasticsearch-it-connect-cluster-22,connect-offset-topic-elasticsearch-it-connect-cluster-24,connect-offset-topic-elasticsearch-it-connect-cluster-18,connect-offset-topic-elasticsearch-it-connect-cluster-2,connect-offset-topic-elasticsearch-it-connect-cluster-4,connect-offset-topic-elasticsearch-it-connect-cluster-8,connect-offset-topic-elasticsearch-it-connect-cluster-9,connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,504] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,505] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,505] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,505] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,505] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,505] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,505] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,505] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,505] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,505] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:27,505] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,517] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 25 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:27,518] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 25 partitions (state.change.logger:66)
[2025-10-07 11:33:27,518] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:27,519] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
Oct 07, 2025 11:33:27 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Oct 07, 2025 11:33:27 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Oct 07, 2025 11:33:27 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Oct 07, 2025 11:33:27 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
[2025-10-07 11:33:27,521] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-offset-topic-elasticsearch-it-connect-cluster-20, connect-offset-topic-elasticsearch-it-connect-cluster-7, connect-offset-topic-elasticsearch-it-connect-cluster-0, connect-offset-topic-elasticsearch-it-connect-cluster-12, connect-offset-topic-elasticsearch-it-connect-cluster-13, connect-offset-topic-elasticsearch-it-connect-cluster-5, connect-offset-topic-elasticsearch-it-connect-cluster-21, connect-offset-topic-elasticsearch-it-connect-cluster-14, connect-offset-topic-elasticsearch-it-connect-cluster-6, connect-offset-topic-elasticsearch-it-connect-cluster-1, connect-offset-topic-elasticsearch-it-connect-cluster-19, connect-offset-topic-elasticsearch-it-connect-cluster-17, connect-offset-topic-elasticsearch-it-connect-cluster-16, connect-offset-topic-elasticsearch-it-connect-cluster-23, connect-offset-topic-elasticsearch-it-connect-cluster-10, connect-offset-topic-elasticsearch-it-connect-cluster-3, connect-offset-topic-elasticsearch-it-connect-cluster-15, connect-offset-topic-elasticsearch-it-connect-cluster-22, connect-offset-topic-elasticsearch-it-connect-cluster-24, connect-offset-topic-elasticsearch-it-connect-cluster-18, connect-offset-topic-elasticsearch-it-connect-cluster-2, connect-offset-topic-elasticsearch-it-connect-cluster-4, connect-offset-topic-elasticsearch-it-connect-cluster-8, connect-offset-topic-elasticsearch-it-connect-cluster-9, connect-offset-topic-elasticsearch-it-connect-cluster-11) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:27,521] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 25 partitions (state.change.logger:66)
[2025-10-07 11:33:27,523] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-10, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,524] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-10 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,525] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,525] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,525] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,537] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-14, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,537] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-14 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,537] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,537] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,537] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,548] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-18, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,549] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-18 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,549] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,549] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,549] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 11:33:27 AM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

[2025-10-07 11:33:27,555] INFO Creating topic test with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:27,559] INFO [Controller id=0] New topics: [Set(test)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(test,Some(K4dWpruFTJaz0p2CEl9T5A),Map(test-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:27,559] INFO [Controller id=0] New partition creation callback for test-0 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:27,559] INFO [Controller id=0 epoch=1] Changed partition test-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,559] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:27,559] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:27,560] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,560] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,560] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,560] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,560] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,563] INFO [Controller id=0 epoch=1] Changed partition test-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,563] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:27,564] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:27,564] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:27,571] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-22, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,571] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-22 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,571] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,571] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,571] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,581] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-7, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,582] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-7 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,582] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,582] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,582] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,592] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-11, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,593] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-11 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,593] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,593] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,593] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,604] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-15, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,604] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-15 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,604] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,604] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,604] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,615] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,615] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,615] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,615] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,615] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,622] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-19, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,623] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-19 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,623] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,623] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,623] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,634] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,634] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,634] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,634] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,634] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,645] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-23, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,645] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-23 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,645] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,645] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,645] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,656] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-8, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,656] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-8 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,656] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,656] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,656] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,666] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-12, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,667] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-12 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,667] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,667] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,667] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,677] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-16, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,678] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-16 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,678] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,678] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,678] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,689] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,689] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,689] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,689] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,689] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,699] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-20, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,700] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-20 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,700] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,700] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,700] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,711] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-5, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,711] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-5 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,711] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,711] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,711] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,718] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-24, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,718] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-24 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,718] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,718] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,718] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,725] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-9, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,725] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-9 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,725] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,725] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,725] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,736] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-13, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,736] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-13 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,736] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,736] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,736] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,747] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-17, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,747] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-17 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,747] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,747] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,747] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,758] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,758] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,758] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,758] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,758] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,769] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-21, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,769] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-21 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,769] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,769] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,769] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,780] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-6, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,780] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-offset-topic-elasticsearch-it-connect-cluster-6 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,780] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,780] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,780] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,791] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 11:33:27,792] INFO [Broker id=0] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger:66)
[2025-10-07 11:33:27,794] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:27,794] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:27,795] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:27,797] INFO [LogLoader partition=test-0, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,797] INFO Created log for partition test-0 in /tmp/EmbeddedKafkaCluster6102101804247647423/test-0 with properties {} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,797] INFO [Partition test-0 broker=0] No checkpointed highwatermark is found for partition test-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,797] INFO [Partition test-0 broker=0] Log loaded for partition test-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,797] INFO [Broker id=0] Leader test-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,808] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:27,808] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger:66)
[2025-10-07 11:33:27,808] INFO Creating topic connect-storage-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:27,812] INFO [Controller id=0] New topics: [Set(connect-storage-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-storage-topic-elasticsearch-it-connect-cluster,Some(E3E9-pSxSLOmmcTpTGVZkw),HashMap(connect-storage-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:27,812] INFO [Controller id=0] New partition creation callback for connect-storage-topic-elasticsearch-it-connect-cluster-2,connect-storage-topic-elasticsearch-it-connect-cluster-0,connect-storage-topic-elasticsearch-it-connect-cluster-3,connect-storage-topic-elasticsearch-it-connect-cluster-4,connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:27,812] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,812] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,812] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,812] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,812] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,812] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:27,813] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:27,817] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,817] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,817] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,817] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,817] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,817] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:27,817] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:27,818] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:27,821] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:27,822] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-storage-topic-elasticsearch-it-connect-cluster-2, connect-storage-topic-elasticsearch-it-connect-cluster-0, connect-storage-topic-elasticsearch-it-connect-cluster-3, connect-storage-topic-elasticsearch-it-connect-cluster-4, connect-storage-topic-elasticsearch-it-connect-cluster-1) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:27,822] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:27,828] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,828] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-storage-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,828] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,828] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,828] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,838] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,839] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-storage-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,839] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,839] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,839] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,849] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,850] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-storage-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,850] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,850] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,850] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,860] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,861] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-storage-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,861] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,861] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,861] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,871] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,872] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-storage-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,872] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,872] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,872] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,881] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 11:33:27,882] INFO [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6 (state.change.logger:66)
[2025-10-07 11:33:27,894] INFO Creating topic connect-config-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:27,897] INFO [Controller id=0] New topics: [Set(connect-config-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-config-topic-elasticsearch-it-connect-cluster,Some(LLLLiE3wRDSHJthZI1KERQ),Map(connect-config-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:27,897] INFO [Controller id=0] New partition creation callback for connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:27,897] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,897] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:27,898] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:27,899] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,899] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:27,899] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:27,900] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:27,900] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:27,900] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(connect-config-topic-elasticsearch-it-connect-cluster-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:27,900] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:27,901] INFO [LogLoader partition=connect-config-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,902] INFO Created log for partition connect-config-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster6102101804247647423/connect-config-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,902] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,902] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-config-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,902] INFO [Broker id=0] Leader connect-config-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,911] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 11:33:27,911] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger:66)
[2025-10-07 11:33:27,935] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 11:33:27,938] INFO [Controller id=0] New topics: [HashSet(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(bmgJM7IHQDePr1fa87JuiA),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 11:33:27,938] INFO [Controller id=0] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:27,938] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,939] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,940] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,940] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,940] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,940] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,940] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,940] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,940] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 11:33:27,940] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:27,940] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:27,951] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,951] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,951] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,951] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,951] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,951] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,951] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,951] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,951] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,951] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,951] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,951] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,951] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,951] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 11:33:27,952] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 50 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 11:33:27,953] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:27,953] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:27,953] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:27,956] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:27,956] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 0 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:27,957] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,958] INFO Created log for partition __consumer_offsets-3 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,958] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,958] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,958] INFO [Broker id=0] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,968] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,968] INFO Created log for partition __consumer_offsets-18 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,968] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,968] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,968] INFO [Broker id=0] Leader __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,978] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,978] INFO Created log for partition __consumer_offsets-41 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,978] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,978] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,978] INFO [Broker id=0] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,988] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,989] INFO Created log for partition __consumer_offsets-10 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,989] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,989] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,989] INFO [Broker id=0] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:27,999] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:27,999] INFO Created log for partition __consumer_offsets-33 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:27,999] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,999] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:27,999] INFO [Broker id=0] Leader __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,009] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,010] INFO Created log for partition __consumer_offsets-48 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,010] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,010] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,010] INFO [Broker id=0] Leader __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,020] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,020] INFO Created log for partition __consumer_offsets-19 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,020] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,020] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,021] INFO [Broker id=0] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,031] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,031] INFO Created log for partition __consumer_offsets-34 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,031] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,031] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,031] INFO [Broker id=0] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,042] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,042] INFO Created log for partition __consumer_offsets-4 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,042] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,042] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,042] INFO [Broker id=0] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,053] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,053] INFO Created log for partition __consumer_offsets-11 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,053] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,053] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,053] INFO [Broker id=0] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,063] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,063] INFO Created log for partition __consumer_offsets-26 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,063] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,063] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,063] INFO [Broker id=0] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,073] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,073] INFO Created log for partition __consumer_offsets-49 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,074] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,074] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,074] INFO [Broker id=0] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,084] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,084] INFO Created log for partition __consumer_offsets-39 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,084] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,084] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,084] INFO [Broker id=0] Leader __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,094] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,095] INFO Created log for partition __consumer_offsets-9 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,095] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,095] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,095] INFO [Broker id=0] Leader __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,105] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,105] INFO Created log for partition __consumer_offsets-24 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,105] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,105] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,105] INFO [Broker id=0] Leader __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,115] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,115] INFO Created log for partition __consumer_offsets-31 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,115] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,116] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,116] INFO [Broker id=0] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,126] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,126] INFO Created log for partition __consumer_offsets-46 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,126] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,126] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,126] INFO [Broker id=0] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,137] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,137] INFO Created log for partition __consumer_offsets-1 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,137] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,137] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,137] INFO [Broker id=0] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,147] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,148] INFO Created log for partition __consumer_offsets-16 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,148] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,148] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,148] INFO [Broker id=0] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,158] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,158] INFO Created log for partition __consumer_offsets-2 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,158] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,158] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,159] INFO [Broker id=0] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,169] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,169] INFO Created log for partition __consumer_offsets-25 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,169] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,169] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,169] INFO [Broker id=0] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,180] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,180] INFO Created log for partition __consumer_offsets-40 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,180] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,180] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,180] INFO [Broker id=0] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,191] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,191] INFO Created log for partition __consumer_offsets-47 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,191] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,191] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,191] INFO [Broker id=0] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,201] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,202] INFO Created log for partition __consumer_offsets-17 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,202] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,202] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,202] INFO [Broker id=0] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,213] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,213] INFO Created log for partition __consumer_offsets-32 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,213] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,213] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,213] INFO [Broker id=0] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,224] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,224] INFO Created log for partition __consumer_offsets-37 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,224] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,224] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,225] INFO [Broker id=0] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,235] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,236] INFO Created log for partition __consumer_offsets-7 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,236] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,236] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,236] INFO [Broker id=0] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,246] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,246] INFO Created log for partition __consumer_offsets-22 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,246] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,246] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,246] INFO [Broker id=0] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,257] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,257] INFO Created log for partition __consumer_offsets-29 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,257] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,257] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,257] INFO [Broker id=0] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,268] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,268] INFO Created log for partition __consumer_offsets-44 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,268] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,268] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,268] INFO [Broker id=0] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,278] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,278] INFO Created log for partition __consumer_offsets-14 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,278] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,278] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,278] INFO [Broker id=0] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,289] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,289] INFO Created log for partition __consumer_offsets-23 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,289] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,289] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,289] INFO [Broker id=0] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,299] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,299] INFO Created log for partition __consumer_offsets-38 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,299] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,300] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,300] INFO [Broker id=0] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,311] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,311] INFO Created log for partition __consumer_offsets-8 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,311] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,311] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,311] INFO [Broker id=0] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,321] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,321] INFO Created log for partition __consumer_offsets-45 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,321] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,321] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,321] INFO [Broker id=0] Leader __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,328] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,328] INFO Created log for partition __consumer_offsets-15 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,328] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,328] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,328] INFO [Broker id=0] Leader __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,339] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,339] INFO Created log for partition __consumer_offsets-30 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,339] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,339] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,339] INFO [Broker id=0] Leader __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,349] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,350] INFO Created log for partition __consumer_offsets-0 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,350] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,350] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,350] INFO [Broker id=0] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,361] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,361] INFO Created log for partition __consumer_offsets-35 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,361] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,361] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,361] INFO [Broker id=0] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,371] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,371] INFO Created log for partition __consumer_offsets-5 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,372] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,372] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,372] INFO [Broker id=0] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,379] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,379] INFO Created log for partition __consumer_offsets-20 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,379] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,379] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,379] INFO [Broker id=0] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,385] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,386] INFO Created log for partition __consumer_offsets-27 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,386] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,386] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,386] INFO [Broker id=0] Leader __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,396] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,396] INFO Created log for partition __consumer_offsets-42 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,396] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,396] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,396] INFO [Broker id=0] Leader __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,407] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,407] INFO Created log for partition __consumer_offsets-12 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,407] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,407] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,407] INFO [Broker id=0] Leader __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,418] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,418] INFO Created log for partition __consumer_offsets-21 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,418] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,418] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,418] INFO [Broker id=0] Leader __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,428] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,428] INFO Created log for partition __consumer_offsets-36 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,428] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,429] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,429] INFO [Broker id=0] Leader __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,439] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,440] INFO Created log for partition __consumer_offsets-6 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,440] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,440] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,440] INFO [Broker id=0] Leader __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,450] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,450] INFO Created log for partition __consumer_offsets-43 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,450] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,450] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,451] INFO [Broker id=0] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,461] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,461] INFO Created log for partition __consumer_offsets-13 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,462] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,462] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,462] INFO [Broker id=0] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,472] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/EmbeddedKafkaCluster6102101804247647423] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 11:33:28,472] INFO Created log for partition __consumer_offsets-28 in /tmp/EmbeddedKafkaCluster6102101804247647423/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 11:33:28,472] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,472] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 11:33:28,472] INFO [Broker id=0] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 11:33:28,482] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,482] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,482] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,482] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,482] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,482] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,482] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,482] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,483] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,483] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,483] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,483] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,483] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,483] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,483] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,483] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,483] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,483] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,483] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,484] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,485] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,486] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [Broker id=0] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 10 (state.change.logger:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,489] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 11:33:28,543] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-integration-test-elasticsearch-it-connect-cluster in Empty state. Created a new member id connect-6-fe27b36a-862c-4ae7-a00a-9e4c96d16110 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,544] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member connect-6-fe27b36a-862c-4ae7-a00a-9e4c96d16110 with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,545] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 1 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,548] INFO [GroupCoordinator 0]: Assignment received from leader connect-6-fe27b36a-862c-4ae7-a00a-9e4c96d16110 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,555] INFO ElasticsearchSinkConnectorConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:40685/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 600000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 600000
	max.buffered.records = 10
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 600000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 11:33:28,557] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:33:28,557] INFO Using unsecured connection to [http://localhost:40685]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:33:28,571] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 1 (__consumer_offsets-4) (reason: Leader connect-6-fe27b36a-862c-4ae7-a00a-9e4c96d16110 re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,572] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 2 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,574] INFO [GroupCoordinator 0]: Assignment received from leader connect-6-fe27b36a-862c-4ae7-a00a-9e4c96d16110 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 2. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,575] INFO [es-connector|worker] ElasticsearchSinkConnectorConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:40685/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 600000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 600000
	max.buffered.records = 10
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 600000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 11:33:28,584] ERROR Could not check connector state info. (io.confluent.connect.elasticsearch.integration.BaseConnectorIT:92)
[2025-10-07 11:33:28,587] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 2 (__consumer_offsets-4) (reason: Leader connect-6-fe27b36a-862c-4ae7-a00a-9e4c96d16110 re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,587] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 3 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,588] INFO [GroupCoordinator 0]: Assignment received from leader connect-6-fe27b36a-862c-4ae7-a00a-9e4c96d16110 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,593] INFO [es-connector|task-0] Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 11:33:28,594] INFO [es-connector|task-0] ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:40685/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 600000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 600000
	max.buffered.records = 10
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 600000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 0
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 11:33:28,594] INFO [es-connector|task-0] Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 11:33:28,594] DEBUG [es-connector|task-0] Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 11:33:28,594] INFO [es-connector|task-0] Using unsecured connection to [http://localhost:40685]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 11:33:28,602] INFO [es-connector|task-0] Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 11:33:28,607] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-es-connector in Empty state. Created a new member id connector-consumer-es-connector-0-9252e462-737e-4e6e-b948-8a3d2b2518e8 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,607] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member connector-consumer-es-connector-0-9252e462-737e-4e6e-b948-8a3d2b2518e8 with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,608] INFO [GroupCoordinator 0]: Stabilized group connect-es-connector generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,611] INFO [GroupCoordinator 0]: Assignment received from leader connector-consumer-es-connector-0-9252e462-737e-4e6e-b948-8a3d2b2518e8 for group connect-es-connector for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:28,698] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,698] INFO [es-connector|task-0] Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 11:33:28,800] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,801] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,802] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,806] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,812] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,814] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,815] DEBUG [es-connector|task-0] Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,816] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,817] DEBUG [es-connector|task-0] Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,818] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,819] DEBUG [es-connector|task-0] Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,820] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,821] DEBUG [es-connector|task-0] Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,822] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,823] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,823] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,824] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,825] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,825] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,826] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,827] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,828] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,828] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,830] DEBUG [es-connector|task-0] Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,831] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,832] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,833] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,833] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,834] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,835] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,835] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,836] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,836] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,837] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,838] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,838] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,839] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,839] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,840] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,841] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,841] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,842] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,842] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,843] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,844] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,844] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,845] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,845] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,846] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,847] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,847] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,849] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,849] DEBUG [es-connector|task-0] Putting 4 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,851] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,851] DEBUG [es-connector|task-0] Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,852] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,853] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,854] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,854] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:28,855] DEBUG [es-connector|task-0] Pausing all partitions (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:363)
[2025-10-07 11:33:28,955] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:29,056] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:29,156] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:29,256] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:29,357] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:29,457] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:29,558] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:29,658] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:29,759] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:29,859] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:29,959] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:30,060] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:30,160] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:30,261] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:30,361] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:30,461] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:30,562] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:30,662] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:30,763] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:30,863] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:30,868] INFO [Controller id=0] Processing automatic preferred replica leader election (kafka.controller.KafkaController:66)
[2025-10-07 11:33:30,963] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:31,064] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:31,164] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:31,264] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:31,284] ERROR [es-connector|task-0] Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:40685], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];' after 1 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:40685], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:40685], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 11:33:31,284] WARN [es-connector|task-0] Bulk request 1 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:40685], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];' after 1 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:40685], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:40685], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 11:33:31,365] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 11:33:31,366] ERROR [es-connector|task-0] WorkerSinkTask{id=es-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Bulk request failed (org.apache.kafka.connect.runtime.WorkerSinkTask:629)
org.apache.kafka.connect.errors.ConnectException: Bulk request failed
	at io.confluent.connect.elasticsearch.ElasticsearchClient$2.afterBulk(ElasticsearchClient.java:455)
	at org.elasticsearch.action.bulk.BulkRequestHandler$1.onFailure(BulkRequestHandler.java:64)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.ActionListener$RunAfterActionListener.onFailure(ActionListener.java:350)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onFailure(Retry.java:123)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:227)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:40685], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];' after 1 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	... 5 more
Caused by: ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:40685], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:40685], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 11:33:31,368] DEBUG [es-connector|task-0] Tried to flush data to Elasticsearch, but BulkProcessor is already closed. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:137)
java.lang.IllegalStateException: bulk process already closed
	at org.elasticsearch.action.bulk.BulkProcessor.ensureOpen(BulkProcessor.java:454)
	at org.elasticsearch.action.bulk.BulkProcessor.flush(BulkProcessor.java:571)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.flush(ElasticsearchClient.java:303)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.preCommit(ElasticsearchSinkTask.java:135)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:404)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.closePartitions(WorkerSinkTask.java:666)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.closeAllPartitions(WorkerSinkTask.java:661)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:204)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[2025-10-07 11:33:31,368] DEBUG [es-connector|task-0] preCommitting offsets {} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 11:33:31,368] ERROR [es-connector|task-0] WorkerSinkTask{id=es-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:195)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:631)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:333)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:234)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:203)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.kafka.connect.errors.ConnectException: Bulk request failed
	at io.confluent.connect.elasticsearch.ElasticsearchClient$2.afterBulk(ElasticsearchClient.java:455)
	at org.elasticsearch.action.bulk.BulkRequestHandler$1.onFailure(BulkRequestHandler.java:64)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.ActionListener$RunAfterActionListener.onFailure(ActionListener.java:350)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onFailure(Retry.java:123)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:227)
	... 5 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:40685], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];' after 1 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	... 5 more
Caused by: ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:40685], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:40685], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 11:33:31,368] DEBUG [es-connector|task-0] Stopping Elasticsearch client. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:146)
[2025-10-07 11:33:31,369] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 1 (__consumer_offsets-23) (reason: Removing member connector-consumer-es-connector-0-9252e462-737e-4e6e-b948-8a3d2b2518e8 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:31,369] INFO [GroupCoordinator 0]: Group connect-es-connector with generation 2 is now empty (__consumer_offsets-23) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:31,370] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connector-consumer-es-connector-0-9252e462-737e-4e6e-b948-8a3d2b2518e8, groupInstanceId=None, clientId=connector-consumer-es-connector-0, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group connect-es-connector through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:31,406] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 3 (__consumer_offsets-4) (reason: Removing member connect-6-fe27b36a-862c-4ae7-a00a-9e4c96d16110 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:31,407] INFO [GroupCoordinator 0]: Group connect-integration-test-elasticsearch-it-connect-cluster with generation 4 is now empty (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:31,407] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connect-6-fe27b36a-862c-4ae7-a00a-9e4c96d16110, groupInstanceId=None, clientId=connect-6, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(sessioned, compatible, default)) has left group connect-integration-test-elasticsearch-it-connect-cluster through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:31,415] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer:66)
[2025-10-07 11:33:31,415] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer:66)
[2025-10-07 11:33:31,416] INFO [Controller id=0] Shutting down broker 0 (kafka.controller.KafkaController:66)
[2025-10-07 11:33:31,417] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 11:33:31,417] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer:66)
[2025-10-07 11:33:31,418] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:31,418] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:31,418] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 11:33:31,418] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:31,419] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 11:33:31,419] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 11:33:31,420] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 11:33:31,420] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:31,506] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:31,506] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:31,507] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis:66)
[2025-10-07 11:33:31,507] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:31,707] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:31,707] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:31,707] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:31,707] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager:66)
[2025-10-07 11:33:31,707] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:31,707] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:31,707] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 11:33:31,708] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 11:33:31,708] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:31,708] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:31,907] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:31,907] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:31,907] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:32,014] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:32,014] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:32,014] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 11:33:32,015] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager:66)
[2025-10-07 11:33:32,015] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:32,015] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:32,015] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 11:33:32,015] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:32,015] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 11:33:32,015] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 11:33:32,015] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 11:33:32,015] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:32,021] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:32,021] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:32,021] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:32,107] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:32,107] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:32,107] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:32,307] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:32,307] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:32,308] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:32,507] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:32,507] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 11:33:32,514] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager:66)
[2025-10-07 11:33:32,514] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:32,515] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:32,515] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:32,515] INFO Broker to controller channel manager for alterIsr shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 11:33:32,515] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:32,515] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:32,515] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 11:33:32,515] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 11:33:32,516] INFO Shutting down. (kafka.log.LogManager:66)
[2025-10-07 11:33:32,516] INFO Shutting down the log cleaner. (kafka.log.LogCleaner:66)
[2025-10-07 11:33:32,516] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner:66)
[2025-10-07 11:33:32,516] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner:66)
[2025-10-07 11:33:32,516] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner:66)
[2025-10-07 11:33:32,529] INFO [ProducerStateManager partition=test-0] Wrote producer snapshot at offset 1001 with 1 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:32,539] INFO [ProducerStateManager partition=__consumer_offsets-23] Wrote producer snapshot at offset 2 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:32,546] INFO [ProducerStateManager partition=connect-config-topic-elasticsearch-it-connect-cluster-0] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:32,553] INFO [ProducerStateManager partition=__consumer_offsets-4] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:32,567] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-2] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:32,578] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-3] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 11:33:32,591] INFO Shutdown complete. (kafka.log.LogManager:66)
[2025-10-07 11:33:32,591] INFO [ControllerEventThread controllerId=0] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:32,591] INFO [ControllerEventThread controllerId=0] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:32,591] INFO [ControllerEventThread controllerId=0] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 11:33:32,592] INFO [PartitionStateMachine controllerId=0] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 11:33:32,593] INFO [ReplicaStateMachine controllerId=0] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 11:33:32,593] INFO [RequestSendThread controllerId=0] Shutting down (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:32,593] INFO [RequestSendThread controllerId=0] Stopped (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:32,593] INFO [RequestSendThread controllerId=0] Shutdown completed (kafka.controller.RequestSendThread:66)
[2025-10-07 11:33:32,593] INFO [Controller id=0] Resigned (kafka.controller.KafkaController:66)
[2025-10-07 11:33:32,593] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:32,593] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:32,593] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 11:33:32,594] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:32,696] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 11:33:32,696] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:32,785] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:32,785] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:32,785] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:32,786] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:32,786] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:32,786] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:32,786] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:32,786] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:32,786] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:32,787] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:32,787] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 11:33:32,787] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer:66)
[2025-10-07 11:33:32,792] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer:66)
[2025-10-07 11:33:32,792] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats:66)
[2025-10-07 11:33:32,792] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer:66)
[WARNING] Tests run: 7, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 52.665 s - in io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorNetworkIT
[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Errors: 
[ERROR]   ElasticsearchConnectorIT.setupBeforeAll:80 Â» ContainerLaunch Container startup...
[ERROR]   ElasticsearchConnectorKerberosIT.setupBeforeAll:37 Â» ContainerLaunch Container...
[ERROR]   ElasticsearchConnectorKerberosWithSslIT.setupBeforeAll:27 Â» ContainerLaunch Co...
[INFO] 
[ERROR] Tests run: 24, Failures: 0, Errors: 3, Skipped: 2
[INFO] 
[INFO] 
[INFO] --- failsafe:3.0.0-M3:verify (verify) @ kafka-connect-elasticsearch ---
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  20:00 min
[INFO] Finished at: 2025-10-07T11:33:33Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-failsafe-plugin:3.0.0-M3:verify (verify) on project kafka-connect-elasticsearch: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/semaphore/kafka-connect-elasticsearch/target/failsafe-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[0m[0mExporting environment variables
Exporting SEMAPHORE_JOB_RESULT
. publish-test-results
Collecting results from surefire-reports tests in each module
Collecting results from failsafe-reports tests in each module
  Copying failsafe-reports test logs from './target/failsafe-reports'
* Using generic parser
* Saving results to /tmp/test-results-2137168723/result-1038652449.json
* Using generic parser
* Saving results to /tmp/test-results-2137168723/result-3378191319.json
* Using generic parser
* Saving results to /tmp/test-results-2137168723/result-3702908015.json
* Using generic parser
* Saving results to /tmp/test-results-2137168723/result-2300882836.json
* Using generic parser
* Saving results to /tmp/test-results-2137168723/result-559597825.json
* Saving results to /tmp/test-results1895249158
* Pushing artifacts:
$ /usr/local/bin/artifact push job /tmp/test-results1895249158 -d test-results/junit.json
* starting to generate summary
* Saving results to /tmp/test-results3624263068
* Pushing artifacts:
$ /usr/local/bin/artifact push job /tmp/test-results3624263068 -d test-results/summary.json
* Pushing artifacts:
$ /usr/local/bin/artifact push workflow /tmp/test-results1895249158 -d test-results/c0b3c2ba-eb31-4c93-8260-f5f5d172d967/0f69c285-4bc3-4ea3-a492-451a8ac50a53.json
* Pushing artifacts:
$ /usr/local/bin/artifact push job target/test-results/TEST-io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT.xml -d test-results/junit-0.xml
* Pushing artifacts:
$ /usr/local/bin/artifact push job target/test-results/TEST-io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT.xml -d test-results/junit-1.xml
* Pushing artifacts:
$ /usr/local/bin/artifact push job target/test-results/TEST-io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT.xml -d test-results/junit-2.xml
* Pushing artifacts:
$ /usr/local/bin/artifact push job target/test-results/TEST-io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorNetworkIT.xml -d test-results/junit-3.xml
* Pushing artifacts:
$ /usr/local/bin/artifact push job target/test-results/TEST-io.confluent.connect.elasticsearch.integration.ElasticsearchSinkTaskIT.xml -d test-results/junit-4.xml
Publish tests to semaphore
artifact push workflow target/test-results
[Oct  7 11:33:36.470] Successfully pushed artifact for current workflow.
[Oct  7 11:33:36.470] * Local source: target/test-results.
[Oct  7 11:33:36.470] * Remote destination: artifacts/workflows/3a8750d6-c0a3-4380-b7e8-d60a38fab4f7/test-results.
artifact push workflow target
[Oct  7 11:33:55.323] Successfully pushed artifact for current workflow.
[Oct  7 11:33:55.323] * Local source: target.
[Oct  7 11:33:55.323] * Remote destination: artifacts/workflows/3a8750d6-c0a3-4380-b7e8-d60a38fab4f7/target.
Running the post-job hook configured in the agent
Running: bash /opt/semaphore/agent/hooks/post-job-hook
+ echo 'Running post-job script'
Running post-job script
+ [[ s1-prod-ubuntu24-04-amd64-4 == *\i\n\i\t\i\a\l\i\z\a\t\i\o\n ]]
+ case $SEMAPHORE_ORGANIZATION_URL in
+ cd /opt/semaphore/agent/hooks/
+ set +x
+ uv run --no-sync python post_job.py
2025-10-07 11:33:58,449 - root - INFO - Collecting pipeline metrics
2025-10-07 11:33:58,449 - root - INFO - Creating the event to be sent to Kafka
2025-10-07 11:33:58,449 - root - INFO - Job 0f69c285-4bc3-4ea3-a492-451a8ac50a53 started at 1759835586 and ran for 1252 seconds with result failed
2025-10-07 11:34:03,701 - root - INFO - CI metrics event: {'branch': 'CC-36923/ext_resource_usage_limit', 'commit': '037731caad6c35a7aebe2150b728e8ad0e666f21', 'author_name': 'airlock-confluentinc[bot]', 'repo': 'confluentinc/kafka-connect-elasticsearch', 'name': 'kafka-connect-elasticsearch', 'job_url': 'https://semaphore.ci.confluent.io/jobs/0f69c285-4bc3-4ea3-a492-451a8ac50a53', 'pipeline_url': 'https://semaphore.ci.confluent.io/workflows/3a8750d6-c0a3-4380-b7e8-d60a38fab4f7?pipeline_id=c0b3c2ba-eb31-4c93-8260-f5f5d172d967', 'job_id': '0f69c285-4bc3-4ea3-a492-451a8ac50a53', 'job_name': 'Test', 'job_type': 'unknown', 'job_status': 'failed', 'job_start_time': '1759835586', 'job_duration': 1252, 'pipeline_id': 'c0b3c2ba-eb31-4c93-8260-f5f5d172d967', 'ref_type': 'pull-request'}
2025-10-07 11:34:03,701 - root - INFO - Producing an event
2025-10-07 11:34:04,358 - root - INFO - Message delivered to ci_pipeline_metrics [6]
2025-10-07 11:34:04,358 - root - INFO - Message produced to Kafka broker
%6|1759836844.358|GETSUBSCRIPTIONS|rdkafka#producer-1| [thrd:main]: Telemetry client instance id changed from AAAAAAAAAAAAAAAAAAAAAA to P15KyWX5R7iNJQhFdfJ+8g
2025-10-07 11:34:04,360 - root - INFO - Collecting granular job metrics
{
    "resource_metrics": [
        {
            "resource": {
                "attributes": {
                    "telemetry.sdk.language": "python",
                    "telemetry.sdk.name": "opentelemetry",
                    "telemetry.sdk.version": "1.36.0",
                    "service.namespace": "ci-metrics",
                    "service.name": "post_job.py"
                },
                "schema_url": ""
            },
            "scope_metrics": [
                {
                    "scope": {
                        "name": "ci-metrics-meter",
                        "version": "0.1.0",
                        "schema_url": "",
                        "attributes": null
                    },
                    "metrics": [
                        {
                            "name": "ci_command_duration",
                            "description": "Histogram of duration for common commands in CI jobs",
                            "unit": "s",
                            "data": {
                                "data_points": [
                                    {
                                        "attributes": {
                                            "command": "Running the pre-job hook configured in the agent",
                                            "git_repo_name": "confluentinc/kafka-connect-elasticsearch",
                                            "pipeline_id": "c0b3c2ba-eb31-4c93-8260-f5f5d172d967",
                                            "job_id": "0f69c285-4bc3-4ea3-a492-451a8ac50a53",
                                            "job_name": "Test",
                                            "pipeline_name": "build-test-release",
                                            "branch_name": "CC-36923/ext_resource_usage_limit",
                                            "ref_type": "pull-request",
                                            "exit_code": 0,
                                            "job_result": "failed",
                                            "command_type": "pre-job",
                                            "workflow_rerun": "false"
                                        },
                                        "start_time_unix_nano": 1759836844577731196,
                                        "time_unix_nano": 1759836844577861999,
                                        "count": 1,
                                        "sum": 7.0,
                                        "bucket_counts": [
                                            0,
                                            0,
                                            1,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0
                                        ],
                                        "explicit_bounds": [
                                            0.0,
                                            5.0,
                                            10.0,
                                            25.0,
                                            50.0,
                                            75.0,
                                            100.0,
                                            250.0,
                                            500.0,
                                            750.0,
                                            1000.0,
                                            2500.0,
                                            5000.0,
                                            7500.0,
                                            10000.0
                                        ],
                                        "min": 7.0,
                                        "max": 7.0,
                                        "exemplars": []
                                    },
                                    {
                                        "attributes": {
                                            "command": "Running the post-job hook configured in the agent",
                                            "git_repo_name": "confluentinc/kafka-connect-elasticsearch",
                                            "pipeline_id": "c0b3c2ba-eb31-4c93-8260-f5f5d172d967",
                                            "job_id": "0f69c285-4bc3-4ea3-a492-451a8ac50a53",
                                            "job_name": "Test",
                                            "pipeline_name": "build-test-release",
                                            "branch_name": "CC-36923/ext_resource_usage_limit",
                                            "ref_type": "pull-request",
                                            "exit_code": 0,
                                            "job_result": "failed",
                                            "command_type": "post-job",
                                            "workflow_rerun": "false"
                                        },
                                        "start_time_unix_nano": 1759836844577796487,
                                        "time_unix_nano": 1759836844577861999,
                                        "count": 1,
                                        "sum": 9.0,
                                        "bucket_counts": [
                                            0,
                                            0,
                                            1,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0
                                        ],
                                        "explicit_bounds": [
                                            0.0,
                                            5.0,
                                            10.0,
                                            25.0,
                                            50.0,
                                            75.0,
                                            100.0,
                                            250.0,
                                            500.0,
                                            750.0,
                                            1000.0,
                                            2500.0,
                                            5000.0,
                                            7500.0,
                                            10000.0
                                        ],
                                        "min": 9.0,
                                        "max": 9.0,
                                        "exemplars": []
                                    }
                                ],
                                "aggregation_temporality": 2
                            }
                        }
                    ],
                    "schema_url": ""
                }
            ],
            "schema_url": ""
        }
    ]
}
+ set +exuo pipefail
Successfully completed post-job script