Exporting environment variables
Exporting CI
Exporting SEMAPHORE
Exporting SEMAPHORE_AGENT_MACHINE_ENVIRONMENT_TYPE
Exporting SEMAPHORE_AGENT_MACHINE_OS_IMAGE
Exporting SEMAPHORE_AGENT_MACHINE_TYPE
Exporting SEMAPHORE_ARTIFACT_TOKEN
Exporting SEMAPHORE_BLOCK_NAME
Exporting SEMAPHORE_CACHE_BACKEND
Exporting SEMAPHORE_CACHE_S3_BUCKET
Exporting SEMAPHORE_CACHE_USE_EC2_INSTANCE_PROFILE
Exporting SEMAPHORE_GIT_BRANCH
Exporting SEMAPHORE_GIT_COMMITTER
Exporting SEMAPHORE_GIT_COMMIT_AUTHOR
Exporting SEMAPHORE_GIT_COMMIT_RANGE
Exporting SEMAPHORE_GIT_DIR
Exporting SEMAPHORE_GIT_PROVIDER
Exporting SEMAPHORE_GIT_PR_BRANCH
Exporting SEMAPHORE_GIT_PR_NAME
Exporting SEMAPHORE_GIT_PR_NUMBER
Exporting SEMAPHORE_GIT_PR_SHA
Exporting SEMAPHORE_GIT_PR_SLUG
Exporting SEMAPHORE_GIT_REF
Exporting SEMAPHORE_GIT_REF_TYPE
Exporting SEMAPHORE_GIT_REPO_NAME
Exporting SEMAPHORE_GIT_REPO_SLUG
Exporting SEMAPHORE_GIT_SHA
Exporting SEMAPHORE_GIT_URL
Exporting SEMAPHORE_GIT_WORKING_BRANCH
Exporting SEMAPHORE_JOB_CREATION_TIME
Exporting SEMAPHORE_JOB_ID
Exporting SEMAPHORE_JOB_NAME
Exporting SEMAPHORE_JOB_TYPE
Exporting SEMAPHORE_OIDC_TOKEN
Exporting SEMAPHORE_ORGANIZATION_URL
Exporting SEMAPHORE_PIPELINE_0_ARTEFACT_ID
Exporting SEMAPHORE_PIPELINE_ARTEFACT_ID
Exporting SEMAPHORE_PIPELINE_ID
Exporting SEMAPHORE_PIPELINE_NAME
Exporting SEMAPHORE_PIPELINE_PROMOTED_BY
Exporting SEMAPHORE_PIPELINE_PROMOTION
Exporting SEMAPHORE_PIPELINE_RERUN
Exporting SEMAPHORE_PROJECT_ID
Exporting SEMAPHORE_PROJECT_NAME
Exporting SEMAPHORE_TOOLBOX_METRICS_ENABLED
Exporting SEMAPHORE_WORKFLOW_HOOK_SOURCE
Exporting SEMAPHORE_WORKFLOW_ID
Exporting SEMAPHORE_WORKFLOW_NUMBER
Exporting SEMAPHORE_WORKFLOW_RERUN
Exporting SEMAPHORE_WORKFLOW_TRIGGERED_BY
Exporting SEMAPHORE_WORKFLOW_TRIGGERED_BY_API
Exporting SEMAPHORE_WORKFLOW_TRIGGERED_BY_HOOK
Exporting SEMAPHORE_WORKFLOW_TRIGGERED_BY_MANUAL_RUN
Exporting SEMAPHORE_WORKFLOW_TRIGGERED_BY_SCHEDULE
Exporting TERM
Injecting Files
Injecting /home/semaphore/.ssh/id_rsa with file mode 0600
Running the pre-job hook configured in the agent
Running: source /opt/semaphore/agent/hooks/pre-job
Warning: The agent is configured to fail the job if the pre-job hook fails.
Fetching IMDS token
Fetching instance ID
Running on self hosted agent: i-0b1f0d891a53699a7
Fetching AMI ID
Running AMI: ami-0c28fc744e47a0000
Fetching region
Running in us-west-2
Running on agent: s1-prod-ubuntu24-04-amd64-4
Calculating job queue time
Queue time: 75 seconds
Pushing metrics to CloudWatch
Disabling EC2 instance profile for cache
Assuming Semaphore OIDC role
Additional logs and metrics on build instance:
Cloudwatch Logs: https://d-926757b88b.awsapps.com/start/#/console?account_id=519856050701&role_name=developer&destination=https%3A%2F%2Fconsole.aws.amazon.com%2Fcloudwatch%2Fhome%3Fregion%3Dus-west-2%23logsV2%3Alogs-insights%243FqueryDetail%243D%7E%28end%7E0%7Estart%7E-86400%7EtimeType%7E%27RELATIVE%7Etz%7E%27LOCAL%7Eunit%7E%27seconds%7EeditorString%7E%27fields*20*40timestamp*2c*20*40log*2c*20*40message*0a*7c*20sort*20*40timestamp*20desc*0a*7c*20limit*2010000*0a*7c*20filter*20*40logStream*20like*20*22i-0b1f0d891a53699a7*22%7EqueryId%7E%277ad2fb3c-d30c-4bdb-9d65-ea38e786c121%7Esource%7E%28%7E%27*2fsemaphore*2fagent%7E%27*2fsemaphore*2fkern.log%7E%27*2fsemaphore*2fsyslog%7E%27*2fsemaphore*2fsystem%29%7Elang%7E%27CWLI%29
Cloudwatch Metrics: https://d-926757b88b.awsapps.com/start/#/console?account_id=519856050701&role_name=developer&destination=https%3A%2F%2Fconsole.aws.amazon.com%2Fcloudwatch%2Fhome%3Fregion%3Dus-west-2%23metricsV2%3Fgraph%3D%7E%28%29%26query%3D%7E%27i-0b1f0d891a53699a7
Log and Metric FAQ Guide: https://confluentinc.atlassian.net/wiki/spaces/TOOLS/pages/1342058645/Semaphore+FAQ#System-logging-and-metrics-of-my-Semaphore-build-agent
Setting tags
[1]   Done                    aws cloudwatch put-metric-data --metric-name QueueTime --namespace Semaphore --value $queue_time --unit Seconds --dimensions "MachineType=$machine_type"
[2]   Done                    aws cloudwatch put-metric-data --metric-name QueueTime --namespace Semaphore --value $queue_time --unit Seconds --dimensions "MachineType=$machine_type,InstanceId=$instance_id"
Setting up uv environment for post-job
/home/semaphore
Setting up goproxy-aws
Getting CodeArtifact auth token
Ref Type is pull-request, generating a read-only token
Setting up .netrc
OS version: 24.04
Setting up Semaphore-Agent GitHub App variables
Setting up Vault V3 Agent for .netrc management
Setting up .gitconfig
Setting up Bazel downloader config
Setting up .bazelrc
[0m==> Note: Vault Agent version does not match Vault server version. Vault Agent version: 1.20.4, Vault server version: 1.18.5[0m
[0m==> Vault Agent started! Log data will stream in below:
[0m
[0m==> Vault Agent configuration:
[0m
[0m           Api Address 1: http://bufconn[0m
[0m                     Cgo: disabled[0m
[0m               Log Level: error[0m
[0m                 Version: Vault v1.20.4, built 2025-09-23T13:22:38Z[0m
[0m             Version Sha: 55bd8f18c6c84aa89fdede4850a622c57f03bd7e[0m
[0m[0m
Setting up CodeArtifact creds for Bazel
Setting up pip.conf
Setting up .pypirc
Setting up uv.toml
Setting up .npmrc
Setting up .yarnrc.yml
Setting up .ci-tools.ini
Running on EBS volume: vol-0c1ccef6553e4aef4
[34mDEBUG[39m uv 0.8.23
Completed setting up .ci-tools.ini
Setting up gradle.properties
[34mDEBUG[39m Acquired shared lock for `/home/semaphore/.cache/uv`
[34mDEBUG[39m Found project root: `/opt/semaphore/agent/hooks`
[34mDEBUG[39m No workspace root found, using project root
[34mDEBUG[39m Discovered project `ci-build-agent-infra` at: /opt/semaphore/agent/hooks
[34mDEBUG[39m Acquired lock for `/opt/semaphore/agent/hooks`
[34mDEBUG[39m No Python version file found in workspace: /opt/semaphore/agent/hooks
[34mDEBUG[39m Using Python request `==3.13.*` from `requires-python` metadata
[34mDEBUG[39m Checking for Python environment at: `.venv`

[3]   Done                    set_tags
Setting up settings.xml
Setting up coursier credentials.properties
Setting up Dockerhub creds
Setting up Confluent Catalog creds
Setting up Jira creds
[34mDEBUG[39m The project environment's Python version satisfies the request: `Python ==3.13.*`
[34mDEBUG[39m Released lock at `/tmp/uv-4f0123aad978f91a.lock`
[34mDEBUG[39m Skipping environment synchronization due to `--no-sync`
[34mDEBUG[39m Using Python 3.13.7 interpreter at: /opt/semaphore/agent/hooks/.venv/bin/python3
Setting up Docker BuildKit
Setting up Launchdarkly creds
[34mDEBUG[39m Running `python --version`
[34mDEBUG[39m Spawned child 3264 in process group 2887
Python 3.13.7
[34mDEBUG[39m Command exited with code: 0
[34mDEBUG[39m Released lock at `/home/semaphore/.cache/uv/.lock`
last 4 chars of the token: 921d
Successfully completed pre-job script
checkout
Cloning into 'kafka-connect-elasticsearch'...
remote: Enumerating objects: 7072, done.[K
remote: Counting objects:   0% (1/7072)[K
remote: Counting objects:   1% (71/7072)[K
remote: Counting objects:   2% (142/7072)[K
remote: Counting objects:   3% (213/7072)[K
remote: Counting objects:   4% (283/7072)[K
remote: Counting objects:   5% (354/7072)[K
remote: Counting objects:   6% (425/7072)[K
remote: Counting objects:   7% (496/7072)[K
remote: Counting objects:   8% (566/7072)[K
remote: Counting objects:   9% (637/7072)[K
remote: Counting objects:  10% (708/7072)[K
remote: Counting objects:  11% (778/7072)[K
remote: Counting objects:  12% (849/7072)[K
remote: Counting objects:  13% (920/7072)[K
remote: Counting objects:  14% (991/7072)[K
remote: Counting objects:  15% (1061/7072)[K
remote: Counting objects:  16% (1132/7072)[K
remote: Counting objects:  17% (1203/7072)[K
remote: Counting objects:  18% (1273/7072)[K
remote: Counting objects:  19% (1344/7072)[K
remote: Counting objects:  20% (1415/7072)[K
remote: Counting objects:  21% (1486/7072)[K
remote: Counting objects:  22% (1556/7072)[K
remote: Counting objects:  23% (1627/7072)[K
remote: Counting objects:  24% (1698/7072)[K
remote: Counting objects:  25% (1768/7072)[K
remote: Counting objects:  26% (1839/7072)[K
remote: Counting objects:  27% (1910/7072)[K
remote: Counting objects:  28% (1981/7072)[K
remote: Counting objects:  29% (2051/7072)[K
remote: Counting objects:  30% (2122/7072)[K
remote: Counting objects:  31% (2193/7072)[K
remote: Counting objects:  32% (2264/7072)[K
remote: Counting objects:  33% (2334/7072)[K
remote: Counting objects:  34% (2405/7072)[K
remote: Counting objects:  35% (2476/7072)[K
remote: Counting objects:  36% (2546/7072)[K
remote: Counting objects:  37% (2617/7072)[K
remote: Counting objects:  38% (2688/7072)[K
remote: Counting objects:  39% (2759/7072)[K
remote: Counting objects:  40% (2829/7072)[K
remote: Counting objects:  41% (2900/7072)[K
remote: Counting objects:  42% (2971/7072)[K
remote: Counting objects:  43% (3041/7072)[K
remote: Counting objects:  44% (3112/7072)[K
remote: Counting objects:  45% (3183/7072)[K
remote: Counting objects:  46% (3254/7072)[K
remote: Counting objects:  47% (3324/7072)[K
remote: Counting objects:  48% (3395/7072)[K
remote: Counting objects:  49% (3466/7072)[K
remote: Counting objects:  50% (3536/7072)[K
remote: Counting objects:  51% (3607/7072)[K
remote: Counting objects:  52% (3678/7072)[K
remote: Counting objects:  53% (3749/7072)[K
remote: Counting objects:  54% (3819/7072)[K
remote: Counting objects:  55% (3890/7072)[K
remote: Counting objects:  56% (3961/7072)[K
remote: Counting objects:  57% (4032/7072)[K
remote: Counting objects:  58% (4102/7072)[K
remote: Counting objects:  59% (4173/7072)[K
remote: Counting objects:  60% (4244/7072)[K
remote: Counting objects:  61% (4314/7072)[K
remote: Counting objects:  62% (4385/7072)[K
remote: Counting objects:  63% (4456/7072)[K
remote: Counting objects:  64% (4527/7072)[K
remote: Counting objects:  65% (4597/7072)[K
remote: Counting objects:  66% (4668/7072)[K
remote: Counting objects:  67% (4739/7072)[K
remote: Counting objects:  68% (4809/7072)[K
remote: Counting objects:  69% (4880/7072)[K
remote: Counting objects:  70% (4951/7072)[K
remote: Counting objects:  71% (5022/7072)[K
remote: Counting objects:  72% (5092/7072)[K
remote: Counting objects:  73% (5163/7072)[K
remote: Counting objects:  74% (5234/7072)[K
remote: Counting objects:  75% (5304/7072)[K
remote: Counting objects:  76% (5375/7072)[K
remote: Counting objects:  77% (5446/7072)[K
remote: Counting objects:  78% (5517/7072)[K
remote: Counting objects:  79% (5587/7072)[K
remote: Counting objects:  80% (5658/7072)[K
remote: Counting objects:  81% (5729/7072)[K
remote: Counting objects:  82% (5800/7072)[K
remote: Counting objects:  83% (5870/7072)[K
remote: Counting objects:  84% (5941/7072)[K
remote: Counting objects:  85% (6012/7072)[K
remote: Counting objects:  86% (6082/7072)[K
remote: Counting objects:  87% (6153/7072)[K
remote: Counting objects:  88% (6224/7072)[K
remote: Counting objects:  89% (6295/7072)[K
remote: Counting objects:  90% (6365/7072)[K
remote: Counting objects:  91% (6436/7072)[K
remote: Counting objects:  92% (6507/7072)[K
remote: Counting objects:  93% (6577/7072)[K
remote: Counting objects:  94% (6648/7072)[K
remote: Counting objects:  95% (6719/7072)[K
remote: Counting objects:  96% (6790/7072)[K
remote: Counting objects:  97% (6860/7072)[K
remote: Counting objects:  98% (6931/7072)[K
remote: Counting objects:  99% (7002/7072)[K
remote: Counting objects: 100% (7072/7072)[K
remote: Counting objects: 100% (7072/7072), done.[K
remote: Compressing objects:   0% (1/2992)[K
remote: Compressing objects:   1% (30/2992)[K
remote: Compressing objects:   2% (60/2992)[K
remote: Compressing objects:   3% (90/2992)[K
remote: Compressing objects:   4% (120/2992)[K
remote: Compressing objects:   5% (150/2992)[K
remote: Compressing objects:   6% (180/2992)[K
remote: Compressing objects:   7% (210/2992)[K
remote: Compressing objects:   8% (240/2992)[K
remote: Compressing objects:   9% (270/2992)[K
remote: Compressing objects:  10% (300/2992)[K
remote: Compressing objects:  11% (330/2992)[K
remote: Compressing objects:  12% (360/2992)[K
remote: Compressing objects:  13% (389/2992)[K
remote: Compressing objects:  14% (419/2992)[K
remote: Compressing objects:  15% (449/2992)[K
remote: Compressing objects:  16% (479/2992)[K
remote: Compressing objects:  17% (509/2992)[K
remote: Compressing objects:  18% (539/2992)[K
remote: Compressing objects:  19% (569/2992)[K
remote: Compressing objects:  20% (599/2992)[K
remote: Compressing objects:  21% (629/2992)[K
remote: Compressing objects:  22% (659/2992)[K
remote: Compressing objects:  23% (689/2992)[K
remote: Compressing objects:  24% (719/2992)[K
remote: Compressing objects:  25% (748/2992)[K
remote: Compressing objects:  26% (778/2992)[K
remote: Compressing objects:  27% (808/2992)[K
remote: Compressing objects:  28% (838/2992)[K
remote: Compressing objects:  29% (868/2992)[K
remote: Compressing objects:  30% (898/2992)[K
remote: Compressing objects:  31% (928/2992)[K
remote: Compressing objects:  32% (958/2992)[K
remote: Compressing objects:  33% (988/2992)[K
remote: Compressing objects:  34% (1018/2992)[K
remote: Compressing objects:  35% (1048/2992)[K
remote: Compressing objects:  36% (1078/2992)[K
remote: Compressing objects:  37% (1108/2992)[K
remote: Compressing objects:  38% (1137/2992)[K
remote: Compressing objects:  39% (1167/2992)[K
remote: Compressing objects:  40% (1197/2992)[K
remote: Compressing objects:  41% (1227/2992)[K
remote: Compressing objects:  42% (1257/2992)[K
remote: Compressing objects:  43% (1287/2992)[K
remote: Compressing objects:  44% (1317/2992)[K
remote: Compressing objects:  45% (1347/2992)[K
remote: Compressing objects:  46% (1377/2992)[K
remote: Compressing objects:  47% (1407/2992)[K
remote: Compressing objects:  48% (1437/2992)[K
remote: Compressing objects:  49% (1467/2992)[K
remote: Compressing objects:  50% (1496/2992)[K
remote: Compressing objects:  51% (1526/2992)[K
remote: Compressing objects:  52% (1556/2992)[K
remote: Compressing objects:  53% (1586/2992)[K
remote: Compressing objects:  54% (1616/2992)[K
remote: Compressing objects:  55% (1646/2992)[K
remote: Compressing objects:  56% (1676/2992)[K
remote: Compressing objects:  57% (1706/2992)[K
remote: Compressing objects:  58% (1736/2992)[K
remote: Compressing objects:  59% (1766/2992)[K
remote: Compressing objects:  60% (1796/2992)[K
remote: Compressing objects:  61% (1826/2992)[K
remote: Compressing objects:  62% (1856/2992)[K
remote: Compressing objects:  63% (1885/2992)[K
remote: Compressing objects:  64% (1915/2992)[K
remote: Compressing objects:  65% (1945/2992)[K
remote: Compressing objects:  66% (1975/2992)[K
remote: Compressing objects:  67% (2005/2992)[K
remote: Compressing objects:  68% (2035/2992)[K
remote: Compressing objects:  69% (2065/2992)[K
remote: Compressing objects:  70% (2095/2992)[K
remote: Compressing objects:  71% (2125/2992)[K
remote: Compressing objects:  72% (2155/2992)[K
remote: Compressing objects:  73% (2185/2992)[K
remote: Compressing objects:  74% (2215/2992)[K
remote: Compressing objects:  75% (2244/2992)[K
remote: Compressing objects:  76% (2274/2992)[K
remote: Compressing objects:  77% (2304/2992)[K
remote: Compressing objects:  78% (2334/2992)[K
remote: Compressing objects:  79% (2364/2992)[K
remote: Compressing objects:  80% (2394/2992)[K
remote: Compressing objects:  81% (2424/2992)[K
remote: Compressing objects:  82% (2454/2992)[K
remote: Compressing objects:  83% (2484/2992)[K
remote: Compressing objects:  84% (2514/2992)[K
remote: Compressing objects:  85% (2544/2992)[K
remote: Compressing objects:  86% (2574/2992)[K
remote: Compressing objects:  87% (2604/2992)[K
remote: Compressing objects:  88% (2633/2992)[K
remote: Compressing objects:  89% (2663/2992)[K
remote: Compressing objects:  90% (2693/2992)[K
remote: Compressing objects:  91% (2723/2992)[K
remote: Compressing objects:  92% (2753/2992)[K
remote: Compressing objects:  93% (2783/2992)[K
remote: Compressing objects:  94% (2813/2992)[K
remote: Compressing objects:  95% (2843/2992)[K
remote: Compressing objects:  96% (2873/2992)[K
remote: Compressing objects:  97% (2903/2992)[K
remote: Compressing objects:  98% (2933/2992)[K
remote: Compressing objects:  99% (2963/2992)[K
remote: Compressing objects: 100% (2992/2992)[K
remote: Compressing objects: 100% (2992/2992), done.[K
Receiving objects:   0% (1/7072)
Receiving objects:   1% (71/7072)
Receiving objects:   2% (142/7072)
Receiving objects:   3% (213/7072)
Receiving objects:   4% (283/7072)
Receiving objects:   5% (354/7072)
Receiving objects:   6% (425/7072)
Receiving objects:   7% (496/7072)
Receiving objects:   8% (566/7072)
Receiving objects:   9% (637/7072)
Receiving objects:  10% (708/7072)
Receiving objects:  11% (778/7072)
Receiving objects:  12% (849/7072)
Receiving objects:  13% (920/7072)
Receiving objects:  14% (991/7072)
Receiving objects:  15% (1061/7072)
Receiving objects:  16% (1132/7072)
Receiving objects:  17% (1203/7072)
Receiving objects:  18% (1273/7072)
Receiving objects:  19% (1344/7072)
Receiving objects:  20% (1415/7072)
Receiving objects:  21% (1486/7072)
Receiving objects:  22% (1556/7072)
Receiving objects:  23% (1627/7072)
Receiving objects:  24% (1698/7072)
Receiving objects:  25% (1768/7072)
Receiving objects:  26% (1839/7072)
Receiving objects:  27% (1910/7072)
Receiving objects:  28% (1981/7072)
Receiving objects:  29% (2051/7072)
Receiving objects:  30% (2122/7072)
Receiving objects:  31% (2193/7072)
Receiving objects:  32% (2264/7072)
Receiving objects:  33% (2334/7072)
Receiving objects:  34% (2405/7072)
Receiving objects:  35% (2476/7072)
Receiving objects:  36% (2546/7072)
Receiving objects:  37% (2617/7072)
Receiving objects:  38% (2688/7072)
Receiving objects:  39% (2759/7072)
Receiving objects:  40% (2829/7072)
Receiving objects:  41% (2900/7072)
Receiving objects:  42% (2971/7072)
Receiving objects:  43% (3041/7072)
Receiving objects:  44% (3112/7072)
Receiving objects:  45% (3183/7072)
Receiving objects:  46% (3254/7072)
Receiving objects:  47% (3324/7072)
Receiving objects:  48% (3395/7072)
Receiving objects:  49% (3466/7072)
Receiving objects:  50% (3536/7072)
Receiving objects:  51% (3607/7072)
Receiving objects:  52% (3678/7072)
Receiving objects:  53% (3749/7072)
Receiving objects:  54% (3819/7072)
Receiving objects:  55% (3890/7072)
Receiving objects:  56% (3961/7072)
Receiving objects:  57% (4032/7072)
Receiving objects:  58% (4102/7072)
Receiving objects:  59% (4173/7072)
Receiving objects:  60% (4244/7072)
Receiving objects:  61% (4314/7072)
Receiving objects:  62% (4385/7072)
Receiving objects:  63% (4456/7072)
Receiving objects:  64% (4527/7072)
Receiving objects:  65% (4597/7072)
Receiving objects:  66% (4668/7072)
Receiving objects:  67% (4739/7072)
Receiving objects:  68% (4809/7072)
Receiving objects:  69% (4880/7072)
Receiving objects:  70% (4951/7072)
Receiving objects:  71% (5022/7072)
Receiving objects:  72% (5092/7072)
Receiving objects:  73% (5163/7072)
Receiving objects:  74% (5234/7072)
Receiving objects:  75% (5304/7072)
Receiving objects:  76% (5375/7072)
Receiving objects:  77% (5446/7072)
Receiving objects:  78% (5517/7072)
Receiving objects:  79% (5587/7072)
Receiving objects:  80% (5658/7072)
Receiving objects:  81% (5729/7072)
Receiving objects:  82% (5800/7072)
Receiving objects:  83% (5870/7072)
Receiving objects:  84% (5941/7072)
Receiving objects:  85% (6012/7072)
Receiving objects:  86% (6082/7072)
Receiving objects:  87% (6153/7072)
Receiving objects:  88% (6224/7072)
Receiving objects:  89% (6295/7072)
Receiving objects:  90% (6365/7072)
Receiving objects:  91% (6436/7072)
Receiving objects:  92% (6507/7072)
Receiving objects:  93% (6577/7072)
Receiving objects:  94% (6648/7072)
Receiving objects:  95% (6719/7072)
Receiving objects:  96% (6790/7072)
remote: Total 7072 (delta 3008), reused 6573 (delta 2607), pack-reused 0 (from 0)[K
Receiving objects:  97% (6860/7072)
Receiving objects:  98% (6931/7072)
Receiving objects:  99% (7002/7072)
Receiving objects: 100% (7072/7072)
Receiving objects: 100% (7072/7072), 1.25 MiB | 36.57 MiB/s, done.
Resolving deltas:   0% (0/3008)
Resolving deltas:   1% (33/3008)
Resolving deltas:   2% (61/3008)
Resolving deltas:   3% (91/3008)
Resolving deltas:   4% (121/3008)
Resolving deltas:   5% (151/3008)
Resolving deltas:   6% (181/3008)
Resolving deltas:   7% (211/3008)
Resolving deltas:   8% (242/3008)
Resolving deltas:   9% (272/3008)
Resolving deltas:  10% (301/3008)
Resolving deltas:  11% (333/3008)
Resolving deltas:  12% (361/3008)
Resolving deltas:  13% (392/3008)
Resolving deltas:  14% (424/3008)
Resolving deltas:  15% (453/3008)
Resolving deltas:  16% (482/3008)
Resolving deltas:  17% (513/3008)
Resolving deltas:  18% (542/3008)
Resolving deltas:  19% (572/3008)
Resolving deltas:  20% (602/3008)
Resolving deltas:  21% (632/3008)
Resolving deltas:  22% (662/3008)
Resolving deltas:  23% (694/3008)
Resolving deltas:  24% (722/3008)
Resolving deltas:  25% (752/3008)
Resolving deltas:  26% (785/3008)
Resolving deltas:  27% (814/3008)
Resolving deltas:  28% (843/3008)
Resolving deltas:  29% (873/3008)
Resolving deltas:  30% (903/3008)
Resolving deltas:  31% (933/3008)
Resolving deltas:  32% (963/3008)
Resolving deltas:  33% (993/3008)
Resolving deltas:  34% (1026/3008)
Resolving deltas:  35% (1054/3008)
Resolving deltas:  36% (1083/3008)
Resolving deltas:  37% (1113/3008)
Resolving deltas:  38% (1145/3008)
Resolving deltas:  39% (1174/3008)
Resolving deltas:  40% (1206/3008)
Resolving deltas:  41% (1235/3008)
Resolving deltas:  42% (1265/3008)
Resolving deltas:  43% (1295/3008)
Resolving deltas:  44% (1324/3008)
Resolving deltas:  45% (1354/3008)
Resolving deltas:  46% (1384/3008)
Resolving deltas:  47% (1415/3008)
Resolving deltas:  48% (1444/3008)
Resolving deltas:  49% (1474/3008)
Resolving deltas:  50% (1505/3008)
Resolving deltas:  51% (1535/3008)
Resolving deltas:  52% (1566/3008)
Resolving deltas:  53% (1595/3008)
Resolving deltas:  54% (1625/3008)
Resolving deltas:  55% (1655/3008)
Resolving deltas:  56% (1685/3008)
Resolving deltas:  57% (1718/3008)
Resolving deltas:  58% (1745/3008)
Resolving deltas:  59% (1775/3008)
Resolving deltas:  60% (1805/3008)
Resolving deltas:  61% (1835/3008)
Resolving deltas:  62% (1865/3008)
Resolving deltas:  63% (1896/3008)
Resolving deltas:  64% (1927/3008)
Resolving deltas:  65% (1956/3008)
Resolving deltas:  66% (1986/3008)
Resolving deltas:  67% (2016/3008)
Resolving deltas:  68% (2047/3008)
Resolving deltas:  69% (2076/3008)
Resolving deltas:  70% (2107/3008)
Resolving deltas:  71% (2136/3008)
Resolving deltas:  72% (2166/3008)
Resolving deltas:  73% (2196/3008)
Resolving deltas:  74% (2227/3008)
Resolving deltas:  75% (2256/3008)
Resolving deltas:  76% (2288/3008)
Resolving deltas:  77% (2317/3008)
Resolving deltas:  78% (2347/3008)
Resolving deltas:  79% (2377/3008)
Resolving deltas:  80% (2408/3008)
Resolving deltas:  81% (2438/3008)
Resolving deltas:  82% (2469/3008)
Resolving deltas:  83% (2497/3008)
Resolving deltas:  84% (2527/3008)
Resolving deltas:  85% (2557/3008)
Resolving deltas:  86% (2587/3008)
Resolving deltas:  87% (2617/3008)
Resolving deltas:  88% (2648/3008)
Resolving deltas:  89% (2678/3008)
Resolving deltas:  90% (2708/3008)
Resolving deltas:  91% (2738/3008)
Resolving deltas:  92% (2768/3008)
Resolving deltas:  93% (2798/3008)
Resolving deltas:  94% (2828/3008)
Resolving deltas:  95% (2858/3008)
Resolving deltas:  96% (2888/3008)
Resolving deltas:  97% (2919/3008)
Resolving deltas:  98% (2948/3008)
Resolving deltas:  99% (2978/3008)
Resolving deltas: 100% (3008/3008)
Resolving deltas: 100% (3008/3008), done.
HEAD is now at 08ce85034268ec229d7d73cfb04c0d3c0faad98a
sem-version java 8
  1.8
  1.8.0.462
  11.0.28
  21.0.8
* 8 (set by /home/semaphore/.jenv/version)
  openjdk64-1.8.0.462
  openjdk64-11.0.28
  openjdk64-21.0.8

[2025-10-07T12:22:25+00:00]: Switch successful.
. cache-maven restore
cache_key is cache-maven-14.1.x
MISS: 'cache-maven-14.1.x'.
. sem-pint
ðŸ›‘ Skipping pint check, since it is a PR branch ðŸ›‘
mvn -Dcloud -Pjenkins -U -Ddependency.check.skip=true -Dmaven.wagon.http.retryHandler.count=10 --batch-mode --no-transfer-progress clean verify install dependency:analyze validate
[0m[0m[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Detecting the operating system and CPU architecture
[INFO] ------------------------------------------------------------------------
[INFO] os.detected.name: linux
[INFO] os.detected.arch: x86_64
[INFO] os.detected.version: 6.14
[INFO] os.detected.version.major: 6
[INFO] os.detected.version.minor: 14
[INFO] os.detected.release: ubuntu
[INFO] os.detected.release.version: 24.04
[INFO] os.detected.release.like.ubuntu: true
[INFO] os.detected.release.like.debian: true
[INFO] os.detected.classifier: linux-x86_64
[INFO] 
[INFO] --------------< io.confluent:kafka-connect-elasticsearch >--------------
[INFO] Building kafka-connect-elasticsearch 14.1.6-SNAPSHOT
[INFO]   from pom.xml
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- clean:3.1.0:clean (default-clean) @ kafka-connect-elasticsearch ---
[INFO] 
[INFO] --- buildnumber:1.4:create (default) @ kafka-connect-elasticsearch ---
[INFO] Executing: /bin/sh -c cd '/home/semaphore/kafka-connect-elasticsearch' && 'git' 'rev-parse' '--verify' 'HEAD'
[INFO] Working directory: /home/semaphore/kafka-connect-elasticsearch
[INFO] Storing buildNumber: 08ce85034268ec229d7d73cfb04c0d3c0faad98a at timestamp: 1759839846760
[WARNING] Cannot get the branch information from the git repository: 
Detecting the current branch failed: fatal: ref HEAD is not a symbolic ref

[INFO] Executing: /bin/sh -c cd '/home/semaphore/kafka-connect-elasticsearch' && 'git' 'rev-parse' '--verify' 'HEAD'
[INFO] Working directory: /home/semaphore/kafka-connect-elasticsearch
[INFO] Storing buildScmBranch: UNKNOWN
[INFO] 
[INFO] --- checkstyle:3.1.1:check (validate) @ kafka-connect-elasticsearch ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- enforcer:3.0.0-M3:enforce (enforce-versions) @ kafka-connect-elasticsearch ---
[INFO] 
[INFO] --- jacoco:0.8.6:prepare-agent (pre-unit-test) @ kafka-connect-elasticsearch ---
[INFO] argLine set to -javaagent:/home/semaphore/.m2/repository/org/jacoco/org.jacoco.agent/0.8.6/org.jacoco.agent-0.8.6-runtime.jar=destfile=/home/semaphore/kafka-connect-elasticsearch/target/jacoco.exec
[INFO] 
[INFO] --- jacoco:0.8.6:prepare-agent (prepare-agent) @ kafka-connect-elasticsearch ---
[INFO] argLine set to -javaagent:/home/semaphore/.m2/repository/org/jacoco/org.jacoco.agent/0.8.6/org.jacoco.agent-0.8.6-runtime.jar=destfile=/home/semaphore/kafka-connect-elasticsearch/target/jacoco.exec
[INFO] 
[INFO] --- resources:3.1.0:resources (default-resources) @ kafka-connect-elasticsearch ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- compiler:3.8.1:compile (default-compile) @ kafka-connect-elasticsearch ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 17 source files to /home/semaphore/kafka-connect-elasticsearch/target/classes
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[27,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ExternalResourceExistenceChecker.java:[22,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[55,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[56,32] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[27,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ExternalResourceExistenceChecker.java:[22,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[55,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[56,32] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[27,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ExternalResourceExistenceChecker.java:[22,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[55,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[56,32] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ExternalResourceExistenceChecker.java:[47,20] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ExternalResourceExistenceChecker.java:[57,27] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ExternalResourceExistenceChecker.java:[71,27] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[166,12] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[452,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[492,39] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[550,35] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[626,11] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[628,16] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/Validator.java:[652,5] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[115,17] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[194,5] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[194,47] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/main/java/io/confluent/connect/elasticsearch/ElasticsearchClient.java:[239,10] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[INFO] 
[INFO] --- resources:3.1.0:testResources (default-testResources) @ kafka-connect-elasticsearch ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 14 resources
[INFO] 
[INFO] --- compiler:3.8.1:testCompile (default-testCompile) @ kafka-connect-elasticsearch ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 24 source files to /home/semaphore/kafka-connect-elasticsearch/target/test-classes
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[29,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[30,32] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/ValidatorTest.java:[70,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[29,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[30,32] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/ValidatorTest.java:[70,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[29,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[30,32] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/ValidatorTest.java:[70,32] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[66,11] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/helper/ElasticsearchHelperClient.java:[73,23] org.elasticsearch.client.RestHighLevelClientBuilder in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/ValidatorTest.java:[97,11] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[WARNING] /home/semaphore/kafka-connect-elasticsearch/src/test/java/io/confluent/connect/elasticsearch/ValidatorTest.java:[104,23] org.elasticsearch.client.RestHighLevelClient in org.elasticsearch.client has been deprecated
[INFO] 
[INFO] --- surefire:3.0.0-M4:test (default-test) @ kafka-connect-elasticsearch ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- jacoco:0.8.6:report (report) @ kafka-connect-elasticsearch ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- jar:3.2.0:jar (default-jar) @ kafka-connect-elasticsearch ---
[INFO] Building jar: /home/semaphore/kafka-connect-elasticsearch/target/kafka-connect-elasticsearch-14.1.6-SNAPSHOT.jar
[INFO] 
[INFO] --- kafka-connect:0.11.1:kafka-connect (default) @ kafka-connect-elasticsearch ---
[ERROR] sourceFile /home/semaphore/kafka-connect-elasticsearch/logos/elasticsearch.jpg does not exist
[INFO] Copied /home/semaphore/kafka-connect-elasticsearch/logos/confluent.png to /home/semaphore/kafka-connect-elasticsearch/target/components/assets/confluent.png
[INFO] Copied /home/semaphore/kafka-connect-elasticsearch/logos/confluent.png to /home/semaphore/kafka-connect-elasticsearch/target/components/assets/confluent.png
[INFO] Writing manifest file at: /home/semaphore/kafka-connect-elasticsearch/target/manifest.json
[INFO] Building jar: /home/semaphore/kafka-connect-elasticsearch/target/components/kafka-connect-elasticsearch-14.1.6-SNAPSHOT.jar
[INFO] Reading assembly descriptor: /home/semaphore/kafka-connect-elasticsearch/target/components/component-package.xml
[WARNING] The assembly descriptor contains a filesystem-root relative reference, which is not cross platform compatible /
[WARNING] The following patterns were never triggered in this artifact exclusion filter:
o  'io.confluent:kafka-connect-maven-plugin'
o  'org.apache.kafka:connect-api'
o  'org.apache.kafka:connect-file'
o  'org.apache.kafka:connect-json'
o  'org.apache.kafka:connect-runtime'
o  'org.apache.kafka:connect-transform'
o  'org.apache.kafka:kafka-clients'

[INFO] Copying files to /home/semaphore/kafka-connect-elasticsearch/target/components/packages/confluentinc-kafka-connect-elasticsearch-14.1.6-SNAPSHOT
[WARNING] The assembly descriptor contains a filesystem-root relative reference, which is not cross platform compatible /
[WARNING] The following patterns were never triggered in this artifact exclusion filter:
o  'io.confluent:kafka-connect-maven-plugin'
o  'org.apache.kafka:connect-api'
o  'org.apache.kafka:connect-file'
o  'org.apache.kafka:connect-json'
o  'org.apache.kafka:connect-runtime'
o  'org.apache.kafka:connect-transform'
o  'org.apache.kafka:kafka-clients'

[INFO] Building zip: /home/semaphore/kafka-connect-elasticsearch/target/components/packages/confluentinc-kafka-connect-elasticsearch-14.1.6-SNAPSHOT.zip
[INFO] 
[INFO] --- assembly:3.3.0:single (make-assembly) @ kafka-connect-elasticsearch ---
[INFO] Reading assembly descriptor: src/assembly/development.xml
[INFO] Reading assembly descriptor: src/assembly/package.xml
[WARNING] The following patterns were never triggered in this artifact exclusion filter:
o  'org.apache.kafka:connect-json'

[INFO] Copying files to /home/semaphore/kafka-connect-elasticsearch/target/kafka-connect-elasticsearch-14.1.6-SNAPSHOT-development
[WARNING] Assembly file: /home/semaphore/kafka-connect-elasticsearch/target/kafka-connect-elasticsearch-14.1.6-SNAPSHOT-development is not a regular file (it may be a directory). It cannot be attached to the project build for installation or deployment.
[WARNING] The following patterns were never triggered in this artifact exclusion filter:
o  'org.apache.kafka:connect-json'

[INFO] Copying files to /home/semaphore/kafka-connect-elasticsearch/target/kafka-connect-elasticsearch-14.1.6-SNAPSHOT-package
[WARNING] Assembly file: /home/semaphore/kafka-connect-elasticsearch/target/kafka-connect-elasticsearch-14.1.6-SNAPSHOT-package is not a regular file (it may be a directory). It cannot be attached to the project build for installation or deployment.
[INFO] 
[INFO] --- jar:3.2.0:test-jar (default) @ kafka-connect-elasticsearch ---
[INFO] Building jar: /home/semaphore/kafka-connect-elasticsearch/target/kafka-connect-elasticsearch-14.1.6-SNAPSHOT-tests.jar
[INFO] 
[INFO] --- cyclonedx:2.7.9:makeAggregateBom (default) @ kafka-connect-elasticsearch ---
[INFO] CycloneDX: Resolving Dependencies
[INFO] CycloneDX: Creating BOM version 1.4 with 297 component(s)
[INFO] CycloneDX: Writing and validating BOM (XML): /home/semaphore/kafka-connect-elasticsearch/target/bom.xml
[INFO]            attaching as kafka-connect-elasticsearch-14.1.6-SNAPSHOT-cyclonedx.xml
[INFO] CycloneDX: Writing and validating BOM (JSON): /home/semaphore/kafka-connect-elasticsearch/target/bom.json
[WARNING] Unknown keyword additionalItems - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword
[INFO]            attaching as kafka-connect-elasticsearch-14.1.6-SNAPSHOT-cyclonedx.json
[INFO] 
[INFO] --- jacoco:0.8.6:prepare-agent-integration (prepare-agent-it) @ kafka-connect-elasticsearch ---
[INFO] argLine set to -javaagent:/home/semaphore/.m2/repository/org/jacoco/org.jacoco.agent/0.8.6/org.jacoco.agent-0.8.6-runtime.jar=destfile=/home/semaphore/kafka-connect-elasticsearch/target/jacoco-it.exec
[INFO] 
[INFO] --- failsafe:3.0.0-M3:integration-test (integration-test) @ kafka-connect-elasticsearch ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/semaphore/.m2/repository/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/semaphore/.m2/repository/org/slf4j/slf4j-reload4j/1.7.36/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
[INFO] Running io.confluent.connect.elasticsearch.integration.ElasticsearchSinkTaskIT
[2025-10-07 12:25:53,026] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:25:53,098] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:36423/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = true
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = false
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:25:53,132] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:25:53,261] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:25:53,262] INFO Using unsecured connection to [http://localhost:36423]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:25:54,150] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:25:54,163] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:25:54,168] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:25:54,210] ERROR Can't convert record from topic=test partition=0 offset=1. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:294)
org.apache.kafka.connect.errors.DataException: Key is used as document id and can not be null.
	at io.confluent.connect.elasticsearch.DataConverter.convertKey(DataConverter.java:94)
	at io.confluent.connect.elasticsearch.DataConverter.convertRecord(DataConverter.java:165)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.tryWriteRecord(ElasticsearchSinkTask.java:289)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.put(ElasticsearchSinkTask.java:125)
	at io.confluent.connect.elasticsearch.integration.ElasticsearchSinkTaskIT.testConvertDataException(ElasticsearchSinkTaskIT.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at com.github.tomakehurst.wiremock.junit.WireMockRule$1.evaluate(WireMockRule.java:74)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[2025-10-07 12:25:54,268] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=3, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:25:54,327] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:25:54,328] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:36423/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = false
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:25:54,329] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:25:54,330] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:25:54,330] INFO Using unsecured connection to [http://localhost:36423]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:25:54,369] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:25:54,370] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:25:54,370] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:25:54,386] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=0, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:25:54,416] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:25:54,417] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:45635/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 1
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 5
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:25:54,417] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:25:54,418] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:25:54,418] INFO Using unsecured connection to [http://localhost:45635]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:25:54,440] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:25:54,440] DEBUG Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:25:54,440] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:25:54,451] DEBUG Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:25:54,571] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=2, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:25:54,591] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:25:54,592] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:43227/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 1
	max.buffered.records = 2
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 5
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:25:54,593] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:25:54,593] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:25:54,594] INFO Using unsecured connection to [http://localhost:43227]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:25:54,608] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:25:54,609] DEBUG Putting 100 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:25:54,609] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:25:55,621] WARN Failed to execute bulk request due to java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-6 [ACTIVE]. Retrying attempt (1/3) after backoff of 5 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:25:55,742] DEBUG Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:25:55,775] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:25:55,775] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 3
	behavior.on.malformed.documents = ignore
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:39927/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:25:55,776] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:25:55,777] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:25:55,777] INFO Using unsecured connection to [http://localhost:39927]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:25:55,802] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:25:55,806] DEBUG Putting 6 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:25:55,806] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:25:55,879] DEBUG Encountered an illegal document error. Ignoring and will not index record. Please check DLQ topic for errors. (io.confluent.connect.elasticsearch.ElasticsearchClient:648)
[2025-10-07 12:25:55,884] DEBUG Encountered an illegal document error. Ignoring and will not index record. Please check DLQ topic for errors. (io.confluent.connect.elasticsearch.ElasticsearchClient:648)
[2025-10-07 12:25:55,884] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=6, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:25:55,884] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:25:55,885] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 3
	behavior.on.malformed.documents = fail
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:39927/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:25:55,886] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:25:55,887] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:25:55,887] INFO Using unsecured connection to [http://localhost:39927]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:25:55,900] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:25:55,901] DEBUG Putting 6 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:25:55,901] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:25:55,907] ERROR Encountered an illegal document error. Please check DLQ topic for errors. To ignore future records like this, change the configuration 'behavior.on.malformed.documents' to 'IGNORE'. (io.confluent.connect.elasticsearch.ElasticsearchClient:655)
[2025-10-07 12:25:55,911] DEBUG Tried to flush data to Elasticsearch, but BulkProcessor is already closed. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:137)
java.lang.IllegalStateException: bulk process already closed
	at org.elasticsearch.action.bulk.BulkProcessor.ensureOpen(BulkProcessor.java:454)
	at org.elasticsearch.action.bulk.BulkProcessor.flush(BulkProcessor.java:571)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.flush(ElasticsearchClient.java:303)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.preCommit(ElasticsearchSinkTask.java:135)
	at io.confluent.connect.elasticsearch.integration.ElasticsearchSinkTaskIT.testIndividualFailure(ElasticsearchSinkTaskIT.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at com.github.tomakehurst.wiremock.junit.WireMockRule$1.evaluate(WireMockRule.java:74)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[2025-10-07 12:25:55,912] DEBUG preCommitting offsets {} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:25:55,926] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:25:55,927] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:36971/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 1
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 5
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:25:55,927] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:25:55,928] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:25:55,928] INFO Using unsecured connection to [http://localhost:36971]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:25:55,938] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:25:55,938] DEBUG Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:25:55,938] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:25:56,943] WARN Failed to execute bulk request due to java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-11 [ACTIVE]. Retrying attempt (1/3) after backoff of 19 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:25:56,944] WARN Failed to execute bulk request due to java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-12 [ACTIVE]. Retrying attempt (1/3) after backoff of 12 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:25:57,966] WARN Failed to execute bulk request due to java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-14 [ACTIVE]. Retrying attempt (2/3) after backoff of 1 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:25:58,960] WARN Failed to execute bulk request due to java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-13 [ACTIVE]. Retrying attempt (2/3) after backoff of 17 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:25:58,970] ERROR Failed to execute bulk request due to 'java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-15 [ACTIVE]' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-15 [ACTIVE]
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:917)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-15 [ACTIVE]
	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.timeout(HttpAsyncRequestExecutor.java:387)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:92)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:39)
	at org.apache.http.impl.nio.reactor.AbstractIODispatch.timeout(AbstractIODispatch.java:175)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.sessionTimedOut(BaseIOReactor.java:261)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.timeoutCheck(AbstractIOReactor.java:502)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.validate(BaseIOReactor.java:211)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:280)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591)
	... 1 more
[2025-10-07 12:25:58,972] WARN Bulk request 1 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-15 [ACTIVE]' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-15 [ACTIVE]
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:917)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
Caused by: java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-15 [ACTIVE]
	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.timeout(HttpAsyncRequestExecutor.java:387)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:92)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:39)
	at org.apache.http.impl.nio.reactor.AbstractIODispatch.timeout(AbstractIODispatch.java:175)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.sessionTimedOut(BaseIOReactor.java:261)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.timeoutCheck(AbstractIOReactor.java:502)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.validate(BaseIOReactor.java:211)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:280)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591)
	... 1 more
[2025-10-07 12:25:59,980] ERROR Failed to execute bulk request due to 'java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-16 [ACTIVE]' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-16 [ACTIVE]
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:917)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-16 [ACTIVE]
	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.timeout(HttpAsyncRequestExecutor.java:387)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:92)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:39)
	at org.apache.http.impl.nio.reactor.AbstractIODispatch.timeout(AbstractIODispatch.java:175)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.sessionTimedOut(BaseIOReactor.java:261)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.timeoutCheck(AbstractIOReactor.java:502)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.validate(BaseIOReactor.java:211)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:280)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591)
	... 1 more
[2025-10-07 12:25:59,982] WARN Bulk request 2 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-16 [ACTIVE]' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-16 [ACTIVE]
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:917)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
Caused by: java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-16 [ACTIVE]
	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.timeout(HttpAsyncRequestExecutor.java:387)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:92)
	at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:39)
	at org.apache.http.impl.nio.reactor.AbstractIODispatch.timeout(AbstractIODispatch.java:175)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.sessionTimedOut(BaseIOReactor.java:261)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.timeoutCheck(AbstractIOReactor.java:502)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.validate(BaseIOReactor.java:211)
	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:280)
	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591)
	... 1 more
[2025-10-07 12:25:59,983] DEBUG preCommitting offsets {} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:26:00,497] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:26:00,498] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = ignore
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:43359/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:26:00,498] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:26:00,499] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:26:00,499] INFO Using unsecured connection to [http://localhost:43359]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:26:00,524] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:26:00,524] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:26:00,524] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:26:00,530] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=3, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:26:00,530] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:26:00,530] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = fail
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:43359/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = true
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:26:00,531] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:26:00,531] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:26:00,532] INFO Using unsecured connection to [http://localhost:43359]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:26:00,553] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:26:00,554] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:26:00,554] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:26:00,559] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=0, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:26:00,581] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:26:00,582] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:41997/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = true
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = false
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:26:00,582] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:26:00,583] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:26:00,583] INFO Using unsecured connection to [http://localhost:41997]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:26:00,594] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:26:00,595] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:26:00,598] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:26:00,601] ERROR Can't convert record from topic=test partition=0 offset=1. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:294)
org.apache.kafka.connect.errors.DataException: Key is used as document id and can not be null.
	at io.confluent.connect.elasticsearch.DataConverter.convertKey(DataConverter.java:94)
	at io.confluent.connect.elasticsearch.DataConverter.convertRecord(DataConverter.java:165)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.tryWriteRecord(ElasticsearchSinkTask.java:289)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.put(ElasticsearchSinkTask.java:125)
	at io.confluent.connect.elasticsearch.integration.ElasticsearchSinkTaskIT.testConvertDataException(ElasticsearchSinkTaskIT.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at com.github.tomakehurst.wiremock.junit.WireMockRule$1.evaluate(WireMockRule.java:74)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[2025-10-07 12:26:00,605] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=3, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:26:00,605] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:26:00,606] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:41997/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = false
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:26:00,606] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:26:00,607] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:26:00,607] INFO Using unsecured connection to [http://localhost:41997]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:26:00,615] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:26:00,615] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:26:00,615] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:26:00,622] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:26:00,636] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:26:00,636] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:39997/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 1
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 5
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:26:00,636] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:26:00,637] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:26:00,637] INFO Using unsecured connection to [http://localhost:39997]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:26:00,648] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:26:00,648] DEBUG Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:26:00,648] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:26:00,753] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:26:00,754] DEBUG Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:26:00,855] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=2, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:26:00,856] WARN Failed to execute bulk request due to org.apache.http.ConnectionClosedException: Connection is closed. Retrying attempt (1/3) after backoff of 6 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:26:00,864] WARN Failed to execute bulk request due to java.net.ConnectException: Connection refused. Retrying attempt (2/3) after backoff of 19 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:26:00,885] ERROR Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 12:26:00,886] WARN Bulk request 2 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 12:26:01,374] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:26:01,375] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:34809/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 1
	max.buffered.records = 2
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 5
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:26:01,375] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:26:01,376] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:26:01,376] INFO Using unsecured connection to [http://localhost:34809]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:26:01,388] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:26:01,388] DEBUG Putting 100 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:26:01,388] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:26:02,393] WARN Failed to execute bulk request due to java.net.SocketTimeoutException: 1,000 milliseconds timeout on connection http-outgoing-24 [ACTIVE]. Retrying attempt (1/3) after backoff of 19 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:26:02,421] DEBUG Pausing all partitions (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:363)
[2025-10-07 12:26:02,532] DEBUG Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:26:02,532] DEBUG Resuming all partitions (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:348)
[2025-10-07 12:26:02,552] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:26:02,553] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:36189/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 1
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 5
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:26:02,553] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:26:02,554] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:26:02,554] INFO Using unsecured connection to [http://localhost:36189]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:26:02,569] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:26:02,569] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:26:02,569] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:26:02,677] DEBUG preCommitting offsets {test-1=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}, test-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:26:02,780] DEBUG preCommitting offsets {test-1=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:26:02,781] WARN Failed to execute bulk request due to org.apache.http.ConnectionClosedException: Connection is closed. Retrying attempt (1/3) after backoff of 1 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:26:02,783] WARN Failed to execute bulk request due to java.net.ConnectException: Connection refused. Retrying attempt (2/3) after backoff of 39 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:26:02,824] ERROR Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 12:26:02,824] WARN Bulk request 2 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 12:26:03,303] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:26:03,303] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 3
	behavior.on.malformed.documents = ignore
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:43375/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:26:03,304] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:26:03,304] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:26:03,304] INFO Using unsecured connection to [http://localhost:43375]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:26:03,316] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:26:03,316] DEBUG Putting 6 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:26:03,316] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:26:03,321] DEBUG Encountered an illegal document error. Ignoring and will not index record. Please check DLQ topic for errors. (io.confluent.connect.elasticsearch.ElasticsearchClient:648)
[2025-10-07 12:26:03,324] DEBUG Encountered an illegal document error. Ignoring and will not index record. Please check DLQ topic for errors. (io.confluent.connect.elasticsearch.ElasticsearchClient:648)
[2025-10-07 12:26:03,325] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=6, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:26:03,325] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:26:03,325] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 3
	behavior.on.malformed.documents = fail
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:43375/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:26:03,326] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:26:03,326] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:26:03,326] INFO Using unsecured connection to [http://localhost:43375]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:26:03,338] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:26:03,338] DEBUG Putting 6 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:26:03,338] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:26:03,342] ERROR Encountered an illegal document error. Please check DLQ topic for errors. To ignore future records like this, change the configuration 'behavior.on.malformed.documents' to 'IGNORE'. (io.confluent.connect.elasticsearch.ElasticsearchClient:655)
[2025-10-07 12:26:03,345] DEBUG Tried to flush data to Elasticsearch, but BulkProcessor is already closed. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:137)
java.lang.IllegalStateException: bulk process already closed
	at org.elasticsearch.action.bulk.BulkProcessor.ensureOpen(BulkProcessor.java:454)
	at org.elasticsearch.action.bulk.BulkProcessor.flush(BulkProcessor.java:571)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.flush(ElasticsearchClient.java:303)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.preCommit(ElasticsearchSinkTask.java:135)
	at io.confluent.connect.elasticsearch.integration.ElasticsearchSinkTaskIT.testIndividualFailure(ElasticsearchSinkTaskIT.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at com.github.tomakehurst.wiremock.junit.WireMockRule$1.evaluate(WireMockRule.java:74)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[2025-10-07 12:26:03,346] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:26:03,359] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:26:03,360] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:41401/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 1
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 5
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:26:03,360] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:26:03,361] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:26:03,361] INFO Using unsecured connection to [http://localhost:41401]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:26:03,371] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:26:03,371] DEBUG Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:26:03,371] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:26:03,374] DEBUG preCommitting offsets {} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:26:03,375] WARN Failed to execute bulk request due to org.apache.http.ConnectionClosedException: Connection is closed. Retrying attempt (1/3) after backoff of 2 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:26:03,375] WARN Failed to execute bulk request due to java.net.ConnectException: Connection refused. Retrying attempt (1/3) after backoff of 2 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:26:03,378] WARN Failed to execute bulk request due to java.net.ConnectException: Connection refused. Retrying attempt (2/3) after backoff of 19 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:26:03,378] WARN Failed to execute bulk request due to java.net.ConnectException: Connection refused. Retrying attempt (2/3) after backoff of 14 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:26:03,391] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:26:03,392] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = ignore
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:35589/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:26:03,392] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:26:03,393] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:26:03,393] INFO Using unsecured connection to [http://localhost:35589]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:26:03,393] ERROR Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 12:26:03,395] WARN Bulk request 1 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 12:26:03,401] ERROR Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 12:26:03,401] WARN Bulk request 2 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'java.net.ConnectException: Connection refused' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:932)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:300)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174)
	at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148)
	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351)
	at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221)
	at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)
	... 1 more
[2025-10-07 12:26:03,413] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:26:03,413] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:26:03,413] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:26:03,418] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=3, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:26:03,418] INFO Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:26:03,418] INFO ElasticsearchSinkTaskConfig values: 
	batch.size = 10
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = fail
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:35589/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 10000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 1
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = UPSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:26:03,419] INFO Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:26:03,419] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:26:03,419] INFO Using unsecured connection to [http://localhost:35589]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:26:03,428] INFO Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:26:03,428] DEBUG Putting 3 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:26:03,428] INFO Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:26:03,433] DEBUG preCommitting offsets {test-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:26:03,479] WARN Attempted to read Testcontainers configuration file at file:/home/semaphore/.testcontainers.properties but the file was not found. Exception message: FileNotFoundException: /home/semaphore/.testcontainers.properties (No such file or directory) (org.testcontainers.utility.TestcontainersConfiguration:338)
[2025-10-07 12:26:03,484] INFO Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor') (org.testcontainers.utility.ImageNameSubstitutor:50)
[2025-10-07 12:26:04,274] INFO Found Docker environment with local Unix socket (unix:///var/run/docker.sock) (org.testcontainers.dockerclient.DockerClientProviderStrategy:177)
[2025-10-07 12:26:04,278] INFO Docker host IP address is localhost (org.testcontainers.DockerClientFactory:190)
[2025-10-07 12:26:04,311] INFO Connected to docker: 
  Server Version: 28.1.1
  API Version: 1.49
  Operating System: Ubuntu 24.04.3 LTS
  Total Memory: 62976 MB (org.testcontainers.DockerClientFactory:206)
[2025-10-07 12:26:05,684] INFO Starting to pull image (org.testcontainers.DockerClientFactory:38)
[2025-10-07 12:26:05,709] INFO Pulling image layers:  0 pending,  0 downloaded,  0 extracted, (0 bytes/0 bytes) (org.testcontainers.DockerClientFactory:85)
[2025-10-07 12:26:06,008] INFO Pulling image layers:  2 pending,  1 downloaded,  0 extracted, (274 KB/? MB) (org.testcontainers.DockerClientFactory:85)
[2025-10-07 12:26:06,045] INFO Pulling image layers:  1 pending,  2 downloaded,  0 extracted, (307 KB/? MB) (org.testcontainers.DockerClientFactory:85)
[2025-10-07 12:26:06,078] INFO Pulling image layers:  0 pending,  3 downloaded,  0 extracted, (339 KB/5 MB) (org.testcontainers.DockerClientFactory:85)
[2025-10-07 12:26:06,212] INFO Pulling image layers:  0 pending,  3 downloaded,  1 extracted, (2 MB/5 MB) (org.testcontainers.DockerClientFactory:85)
[2025-10-07 12:26:06,295] INFO Pulling image layers:  0 pending,  3 downloaded,  2 extracted, (2 MB/5 MB) (org.testcontainers.DockerClientFactory:85)
[2025-10-07 12:26:06,330] INFO Pulling image layers:  0 pending,  3 downloaded,  3 extracted, (5 MB/5 MB) (org.testcontainers.DockerClientFactory:85)
[2025-10-07 12:26:07,049] INFO Ryuk started - will monitor and terminate Testcontainers containers on JVM exit (org.testcontainers.DockerClientFactory:224)
[2025-10-07 12:26:07,049] INFO Checking the system... (org.testcontainers.DockerClientFactory:235)
[2025-10-07 12:26:07,050] INFO âœ”ï¸Ž Docker server version should be at least 1.6.0 (org.testcontainers.DockerClientFactory:309)
[2025-10-07 12:26:07,105] INFO âœ”ï¸Ž Docker environment should have more than 2GB free disk space (org.testcontainers.DockerClientFactory:309)
[2025-10-07 12:26:07,117] INFO Pulling docker image: docker.elastic.co/elasticsearch/elasticsearch:8.2.2. Please be patient; this may take some time but only needs to be done once. (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:72)
[2025-10-07 12:26:08,006] INFO Starting to pull image (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:38)
[2025-10-07 12:26:08,008] INFO Pulling image layers:  0 pending,  0 downloaded,  0 extracted, (0 bytes/0 bytes) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:08,560] INFO Pulling image layers:  8 pending,  1 downloaded,  0 extracted, (323 KB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:08,635] INFO Pulling image layers:  7 pending,  2 downloaded,  0 extracted, (25 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:08,642] INFO Pulling image layers:  6 pending,  3 downloaded,  0 extracted, (30 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:08,876] INFO Pulling image layers:  5 pending,  4 downloaded,  0 extracted, (31 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:08,901] INFO Pulling image layers:  4 pending,  5 downloaded,  0 extracted, (31 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:09,150] INFO Pulling image layers:  3 pending,  6 downloaded,  0 extracted, (148 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:09,155] INFO Pulling image layers:  2 pending,  7 downloaded,  0 extracted, (148 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:09,415] INFO Pulling image layers:  1 pending,  8 downloaded,  0 extracted, (226 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:09,510] INFO Pulling image layers:  1 pending,  8 downloaded,  1 extracted, (267 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:09,932] INFO Pulling image layers:  1 pending,  8 downloaded,  2 extracted, (447 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:09,952] INFO Pulling image layers:  1 pending,  8 downloaded,  3 extracted, (486 MB/? MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:10,268] INFO Pulling image layers:  0 pending,  9 downloaded,  3 extracted, (596 MB/607 MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:14,254] INFO Pulling image layers:  0 pending,  9 downloaded,  4 extracted, (607 MB/607 MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:14,271] INFO Pulling image layers:  0 pending,  9 downloaded,  5 extracted, (607 MB/607 MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:14,289] INFO Pulling image layers:  0 pending,  9 downloaded,  6 extracted, (607 MB/607 MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:14,311] INFO Pulling image layers:  0 pending,  9 downloaded,  7 extracted, (607 MB/607 MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:14,325] INFO Pulling image layers:  0 pending,  9 downloaded,  8 extracted, (607 MB/607 MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:14,341] INFO Pulling image layers:  0 pending,  9 downloaded,  9 extracted, (607 MB/607 MB) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:85)
[2025-10-07 12:26:14,354] INFO Pull complete. 9 layers, pulled in 6s (downloaded 607 MB at 101 MB/s) (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:106)
[2025-10-07 12:26:14,364] INFO Starting an elasticsearch container using [docker.elastic.co/elasticsearch/elasticsearch:8.2.2] (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:73)
[2025-10-07 12:26:14,388] INFO Setting up basic authentication in a Docker image (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:328)
[2025-10-07 12:26:14,419] INFO Transferred 653 bytes to Docker daemon (org.testcontainers.images.builder.ImageFromDockerfile:137)
[2025-10-07 12:26:22,742] INFO Creating container for image: localhost/testcontainers/dse6kdp33ehhpgnx:latest (ðŸ³ [localhost/testcontainers/dse6kdp33ehhpgnx:latest]:363)
[2025-10-07 12:26:22,796] INFO Container localhost/testcontainers/dse6kdp33ehhpgnx:latest is starting: 0d1ae74dbc5adaf52ac076b493390fe13946217837c6f3618bfe1b536f9893a8 (ðŸ³ [localhost/testcontainers/dse6kdp33ehhpgnx:latest]:424)
2025-10-07 12:26:23,346 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

Created elasticsearch keystore in /usr/share/elasticsearch/config/elasticsearch.keystore
2025-10-07 12:26:23,999 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

2025-10-07 12:26:25,158 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

2025-10-07 12:26:25,977 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

2025-10-07 12:26:26,533 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.AutoConfigureNode.main(AutoConfigureNode.java:157)

Exception in thread "main" java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/sun.management.spi.PlatformMBeanProvider$PlatformComponent.getMBeans(PlatformMBeanProvider.java:195)
	at java.management/java.lang.management.ManagementFactory.getPlatformMXBean(ManagementFactory.java:689)
	at java.management/java.lang.management.ManagementFactory.getOperatingSystemMXBean(ManagementFactory.java:389)
	at org.elasticsearch.tools.launchers.DefaultSystemMemoryInfo.<init>(DefaultSystemMemoryInfo.java:29)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.jvmOptions(JvmOptionsParser.java:132)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:87)
[2025-10-07 12:31:23,084] ERROR Could not start container (ðŸ³ [localhost/testcontainers/dse6kdp33ehhpgnx:latest]:512)
java.lang.IllegalStateException: Container exited with code 1
	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:497)
	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:331)
	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:329)
	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:317)
	at io.confluent.connect.elasticsearch.helper.ElasticsearchContainer.start(ElasticsearchContainer.java:165)
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT.setupBeforeAll(ElasticsearchConnectorIT.java:80)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[2025-10-07 12:31:23,113] ERROR Log output from the failed container:
2025-10-07 12:26:23,346 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

Created elasticsearch keystore in /usr/share/elasticsearch/config/elasticsearch.keystore
2025-10-07 12:26:23,999 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

2025-10-07 12:26:25,158 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

2025-10-07 12:26:25,977 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

2025-10-07 12:26:26,533 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.AutoConfigureNode.main(AutoConfigureNode.java:157)

Exception in thread "main" java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/sun.management.spi.PlatformMBeanProvider$PlatformComponent.getMBeans(PlatformMBeanProvider.java:195)
	at java.management/java.lang.management.ManagementFactory.getPlatformMXBean(ManagementFactory.java:689)
	at java.management/java.lang.management.ManagementFactory.getOperatingSystemMXBean(ManagementFactory.java:389)
	at org.elasticsearch.tools.launchers.DefaultSystemMemoryInfo.<init>(DefaultSystemMemoryInfo.java:29)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.jvmOptions(JvmOptionsParser.java:132)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:87)
 (ðŸ³ [localhost/testcontainers/dse6kdp33ehhpgnx:latest]:519)
[WARNING] Tests run: 14, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 332.415 s - in io.confluent.connect.elasticsearch.integration.ElasticsearchSinkTaskIT
[INFO] Running io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT
[2025-10-07 12:31:23,143] INFO Configuration: (org.apache.hadoop.minikdc.MiniKdc:225)
[2025-10-07 12:31:23,143] INFO --------------------------------------------------------------- (org.apache.hadoop.minikdc.MiniKdc:226)
[2025-10-07 12:31:23,144] INFO   debug: false (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:31:23,144] INFO   transport: TCP (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:31:23,144] INFO   max.ticket.lifetime: 86400000 (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:31:23,144] INFO   org.name: EXAMPLE (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:31:23,144] INFO   kdc.port: 0 (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:31:23,144] INFO   org.domain: COM (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:31:23,144] INFO   max.renewable.lifetime: 604800000 (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:31:23,144] INFO   instance: DefaultKrbServer (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:31:23,144] INFO   kdc.bind.address: localhost (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:31:23,144] INFO --------------------------------------------------------------- (org.apache.hadoop.minikdc.MiniKdc:230)
[2025-10-07 12:31:23,229] INFO MiniKdc started. (org.apache.hadoop.minikdc.MiniKdc:285)
[2025-10-07 12:31:23,239] INFO Starting an elasticsearch container using [docker.elastic.co/elasticsearch/elasticsearch:8.2.2] (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:73)
[2025-10-07 12:31:23,240] INFO Building Elasticsearch image with Kerberos configuration. (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:398)
[2025-10-07 12:31:23,240] INFO Creating Kerberized Elasticsearch image. (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:332)
[2025-10-07 12:31:23,243] INFO Transferred 1 KB to Docker daemon (org.testcontainers.images.builder.ImageFromDockerfile:137)
[2025-10-07 12:31:23,769] INFO Creating container for image: localhost/testcontainers/knvtglbhjyn1erjd:latest (ðŸ³ [localhost/testcontainers/knvtglbhjyn1erjd:latest]:363)
[2025-10-07 12:31:23,813] INFO Container localhost/testcontainers/knvtglbhjyn1erjd:latest is starting: efdff37d25010516545be572c8c0ffcae5e50f1f2f69383a492fccd6895f98d3 (ðŸ³ [localhost/testcontainers/knvtglbhjyn1erjd:latest]:424)
2025-10-07 12:31:24,539 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

2025-10-07 12:31:25,099 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.AutoConfigureNode.main(AutoConfigureNode.java:157)

Exception in thread "main" java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/sun.management.spi.PlatformMBeanProvider$PlatformComponent.getMBeans(PlatformMBeanProvider.java:195)
	at java.management/java.lang.management.ManagementFactory.getPlatformMXBean(ManagementFactory.java:689)
	at java.management/java.lang.management.ManagementFactory.getOperatingSystemMXBean(ManagementFactory.java:389)
	at org.elasticsearch.tools.launchers.DefaultSystemMemoryInfo.<init>(DefaultSystemMemoryInfo.java:29)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.jvmOptions(JvmOptionsParser.java:132)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:87)
[2025-10-07 12:36:24,068] ERROR Could not start container (ðŸ³ [localhost/testcontainers/knvtglbhjyn1erjd:latest]:512)
java.lang.IllegalStateException: Container exited with code 1
	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:497)
	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:331)
	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:329)
	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:317)
	at io.confluent.connect.elasticsearch.helper.ElasticsearchContainer.start(ElasticsearchContainer.java:165)
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT.setupBeforeAll(ElasticsearchConnectorKerberosIT.java:37)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[2025-10-07 12:36:24,095] ERROR Log output from the failed container:
2025-10-07 12:31:24,539 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

2025-10-07 12:31:25,099 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.AutoConfigureNode.main(AutoConfigureNode.java:157)

Exception in thread "main" java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/sun.management.spi.PlatformMBeanProvider$PlatformComponent.getMBeans(PlatformMBeanProvider.java:195)
	at java.management/java.lang.management.ManagementFactory.getPlatformMXBean(ManagementFactory.java:689)
	at java.management/java.lang.management.ManagementFactory.getOperatingSystemMXBean(ManagementFactory.java:389)
	at org.elasticsearch.tools.launchers.DefaultSystemMemoryInfo.<init>(DefaultSystemMemoryInfo.java:29)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.jvmOptions(JvmOptionsParser.java:132)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:87)
 (ðŸ³ [localhost/testcontainers/knvtglbhjyn1erjd:latest]:519)
[2025-10-07 12:36:24,113] INFO Default Internal kdc server stopped. (org.apache.kerby.kerberos.kerb.server.impl.DefaultInternalKdcServerImpl:102)
[2025-10-07 12:36:25,114] INFO MiniKdc stopped. (org.apache.hadoop.minikdc.MiniKdc:359)
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 301.967 s <<< FAILURE! - in io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT
[ERROR] io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT  Time elapsed: 0 s  <<< ERROR!
org.testcontainers.containers.ContainerLaunchException: Container startup failed
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT.setupBeforeAll(ElasticsearchConnectorIT.java:80)
Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT.setupBeforeAll(ElasticsearchConnectorIT.java:80)
Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT.setupBeforeAll(ElasticsearchConnectorIT.java:80)
Caused by: java.lang.IllegalStateException: Container exited with code 1
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT.setupBeforeAll(ElasticsearchConnectorIT.java:80)

[INFO] Running io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT
[2025-10-07 12:36:25,119] INFO Configuration: (org.apache.hadoop.minikdc.MiniKdc:225)
[2025-10-07 12:36:25,119] INFO --------------------------------------------------------------- (org.apache.hadoop.minikdc.MiniKdc:226)
[2025-10-07 12:36:25,119] INFO   debug: false (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:36:25,119] INFO   transport: TCP (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:36:25,120] INFO   max.ticket.lifetime: 86400000 (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:36:25,120] INFO   org.name: EXAMPLE (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:36:25,120] INFO   kdc.port: 0 (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:36:25,120] INFO   org.domain: COM (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:36:25,120] INFO   max.renewable.lifetime: 604800000 (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:36:25,120] INFO   instance: DefaultKrbServer (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:36:25,120] INFO   kdc.bind.address: localhost (org.apache.hadoop.minikdc.MiniKdc:228)
[2025-10-07 12:36:25,120] INFO --------------------------------------------------------------- (org.apache.hadoop.minikdc.MiniKdc:230)
[2025-10-07 12:36:25,122] INFO MiniKdc started. (org.apache.hadoop.minikdc.MiniKdc:285)
[2025-10-07 12:36:25,125] INFO Starting an elasticsearch container using [docker.elastic.co/elasticsearch/elasticsearch:8.2.2] (ðŸ³ [docker.elastic.co/elasticsearch/elasticsearch:8.2.2]:73)
[2025-10-07 12:36:25,126] INFO Building Elasticsearch image with SSL configuration (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:379)
[2025-10-07 12:36:25,126] INFO Building Elasticsearch image with Kerberos configuration. (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:398)
[2025-10-07 12:36:25,127] INFO Creating Kerberized Elasticsearch image. (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:332)
[2025-10-07 12:36:25,127] INFO Extending Docker image to generate certs and enable SSL (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:336)
[2025-10-07 12:36:25,129] INFO Including test machine address 10.113.124.248 in Elasticsearch certs (io.confluent.connect.elasticsearch.helper.ElasticsearchContainer:420)
[2025-10-07 12:36:25,132] INFO Transferred 6 KB to Docker daemon (org.testcontainers.images.builder.ImageFromDockerfile:137)
[2025-10-07 12:36:44,981] INFO Creating container for image: localhost/testcontainers/o2gaevpzgbvimywz:latest (ðŸ³ [localhost/testcontainers/o2gaevpzgbvimywz:latest]:363)
[2025-10-07 12:36:45,010] INFO Container localhost/testcontainers/o2gaevpzgbvimywz:latest is starting: 705ce7c33b5a3a3536d9a2e11f949495f939120a9af4498042fb3efbc80d664b (ðŸ³ [localhost/testcontainers/o2gaevpzgbvimywz:latest]:424)

Replacing the ip address in the /usr/share/elasticsearch/config/ssl/instances.yml file with 10.113.124.248
Setting up Elasticsearch and generating certificates in /usr/share/elasticsearch
=== CREATE Keystore ===
Elastic password is: elastic
2025-10-07 12:36:45,547 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

Created elasticsearch keystore in /usr/share/elasticsearch/config/elasticsearch.keystore
Setting bootstrap.password...
2025-10-07 12:36:46,206 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

=== CREATE SSL CERTS ===
Creating cluster-ca.zip... (warnings are benign)
2025-10-07 12:36:47,117 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.CertificateTool.main(CertificateTool.java:149)

Unzip ca files...
Archive:  /usr/share/elasticsearch/config/ssl/cluster-ca.zip
   creating: /usr/share/elasticsearch/config/ssl/ca/
  inflating: /usr/share/elasticsearch/config/ssl/ca/ca.crt  
  inflating: /usr/share/elasticsearch/config/ssl/ca/ca.key  
Create cluster certs zipfile... (warnings are benign)
2025-10-07 12:36:48,106 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.CertificateTool.main(CertificateTool.java:149)

Unzipping cluster certs zipfile...
Archive:  /usr/share/elasticsearch/config/ssl/cluster.zip
   creating: /usr/share/elasticsearch/config/ssl/cluster/elasticsearch/
  inflating: /usr/share/elasticsearch/config/ssl/cluster/elasticsearch/elasticsearch.crt  
  inflating: /usr/share/elasticsearch/config/ssl/cluster/elasticsearch/elasticsearch.key  
   creating: /usr/share/elasticsearch/config/ssl/cluster/kibana/
  inflating: /usr/share/elasticsearch/config/ssl/cluster/kibana/kibana.crt  
  inflating: /usr/share/elasticsearch/config/ssl/cluster/kibana/kibana.key  
   creating: /usr/share/elasticsearch/config/ssl/cluster/logstash/
  inflating: /usr/share/elasticsearch/config/ssl/cluster/logstash/logstash.crt  
  inflating: /usr/share/elasticsearch/config/ssl/cluster/logstash/logstash.key  
Move elasticsearch certs to SSL config dir...
Generating truststore at /usr/share/elasticsearch/config/ssl/truststore.jks
Certificate was added to keystore
Generating keystore for client at /usr/share/elasticsearch/config/ssl/keystore.jks
Importing keystore /usr/share/elasticsearch/config/ssl/client.p12 to /usr/share/elasticsearch/config/ssl/keystore.jks...
Entry for alias clientkey successfully imported.
Import command completed:  1 entries successfully imported, 0 entries failed or cancelled

Warning:
The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format using "keytool -importkeystore -srckeystore /usr/share/elasticsearch/config/ssl/keystore.jks -destkeystore /usr/share/elasticsearch/config/ssl/keystore.jks -deststoretype pkcs12".
Elasticsearch Configuration
## Used by Docker images in our integration test
http.host: 0.0.0.0
network.host: 0.0.0.0
transport.host: 0.0.0.0

node.store.allow_mmap: false
cluster.routing.allocation.disk.threshold_enabled: false
discovery.type: single-node

xpack.license.self_generated.type: trial
xpack.security.enabled: true
xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.client_authentication: optional
xpack.security.http.ssl.verification_mode: certificate
xpack.security.http.ssl.key:  ssl/elasticsearch.key
xpack.security.http.ssl.certificate: ssl/elasticsearch.crt
xpack.security.http.ssl.certificate_authorities: [ "ssl/ca/ca.crt" ]

xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.key:  ssl/elasticsearch.key
xpack.security.transport.ssl.certificate: ssl/elasticsearch.crt
xpack.security.transport.ssl.certificate_authorities: [ "ssl/ca/ca.crt" ]

# enable anonymous connections since setting passwords requires running a command
xpack.security.authc:
  anonymous:
    username: connect_user
    roles: superuser
    authz_exception: true

# Kerberos realm
xpack.security.authc.realms.kerberos.kerb1:
  order: 3
  keytab.path: es.keytab
  remove_realm_name: false
Starting Elasticsearch with SSL and Kerberos enabled ...
2025-10-07 12:36:49,747 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

2025-10-07 12:36:50,318 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.AutoConfigureNode.main(AutoConfigureNode.java:157)

Exception in thread "main" java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/sun.management.spi.PlatformMBeanProvider$PlatformComponent.getMBeans(PlatformMBeanProvider.java:195)
	at java.management/java.lang.management.ManagementFactory.getPlatformMXBean(ManagementFactory.java:689)
	at java.management/java.lang.management.ManagementFactory.getOperatingSystemMXBean(ManagementFactory.java:389)
	at org.elasticsearch.tools.launchers.DefaultSystemMemoryInfo.<init>(DefaultSystemMemoryInfo.java:29)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.jvmOptions(JvmOptionsParser.java:132)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:87)
[2025-10-07 12:41:45,204] ERROR Could not start container (ðŸ³ [localhost/testcontainers/o2gaevpzgbvimywz:latest]:512)
java.lang.IllegalStateException: Container exited with code 1
	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:497)
	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:331)
	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:329)
	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:317)
	at io.confluent.connect.elasticsearch.helper.ElasticsearchContainer.start(ElasticsearchContainer.java:165)
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT.setupBeforeAll(ElasticsearchConnectorKerberosWithSslIT.java:27)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
[2025-10-07 12:41:45,230] ERROR Log output from the failed container:

Replacing the ip address in the /usr/share/elasticsearch/config/ssl/instances.yml file with 10.113.124.248
Setting up Elasticsearch and generating certificates in /usr/share/elasticsearch
=== CREATE Keystore ===
Elastic password is: elastic
2025-10-07 12:36:45,547 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

Created elasticsearch keystore in /usr/share/elasticsearch/config/elasticsearch.keystore
Setting bootstrap.password...
2025-10-07 12:36:46,206 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

=== CREATE SSL CERTS ===
Creating cluster-ca.zip... (warnings are benign)
2025-10-07 12:36:47,117 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.CertificateTool.main(CertificateTool.java:149)

Unzip ca files...
Archive:  /usr/share/elasticsearch/config/ssl/cluster-ca.zip
   creating: /usr/share/elasticsearch/config/ssl/ca/
  inflating: /usr/share/elasticsearch/config/ssl/ca/ca.crt  
  inflating: /usr/share/elasticsearch/config/ssl/ca/ca.key  
Create cluster certs zipfile... (warnings are benign)
2025-10-07 12:36:48,106 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.CertificateTool.main(CertificateTool.java:149)

Unzipping cluster certs zipfile...
Archive:  /usr/share/elasticsearch/config/ssl/cluster.zip
   creating: /usr/share/elasticsearch/config/ssl/cluster/elasticsearch/
  inflating: /usr/share/elasticsearch/config/ssl/cluster/elasticsearch/elasticsearch.crt  
  inflating: /usr/share/elasticsearch/config/ssl/cluster/elasticsearch/elasticsearch.key  
   creating: /usr/share/elasticsearch/config/ssl/cluster/kibana/
  inflating: /usr/share/elasticsearch/config/ssl/cluster/kibana/kibana.crt  
  inflating: /usr/share/elasticsearch/config/ssl/cluster/kibana/kibana.key  
   creating: /usr/share/elasticsearch/config/ssl/cluster/logstash/
  inflating: /usr/share/elasticsearch/config/ssl/cluster/logstash/logstash.crt  
  inflating: /usr/share/elasticsearch/config/ssl/cluster/logstash/logstash.key  
Move elasticsearch certs to SSL config dir...
Generating truststore at /usr/share/elasticsearch/config/ssl/truststore.jks
Certificate was added to keystore
Generating keystore for client at /usr/share/elasticsearch/config/ssl/keystore.jks
Importing keystore /usr/share/elasticsearch/config/ssl/client.p12 to /usr/share/elasticsearch/config/ssl/keystore.jks...
Entry for alias clientkey successfully imported.
Import command completed:  1 entries successfully imported, 0 entries failed or cancelled

Warning:
The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format using "keytool -importkeystore -srckeystore /usr/share/elasticsearch/config/ssl/keystore.jks -destkeystore /usr/share/elasticsearch/config/ssl/keystore.jks -deststoretype pkcs12".
Elasticsearch Configuration
## Used by Docker images in our integration test
http.host: 0.0.0.0
network.host: 0.0.0.0
transport.host: 0.0.0.0

node.store.allow_mmap: false
cluster.routing.allocation.disk.threshold_enabled: false
discovery.type: single-node

xpack.license.self_generated.type: trial
xpack.security.enabled: true
xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.client_authentication: optional
xpack.security.http.ssl.verification_mode: certificate
xpack.security.http.ssl.key:  ssl/elasticsearch.key
xpack.security.http.ssl.certificate: ssl/elasticsearch.crt
xpack.security.http.ssl.certificate_authorities: [ "ssl/ca/ca.crt" ]

xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.key:  ssl/elasticsearch.key
xpack.security.transport.ssl.certificate: ssl/elasticsearch.crt
xpack.security.transport.ssl.certificate_authorities: [ "ssl/ca/ca.crt" ]

# enable anonymous connections since setting passwords requires running a command
xpack.security.authc:
  anonymous:
    username: connect_user
    roles: superuser
    authz_exception: true

# Kerberos realm
xpack.security.authc.realms.kerberos.kerb1:
  order: 3
  keytab.path: es.keytab
  remove_realm_name: false
Starting Elasticsearch with SSL and Kerberos enabled ...
2025-10-07 12:36:49,747 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.cli.keystore.KeyStoreCli.main(KeyStoreCli.java:33)

2025-10-07 12:36:50,318 main ERROR Could not reconfigure JMX java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/java.lang.management.ManagementFactory.lambda$getPlatformMBeanServer$0(ManagementFactory.java:489)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1779)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
	at java.management/java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:490)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:302)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:209)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:243)
	at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:219)
	at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:248)
	at org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig(LogConfigurator.java:95)
	at org.elasticsearch.common.cli.CommandLoggingConfigurator.configureLoggingWithoutConfig(CommandLoggingConfigurator.java:29)
	at org.elasticsearch.cli.Command.main(Command.java:74)
	at org.elasticsearch.xpack.security.cli.AutoConfigureNode.main(AutoConfigureNode.java:157)

Exception in thread "main" java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
	at java.base/jdk.internal.platform.cgroupv2.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:80)
	at java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:114)
	at java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:177)
	at java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)
	at java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)
	at java.base/jdk.internal.platform.Container.metrics(Container.java:43)
	at jdk.management/com.sun.management.internal.OperatingSystemImpl.<init>(OperatingSystemImpl.java:182)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)
	at jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl$3.nameToMBeanMap(PlatformMBeanProviderImpl.java:199)
	at java.management/sun.management.spi.PlatformMBeanProvider$PlatformComponent.getMBeans(PlatformMBeanProvider.java:195)
	at java.management/java.lang.management.ManagementFactory.getPlatformMXBean(ManagementFactory.java:689)
	at java.management/java.lang.management.ManagementFactory.getOperatingSystemMXBean(ManagementFactory.java:389)
	at org.elasticsearch.tools.launchers.DefaultSystemMemoryInfo.<init>(DefaultSystemMemoryInfo.java:29)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.jvmOptions(JvmOptionsParser.java:132)
	at org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:87)
 (ðŸ³ [localhost/testcontainers/o2gaevpzgbvimywz:latest]:519)
[2025-10-07 12:41:45,247] INFO Default Internal kdc server stopped. (org.apache.kerby.kerberos.kerb.server.impl.DefaultInternalKdcServerImpl:102)
[2025-10-07 12:41:46,247] INFO MiniKdc stopped. (org.apache.hadoop.minikdc.MiniKdc:359)
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 321.122 s <<< FAILURE! - in io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT
[ERROR] io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT  Time elapsed: 0 s  <<< ERROR!
org.testcontainers.containers.ContainerLaunchException: Container startup failed
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT.setupBeforeAll(ElasticsearchConnectorKerberosIT.java:37)
Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT.setupBeforeAll(ElasticsearchConnectorKerberosIT.java:37)
Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT.setupBeforeAll(ElasticsearchConnectorKerberosIT.java:37)
Caused by: java.lang.IllegalStateException: Container exited with code 1
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT.setupBeforeAll(ElasticsearchConnectorKerberosIT.java:37)

[INFO] Running io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.001 s <<< FAILURE! - in io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT
[ERROR] io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT  Time elapsed: 0 s  <<< ERROR!
org.testcontainers.containers.ContainerLaunchException: Container startup failed
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT.setupBeforeAll(ElasticsearchConnectorKerberosWithSslIT.java:27)
Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT.setupBeforeAll(ElasticsearchConnectorKerberosWithSslIT.java:27)
Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT.setupBeforeAll(ElasticsearchConnectorKerberosWithSslIT.java:27)
Caused by: java.lang.IllegalStateException: Container exited with code 1
	at io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT.setupBeforeAll(ElasticsearchConnectorKerberosWithSslIT.java:27)

[INFO] Running io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorNetworkIT
[2025-10-07 12:41:46,644] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$:31)
[2025-10-07 12:41:46,813] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster8998971427263761333
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:39725
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:41:46,924] INFO starting (kafka.server.KafkaServer:66)
[2025-10-07 12:41:46,924] INFO Connecting to zookeeper on 127.0.0.1:39725 (kafka.server.KafkaServer:66)
[2025-10-07 12:41:46,945] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:39725. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:41:46,970] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:41:47,001] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:41:47,116] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:41:47,133] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 12:41:47,134] INFO Cleared cache (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 12:41:47,154] INFO Cluster ID = ssFQpcUFSga_BhRZV1aA8w (kafka.server.KafkaServer:66)
[2025-10-07 12:41:47,158] WARN No meta.properties file under dir /tmp/EmbeddedKafkaCluster8998971427263761333/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2025-10-07 12:41:47,202] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster8998971427263761333
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:39725
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:41:47,212] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster8998971427263761333
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:39725
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:41:47,250] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:41:47,253] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:41:47,256] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:41:47,258] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:41:47,296] INFO Loading logs from log dirs ArraySeq(/tmp/EmbeddedKafkaCluster8998971427263761333) (kafka.log.LogManager:66)
[2025-10-07 12:41:47,301] INFO Attempting recovery for all logs in /tmp/EmbeddedKafkaCluster8998971427263761333 since no clean shutdown file was found (kafka.log.LogManager:66)
[2025-10-07 12:41:47,305] INFO Loaded 0 logs in 0ms. (kafka.log.LogManager:66)
[2025-10-07 12:41:47,305] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:66)
[2025-10-07 12:41:47,308] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:66)
[2025-10-07 12:41:47,322] INFO Starting the log cleaner (kafka.log.LogCleaner:66)
[2025-10-07 12:41:47,406] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner:66)
[2025-10-07 12:41:47,832] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:41:47,956] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas:66)
[2025-10-07 12:41:47,960] INFO Awaiting socket connections on localhost:34529. (kafka.network.Acceptor:66)
[2025-10-07 12:41:47,986] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 12:41:47,993] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:41:48,021] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:48,022] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:48,024] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:48,025] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:48,042] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:41:48,067] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:41:48,086] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1759840908079,1759840908079,1,0,0,72057674237280256,204,0,25
 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:41:48,086] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:34529, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:41:48,153] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:41:48,164] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:48,170] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:41:48,172] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:48,173] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:48,179] INFO [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,183] INFO [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{}) (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,184] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 12:41:48,198] INFO [Controller id=0] Registering handlers (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,198] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 12:41:48,199] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:48,201] INFO [Controller id=0] Deleting log dir event notifications (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,202] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:48,203] INFO [Controller id=0] Deleting isr change notifications (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,204] INFO [Controller id=0] Initializing controller context (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,215] INFO [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25) (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,223] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:41:48,228] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:41:48,228] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:41:48,232] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread:66)
[2025-10-07 12:41:48,234] INFO [Controller id=0] Currently active brokers in the cluster: Set(0) (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,234] INFO [Controller id=0] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,234] INFO [Controller id=0] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,234] INFO [Controller id=0] Fetching topic deletions in progress (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,236] INFO [Controller id=0] List of topics to be deleted:  (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,236] INFO [Controller id=0] List of topics ineligible for deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,236] INFO [Controller id=0] Initializing topic deletion manager (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,237] INFO [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager:66)
[2025-10-07 12:41:48,238] INFO [Controller id=0] Sending update metadata request (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,240] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:48,246] INFO [ReplicaStateMachine controllerId=0] Initializing replica state (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:41:48,247] INFO [ReplicaStateMachine controllerId=0] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:41:48,257] INFO [ReplicaStateMachine controllerId=0] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:41:48,257] INFO [PartitionStateMachine controllerId=0] Initializing partition state (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:41:48,258] INFO [PartitionStateMachine controllerId=0] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:41:48,260] INFO [RequestSendThread controllerId=0] Controller 0 connected to localhost:34529 (id: 0 rack: null) for sending state change requests (kafka.controller.RequestSendThread:66)
[2025-10-07 12:41:48,261] INFO [Controller id=0] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,265] INFO [Controller id=0] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,265] INFO [Controller id=0] Partitions that completed preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,266] INFO [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,266] INFO [Controller id=0] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,267] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,268] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:48,278] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController:66)
[2025-10-07 12:41:48,291] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:41:48,298] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 12:41:48,301] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 12:41:48,302] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 12:41:48,304] INFO [KafkaServer id=0] started (kafka.server.KafkaServer:66)
[2025-10-07 12:41:48,426] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker localhost:34529 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:41:48,426] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:34529 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:41:48,638] INFO [Controller id=0] Acquired new producerId block ProducerIdsBlock{brokerId=0, producerIdStart=0, producerIdLen=1000} by writing to Zk with path version 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:41:51,974] INFO Creating topic connect-offset-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:41:51,999] INFO [Controller id=0] New topics: [Set(connect-offset-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-offset-topic-elasticsearch-it-connect-cluster,Some(HQc5hToFR3K2IOzQiNrtug),HashMap(connect-offset-topic-elasticsearch-it-connect-cluster-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:41:52,000] INFO [Controller id=0] New partition creation callback for connect-offset-topic-elasticsearch-it-connect-cluster-20,connect-offset-topic-elasticsearch-it-connect-cluster-7,connect-offset-topic-elasticsearch-it-connect-cluster-0,connect-offset-topic-elasticsearch-it-connect-cluster-12,connect-offset-topic-elasticsearch-it-connect-cluster-13,connect-offset-topic-elasticsearch-it-connect-cluster-5,connect-offset-topic-elasticsearch-it-connect-cluster-21,connect-offset-topic-elasticsearch-it-connect-cluster-14,connect-offset-topic-elasticsearch-it-connect-cluster-6,connect-offset-topic-elasticsearch-it-connect-cluster-1,connect-offset-topic-elasticsearch-it-connect-cluster-19,connect-offset-topic-elasticsearch-it-connect-cluster-17,connect-offset-topic-elasticsearch-it-connect-cluster-16,connect-offset-topic-elasticsearch-it-connect-cluster-23,connect-offset-topic-elasticsearch-it-connect-cluster-10,connect-offset-topic-elasticsearch-it-connect-cluster-3,connect-offset-topic-elasticsearch-it-connect-cluster-15,connect-offset-topic-elasticsearch-it-connect-cluster-22,connect-offset-topic-elasticsearch-it-connect-cluster-24,connect-offset-topic-elasticsearch-it-connect-cluster-18,connect-offset-topic-elasticsearch-it-connect-cluster-2,connect-offset-topic-elasticsearch-it-connect-cluster-4,connect-offset-topic-elasticsearch-it-connect-cluster-8,connect-offset-topic-elasticsearch-it-connect-cluster-9,connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.controller.KafkaController:66)
[2025-10-07 12:41:52,002] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,002] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,002] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,002] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,002] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,003] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,004] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:52,010] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:52,081] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,081] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,081] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,081] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,081] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,081] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,081] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,081] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,081] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,081] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,081] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,082] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,082] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,082] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,082] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,082] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,082] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,082] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,082] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,082] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,082] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,082] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,082] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,082] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,082] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,083] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 25 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:41:52,085] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 25 partitions (state.change.logger:66)
[2025-10-07 12:41:52,087] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:52,092] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 12:41:52,133] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-offset-topic-elasticsearch-it-connect-cluster-20, connect-offset-topic-elasticsearch-it-connect-cluster-7, connect-offset-topic-elasticsearch-it-connect-cluster-0, connect-offset-topic-elasticsearch-it-connect-cluster-12, connect-offset-topic-elasticsearch-it-connect-cluster-13, connect-offset-topic-elasticsearch-it-connect-cluster-5, connect-offset-topic-elasticsearch-it-connect-cluster-21, connect-offset-topic-elasticsearch-it-connect-cluster-14, connect-offset-topic-elasticsearch-it-connect-cluster-6, connect-offset-topic-elasticsearch-it-connect-cluster-1, connect-offset-topic-elasticsearch-it-connect-cluster-19, connect-offset-topic-elasticsearch-it-connect-cluster-17, connect-offset-topic-elasticsearch-it-connect-cluster-16, connect-offset-topic-elasticsearch-it-connect-cluster-23, connect-offset-topic-elasticsearch-it-connect-cluster-10, connect-offset-topic-elasticsearch-it-connect-cluster-3, connect-offset-topic-elasticsearch-it-connect-cluster-15, connect-offset-topic-elasticsearch-it-connect-cluster-22, connect-offset-topic-elasticsearch-it-connect-cluster-24, connect-offset-topic-elasticsearch-it-connect-cluster-18, connect-offset-topic-elasticsearch-it-connect-cluster-2, connect-offset-topic-elasticsearch-it-connect-cluster-4, connect-offset-topic-elasticsearch-it-connect-cluster-8, connect-offset-topic-elasticsearch-it-connect-cluster-9, connect-offset-topic-elasticsearch-it-connect-cluster-11) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:41:52,134] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 25 partitions (state.change.logger:66)
[2025-10-07 12:41:52,201] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-10, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,220] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-10 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,222] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,223] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,224] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,236] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-14, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,237] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-14 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,237] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,237] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,237] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,248] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-18, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,249] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-18 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,249] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,249] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,249] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,260] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,261] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,261] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,261] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,261] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,272] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-22, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,273] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-22 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,273] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,273] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,273] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,284] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-7, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,284] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-7 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,284] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,284] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,285] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,295] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-11, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,296] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-11 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,296] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,296] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,296] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,307] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-15, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,308] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-15 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,308] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,308] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,308] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,319] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,320] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,320] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,320] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,320] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,331] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-19, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,332] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-19 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,332] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,332] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,332] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,342] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,343] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,343] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,343] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,343] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,354] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-23, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,354] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-23 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,355] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,355] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,355] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,365] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-8, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,366] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-8 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,366] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,366] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,366] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,377] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-12, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,378] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-12 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
Oct 07, 2025 12:41:52 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
[2025-10-07 12:41:52,378] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,378] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,378] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 12:41:52 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Oct 07, 2025 12:41:52 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Oct 07, 2025 12:41:52 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
[2025-10-07 12:41:52,388] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-16, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,389] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-16 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,389] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,389] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,389] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,400] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,400] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,400] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,400] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,400] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,411] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-20, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,412] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-20 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,412] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,412] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,412] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,422] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-5, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,423] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-5 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,423] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,423] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,423] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,437] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-24, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,438] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-24 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,438] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,438] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,438] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,453] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-9, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,455] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-9 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,455] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,455] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,455] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,466] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-13, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,466] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-13 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,466] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,467] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,467] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,478] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-17, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,478] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-17 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,478] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,479] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,479] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,502] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,503] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,503] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,503] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,503] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,515] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-21, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,515] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-21 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,515] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,515] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,515] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,526] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-6, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,527] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-offset-topic-elasticsearch-it-connect-cluster-6 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,527] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,527] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,527] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,544] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 12:41:52,554] INFO [Broker id=0] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger:66)
[2025-10-07 12:41:52,673] INFO Creating topic connect-storage-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:41:52,683] INFO [Controller id=0] New topics: [Set(connect-storage-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-storage-topic-elasticsearch-it-connect-cluster,Some(5orN8bd3TL-F14dgvHe-cw),HashMap(connect-storage-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:41:52,683] INFO [Controller id=0] New partition creation callback for connect-storage-topic-elasticsearch-it-connect-cluster-2,connect-storage-topic-elasticsearch-it-connect-cluster-0,connect-storage-topic-elasticsearch-it-connect-cluster-3,connect-storage-topic-elasticsearch-it-connect-cluster-4,connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.controller.KafkaController:66)
[2025-10-07 12:41:52,684] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,684] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,684] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,684] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,684] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,684] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:52,685] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:52,701] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,701] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,701] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,701] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,701] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,701] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:41:52,702] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions (state.change.logger:66)
[2025-10-07 12:41:52,702] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:52,703] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 12:41:52,708] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-storage-topic-elasticsearch-it-connect-cluster-2, connect-storage-topic-elasticsearch-it-connect-cluster-0, connect-storage-topic-elasticsearch-it-connect-cluster-3, connect-storage-topic-elasticsearch-it-connect-cluster-4, connect-storage-topic-elasticsearch-it-connect-cluster-1) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:41:52,709] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions (state.change.logger:66)
[2025-10-07 12:41:52,713] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,715] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-storage-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,715] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,716] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,716] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,726] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,726] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-storage-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,726] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,726] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,727] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,737] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,738] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-storage-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,738] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,738] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,738] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,749] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,749] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-storage-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,749] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,749] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,749] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,760] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,761] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-storage-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,761] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,761] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,761] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,770] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 12:41:52,772] INFO [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger:66)
[2025-10-07 12:41:52,807] INFO Creating topic connect-config-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:41:52,813] INFO [Controller id=0] New topics: [Set(connect-config-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-config-topic-elasticsearch-it-connect-cluster,Some(jQUkHhppSW2MlvMd5eriTA),Map(connect-config-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:41:52,813] INFO [Controller id=0] New partition creation callback for connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.controller.KafkaController:66)
[2025-10-07 12:41:52,813] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,813] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:52,814] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:52,817] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,818] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:41:52,818] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 12:41:52,818] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:52,818] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:41:52,819] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(connect-config-topic-elasticsearch-it-connect-cluster-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:41:52,819] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 12:41:52,822] INFO [LogLoader partition=connect-config-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,822] INFO Created log for partition connect-config-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster8998971427263761333/connect-config-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,823] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,823] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-config-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,823] INFO [Broker id=0] Leader connect-config-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,832] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 5 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:41:52,834] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6 (state.change.logger:66)
Oct 07, 2025 12:41:52 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

[2025-10-07 12:41:52,875] INFO Creating topic test with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:41:52,876] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:41:52,880] INFO [Controller id=0] New topics: [Set(test)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(test,Some(p7bVWAVLR-ylJXnB6y1Xcg),Map(test-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:41:52,880] INFO [Controller id=0] New partition creation callback for test-0 (kafka.controller.KafkaController:66)
[2025-10-07 12:41:52,881] INFO [Controller id=0 epoch=1] Changed partition test-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,881] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:52,881] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:52,884] INFO [Controller id=0 epoch=1] Changed partition test-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,884] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:41:52,884] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 12:41:52,885] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:52,885] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:41:52,890] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:41:52,890] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 12:41:52,893] INFO [LogLoader partition=test-0, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,893] INFO Created log for partition test-0 in /tmp/EmbeddedKafkaCluster8998971427263761333/test-0 with properties {} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,893] INFO [Controller id=0] New topics: [HashSet(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(SQpEZecqTa6fs8c1DRsBCQ),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:41:52,895] INFO [Partition test-0 broker=0] No checkpointed highwatermark is found for partition test-0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,895] INFO [Partition test-0 broker=0] Log loaded for partition test-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,896] INFO [Broker id=0] Leader test-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,896] INFO [Controller id=0] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,897] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,898] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,899] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,899] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,899] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,899] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,899] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,899] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,899] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:41:52,899] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:52,900] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:52,903] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:41:52,906] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger:66)
[2025-10-07 12:41:52,942] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,942] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,942] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,942] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,942] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,942] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,943] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,944] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,944] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,944] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,944] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,944] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,944] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,944] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,944] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,944] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,944] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,944] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,944] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,945] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,945] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,945] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,945] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,945] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,945] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,945] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,945] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:41:52,945] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 50 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:41:52,946] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 50 partitions (state.change.logger:66)
[2025-10-07 12:41:52,947] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 12:41:52,947] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:52,959] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:41:52,959] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 0 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger:66)
[2025-10-07 12:41:52,962] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,963] INFO Created log for partition __consumer_offsets-3 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,963] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,963] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,963] INFO [Broker id=0] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,978] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,979] INFO Created log for partition __consumer_offsets-18 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,979] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,979] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,979] INFO [Broker id=0] Leader __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:52,990] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:52,996] INFO Created log for partition __consumer_offsets-41 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:52,996] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,996] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:52,996] INFO [Broker id=0] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,008] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,009] INFO Created log for partition __consumer_offsets-10 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,009] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,009] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,009] INFO [Broker id=0] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,019] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,020] INFO Created log for partition __consumer_offsets-33 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,020] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,020] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,020] INFO [Broker id=0] Leader __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,030] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,031] INFO Created log for partition __consumer_offsets-48 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,031] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,031] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,031] INFO [Broker id=0] Leader __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,042] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,042] INFO Created log for partition __consumer_offsets-19 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,042] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,043] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,043] INFO [Broker id=0] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,053] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,054] INFO Created log for partition __consumer_offsets-34 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,054] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,054] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,054] INFO [Broker id=0] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,065] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,065] INFO Created log for partition __consumer_offsets-4 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,066] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,066] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,066] INFO [Broker id=0] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,077] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,077] INFO Created log for partition __consumer_offsets-11 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,077] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,077] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,077] INFO [Broker id=0] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,088] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,089] INFO Created log for partition __consumer_offsets-26 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,089] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,089] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,089] INFO [Broker id=0] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,100] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,100] INFO Created log for partition __consumer_offsets-49 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,100] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,100] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,101] INFO [Broker id=0] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,111] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,111] INFO Created log for partition __consumer_offsets-39 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,111] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,111] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,111] INFO [Broker id=0] Leader __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,122] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,123] INFO Created log for partition __consumer_offsets-9 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,123] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,123] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,123] INFO [Broker id=0] Leader __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,134] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,134] INFO Created log for partition __consumer_offsets-24 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,135] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,135] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,135] INFO [Broker id=0] Leader __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,145] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,146] INFO Created log for partition __consumer_offsets-31 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,146] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,146] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,146] INFO [Broker id=0] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,157] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,157] INFO Created log for partition __consumer_offsets-46 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,157] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,157] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,157] INFO [Broker id=0] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,168] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,168] INFO Created log for partition __consumer_offsets-1 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,168] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,169] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,169] INFO [Broker id=0] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,180] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,180] INFO Created log for partition __consumer_offsets-16 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,180] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,180] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,180] INFO [Broker id=0] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,192] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,192] INFO Created log for partition __consumer_offsets-2 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,193] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,193] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,193] INFO [Broker id=0] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,204] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,204] INFO Created log for partition __consumer_offsets-25 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,204] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,204] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,204] INFO [Broker id=0] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,215] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,215] INFO Created log for partition __consumer_offsets-40 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,215] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,215] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,216] INFO [Broker id=0] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,228] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,229] INFO Created log for partition __consumer_offsets-47 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,229] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,229] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,229] INFO [Broker id=0] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,240] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,240] INFO Created log for partition __consumer_offsets-17 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,240] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,240] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,240] INFO [Broker id=0] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,251] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,253] INFO Created log for partition __consumer_offsets-32 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,253] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,253] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,253] INFO [Broker id=0] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,263] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,263] INFO Created log for partition __consumer_offsets-37 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,263] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,263] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,263] INFO [Broker id=0] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,274] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,275] INFO Created log for partition __consumer_offsets-7 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,275] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,275] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,275] INFO [Broker id=0] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,279] INFO [Controller id=0] Processing automatic preferred replica leader election (kafka.controller.KafkaController:66)
[2025-10-07 12:41:53,286] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,287] INFO Created log for partition __consumer_offsets-22 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,287] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,287] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,287] INFO [Broker id=0] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,298] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,298] INFO Created log for partition __consumer_offsets-29 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,299] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,299] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,299] INFO [Broker id=0] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,309] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,309] INFO Created log for partition __consumer_offsets-44 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,310] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,310] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,310] INFO [Broker id=0] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,321] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,321] INFO Created log for partition __consumer_offsets-14 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,321] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,321] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,321] INFO [Broker id=0] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,333] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,334] INFO Created log for partition __consumer_offsets-23 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,334] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,334] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,334] INFO [Broker id=0] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,345] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,345] INFO Created log for partition __consumer_offsets-38 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,345] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,345] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,345] INFO [Broker id=0] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,357] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,357] INFO Created log for partition __consumer_offsets-8 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,357] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,357] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,357] INFO [Broker id=0] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,370] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,370] INFO Created log for partition __consumer_offsets-45 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,370] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,370] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,370] INFO [Broker id=0] Leader __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,381] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,382] INFO Created log for partition __consumer_offsets-15 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,382] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,382] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,382] INFO [Broker id=0] Leader __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,393] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,394] INFO Created log for partition __consumer_offsets-30 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,394] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,394] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,394] INFO [Broker id=0] Leader __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,405] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,406] INFO Created log for partition __consumer_offsets-0 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,406] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,406] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,406] INFO [Broker id=0] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,417] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,418] INFO Created log for partition __consumer_offsets-35 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,418] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,418] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,418] INFO [Broker id=0] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,429] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,429] INFO Created log for partition __consumer_offsets-5 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,429] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,429] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,429] INFO [Broker id=0] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,440] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,441] INFO Created log for partition __consumer_offsets-20 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,441] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,441] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,441] INFO [Broker id=0] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,452] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,452] INFO Created log for partition __consumer_offsets-27 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,452] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,452] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,452] INFO [Broker id=0] Leader __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,464] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,464] INFO Created log for partition __consumer_offsets-42 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,464] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,464] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,464] INFO [Broker id=0] Leader __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,475] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,476] INFO Created log for partition __consumer_offsets-12 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,476] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,476] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,476] INFO [Broker id=0] Leader __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,487] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,487] INFO Created log for partition __consumer_offsets-21 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,487] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,487] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,488] INFO [Broker id=0] Leader __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,498] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,498] INFO Created log for partition __consumer_offsets-36 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,499] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,499] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,499] INFO [Broker id=0] Leader __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,510] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,510] INFO Created log for partition __consumer_offsets-6 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,510] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,510] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,510] INFO [Broker id=0] Leader __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,522] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,522] INFO Created log for partition __consumer_offsets-43 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,523] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,523] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,523] INFO [Broker id=0] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,534] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,535] INFO Created log for partition __consumer_offsets-13 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,535] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,535] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,535] INFO [Broker id=0] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,546] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/EmbeddedKafkaCluster8998971427263761333] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:41:53,546] INFO Created log for partition __consumer_offsets-28 in /tmp/EmbeddedKafkaCluster8998971427263761333/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:41:53,546] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,546] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:41:53,547] INFO [Broker id=0] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:41:53,557] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,559] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,560] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,560] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,560] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,560] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,560] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,560] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,560] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,561] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,561] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,561] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,561] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,561] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,561] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,561] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,561] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,561] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,561] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,561] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,561] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,561] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,561] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,561] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,561] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,561] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,561] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,562] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,562] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,562] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,562] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,562] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,562] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,562] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,562] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,562] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,562] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,562] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,562] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,562] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,562] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,562] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,562] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,562] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,562] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,563] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,563] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,563] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,563] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,563] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,563] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,563] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,563] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,563] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,563] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,563] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,563] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,563] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,563] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,563] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,563] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,563] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,563] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,563] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,564] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,564] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,564] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,564] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,564] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,564] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,564] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,564] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,564] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,564] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,564] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,564] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,564] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,564] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,564] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,564] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,564] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,564] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,564] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,565] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,565] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,565] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,565] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,565] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,565] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,565] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,565] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 5 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,565] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,566] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,566] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,566] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,566] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,566] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,566] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,566] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,566] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,566] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,566] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,566] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,566] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,566] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,566] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,566] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 12:41:53,566] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,567] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,567] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,567] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,567] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,567] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,567] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,567] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,568] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,568] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,568] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,568] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,569] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,569] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,569] INFO [Broker id=0] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 10 (state.change.logger:66)
[2025-10-07 12:41:53,569] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,569] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,569] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,569] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,570] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,570] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,570] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,570] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,570] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,570] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,570] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,570] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,570] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,571] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,571] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,571] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,571] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,571] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,571] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,571] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,571] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,572] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,572] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,572] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,572] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,572] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,572] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,572] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,573] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,573] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:41:53,613] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-integration-test-elasticsearch-it-connect-cluster in Empty state. Created a new member id connect-1-4b7fe0e4-53ba-4fb7-9bac-0e3cd3f737ff and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,624] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member connect-1-4b7fe0e4-53ba-4fb7-9bac-0e3cd3f737ff with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,630] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 1 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,651] INFO [GroupCoordinator 0]: Assignment received from leader connect-1-4b7fe0e4-53ba-4fb7-9bac-0e3cd3f737ff for group connect-integration-test-elasticsearch-it-connect-cluster for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,749] INFO ElasticsearchSinkConnectorConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:33713/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 600000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 600000
	max.buffered.records = 10
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 600000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 12:41:53,751] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:41:53,751] INFO Using unsecured connection to [http://localhost:33713]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:41:53,788] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 1 (__consumer_offsets-4) (reason: Leader connect-1-4b7fe0e4-53ba-4fb7-9bac-0e3cd3f737ff re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,790] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 2 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,796] INFO [GroupCoordinator 0]: Assignment received from leader connect-1-4b7fe0e4-53ba-4fb7-9bac-0e3cd3f737ff for group connect-integration-test-elasticsearch-it-connect-cluster for generation 2. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,813] INFO [es-connector|worker] ElasticsearchSinkConnectorConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:33713/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 600000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 600000
	max.buffered.records = 10
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 600000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 12:41:53,863] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 2 (__consumer_offsets-4) (reason: Leader connect-1-4b7fe0e4-53ba-4fb7-9bac-0e3cd3f737ff re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,864] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 3 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,869] INFO [GroupCoordinator 0]: Assignment received from leader connect-1-4b7fe0e4-53ba-4fb7-9bac-0e3cd3f737ff for group connect-integration-test-elasticsearch-it-connect-cluster for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,870] ERROR Could not check connector state info. (io.confluent.connect.elasticsearch.integration.BaseConnectorIT:92)
[2025-10-07 12:41:53,894] INFO [es-connector|task-0] Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:41:53,895] INFO [es-connector|task-0] ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:33713/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 600000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 600000
	max.buffered.records = 10
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 600000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 0
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:41:53,895] INFO [es-connector|task-0] Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:41:53,896] DEBUG [es-connector|task-0] Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:41:53,896] INFO [es-connector|task-0] Using unsecured connection to [http://localhost:33713]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:41:53,910] INFO [es-connector|task-0] Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:41:53,928] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-es-connector in Empty state. Created a new member id connector-consumer-es-connector-0-0543713e-b7dd-4c9e-8849-b277f884aa14 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,932] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member connector-consumer-es-connector-0-0543713e-b7dd-4c9e-8849-b277f884aa14 with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,936] INFO [GroupCoordinator 0]: Stabilized group connect-es-connector generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:53,948] INFO [GroupCoordinator 0]: Assignment received from leader connector-consumer-es-connector-0-0543713e-b7dd-4c9e-8849-b277f884aa14 for group connect-es-connector for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:54,027] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,028] INFO [es-connector|task-0] Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:41:54,133] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,135] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,140] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,151] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,153] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,159] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,164] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,167] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,169] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,173] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,177] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,179] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,181] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,188] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,197] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,200] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,202] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,204] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,206] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,208] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,209] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,211] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,212] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,213] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,215] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,219] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,220] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,225] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,226] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,228] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,229] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,231] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,233] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,235] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,237] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,238] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,240] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,242] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,248] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,250] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,251] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,252] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,256] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,257] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,258] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,262] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,263] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,268] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,269] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,270] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,271] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,273] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,274] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,275] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,276] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,278] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,279] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,280] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,281] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,284] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,285] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,287] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,288] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,289] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,291] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,292] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,294] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,296] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,298] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,300] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,301] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,303] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,304] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,307] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,309] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,310] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,312] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,313] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,314] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,315] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,318] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,319] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,321] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,323] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,324] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,327] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,329] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,331] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,333] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,334] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,336] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,338] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,339] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,340] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,342] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,343] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,347] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,348] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,348] DEBUG [es-connector|task-0] Pausing all partitions (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:363)
[2025-10-07 12:41:54,448] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,548] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,649] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,751] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,851] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:54,952] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:55,052] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:55,153] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:55,254] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:55,354] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:55,455] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:55,556] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:55,656] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:55,757] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:55,857] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:55,958] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:56,058] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:56,159] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:56,259] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:56,360] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:56,460] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:56,561] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:56,661] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:56,762] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:56,862] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:56,970] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:57,070] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:57,171] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:57,171] DEBUG [es-connector|task-0] Resuming all partitions (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:348)
[2025-10-07 12:41:57,172] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:57,194] DEBUG [es-connector|task-0] Putting 500 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:57,368] DEBUG [es-connector|task-0] Putting 399 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:41:57,499] DEBUG [es-connector|task-0] preCommitting offsets {test-0=OffsetAndMetadata{offset=1001, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:41:57,516] DEBUG [es-connector|task-0] Stopping Elasticsearch client. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:146)
[2025-10-07 12:41:57,525] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 1 (__consumer_offsets-23) (reason: Removing member connector-consumer-es-connector-0-0543713e-b7dd-4c9e-8849-b277f884aa14 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:57,525] INFO [GroupCoordinator 0]: Group connect-es-connector with generation 2 is now empty (__consumer_offsets-23) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:57,527] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connector-consumer-es-connector-0-0543713e-b7dd-4c9e-8849-b277f884aa14, groupInstanceId=None, clientId=connector-consumer-es-connector-0, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group connect-es-connector through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:57,535] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 3 (__consumer_offsets-4) (reason: Removing member connect-1-4b7fe0e4-53ba-4fb7-9bac-0e3cd3f737ff on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:57,535] INFO [GroupCoordinator 0]: Group connect-integration-test-elasticsearch-it-connect-cluster with generation 4 is now empty (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:57,536] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connect-1-4b7fe0e4-53ba-4fb7-9bac-0e3cd3f737ff, groupInstanceId=None, clientId=connect-1, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(sessioned, compatible, default)) has left group connect-integration-test-elasticsearch-it-connect-cluster through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:57,549] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer:66)
[2025-10-07 12:41:57,550] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer:66)
[2025-10-07 12:41:57,553] INFO [Controller id=0] Shutting down broker 0 (kafka.controller.KafkaController:66)
[2025-10-07 12:41:57,557] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:41:57,559] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer:66)
[2025-10-07 12:41:57,561] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:41:57,562] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:41:57,562] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:41:57,562] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 12:41:57,570] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 12:41:57,570] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 12:41:57,576] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 12:41:57,578] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:57,713] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:57,713] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:57,713] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis:66)
[2025-10-07 12:41:57,714] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:57,913] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:57,913] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:57,915] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:41:57,916] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager:66)
[2025-10-07 12:41:57,916] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:41:57,917] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:41:57,917] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:41:57,919] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:41:57,919] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:57,919] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,113] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,113] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,114] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,136] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,137] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,137] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:41:58,138] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager:66)
[2025-10-07 12:41:58,138] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:41:58,141] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:41:58,141] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:41:58,143] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:41:58,145] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:41:58,145] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 12:41:58,145] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 12:41:58,145] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,237] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,237] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,238] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,313] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,313] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,314] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,512] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,512] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,513] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,513] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,513] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:41:58,520] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager:66)
[2025-10-07 12:41:58,521] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:41:58,521] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:41:58,521] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:41:58,525] INFO Broker to controller channel manager for alterIsr shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 12:41:58,525] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:41:58,525] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:41:58,525] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:41:58,527] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 12:41:58,527] INFO Shutting down. (kafka.log.LogManager:66)
[2025-10-07 12:41:58,528] INFO Shutting down the log cleaner. (kafka.log.LogCleaner:66)
[2025-10-07 12:41:58,529] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner:66)
[2025-10-07 12:41:58,529] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner:66)
[2025-10-07 12:41:58,529] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner:66)
[2025-10-07 12:41:58,553] INFO [ProducerStateManager partition=test-0] Wrote producer snapshot at offset 1001 with 1 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:41:58,578] INFO [ProducerStateManager partition=__consumer_offsets-23] Wrote producer snapshot at offset 3 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:41:58,584] INFO [ProducerStateManager partition=connect-config-topic-elasticsearch-it-connect-cluster-0] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:41:58,591] INFO [ProducerStateManager partition=__consumer_offsets-4] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:41:58,602] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-2] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:41:58,612] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-3] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:41:58,630] INFO Shutdown complete. (kafka.log.LogManager:66)
[2025-10-07 12:41:58,630] INFO [ControllerEventThread controllerId=0] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:41:58,631] INFO [ControllerEventThread controllerId=0] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:41:58,631] INFO [ControllerEventThread controllerId=0] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:41:58,633] INFO [PartitionStateMachine controllerId=0] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:41:58,634] INFO [ReplicaStateMachine controllerId=0] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:41:58,635] INFO [RequestSendThread controllerId=0] Shutting down (kafka.controller.RequestSendThread:66)
[2025-10-07 12:41:58,635] INFO [RequestSendThread controllerId=0] Shutdown completed (kafka.controller.RequestSendThread:66)
[2025-10-07 12:41:58,635] INFO [RequestSendThread controllerId=0] Stopped (kafka.controller.RequestSendThread:66)
[2025-10-07 12:41:58,639] INFO [Controller id=0] Resigned (kafka.controller.KafkaController:66)
[2025-10-07 12:41:58,639] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:41:58,639] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:41:58,639] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:41:58,640] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:41:58,748] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:41:58,748] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:41:59,255] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:41:59,255] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:41:59,255] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:00,255] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:00,255] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:00,256] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:00,258] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:00,258] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:00,258] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:00,260] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:00,260] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:00,261] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer:66)
[2025-10-07 12:42:00,294] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer:66)
[2025-10-07 12:42:00,296] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats:66)
[2025-10-07 12:42:00,297] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer:66)
[2025-10-07 12:42:00,344] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster5349058205751246927
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:44187
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:42:00,348] INFO starting (kafka.server.KafkaServer:66)
[2025-10-07 12:42:00,348] INFO Connecting to zookeeper on 127.0.0.1:44187 (kafka.server.KafkaServer:66)
[2025-10-07 12:42:00,349] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:44187. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:00,349] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:00,354] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:00,374] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:00,377] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 12:42:00,377] INFO Cleared cache (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 12:42:00,382] INFO Cluster ID = lf_JcsXQRRWVde1oKAPU4Q (kafka.server.KafkaServer:66)
[2025-10-07 12:42:00,382] WARN No meta.properties file under dir /tmp/EmbeddedKafkaCluster5349058205751246927/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2025-10-07 12:42:00,386] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster5349058205751246927
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:44187
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:42:00,388] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster5349058205751246927
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:44187
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:42:00,399] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:00,400] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:00,402] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:00,403] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:00,404] INFO Loading logs from log dirs ArraySeq(/tmp/EmbeddedKafkaCluster5349058205751246927) (kafka.log.LogManager:66)
[2025-10-07 12:42:00,404] INFO Attempting recovery for all logs in /tmp/EmbeddedKafkaCluster5349058205751246927 since no clean shutdown file was found (kafka.log.LogManager:66)
[2025-10-07 12:42:00,405] INFO Loaded 0 logs in 0ms. (kafka.log.LogManager:66)
[2025-10-07 12:42:00,405] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:66)
[2025-10-07 12:42:00,405] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:66)
[2025-10-07 12:42:00,406] INFO Starting the log cleaner (kafka.log.LogCleaner:66)
[2025-10-07 12:42:00,418] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner:66)
[2025-10-07 12:42:00,420] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:00,431] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas:66)
[2025-10-07 12:42:00,431] INFO Awaiting socket connections on localhost:33469. (kafka.network.Acceptor:66)
[2025-10-07 12:42:00,434] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 12:42:00,436] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:00,437] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:00,438] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:00,440] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:00,441] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:00,443] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:00,446] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:00,448] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1759840920446,1759840920446,1,0,0,72057675125620736,204,0,25
 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:00,448] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:33469, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:00,458] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:00,459] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:00,460] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:00,461] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:00,462] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:00,462] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:00,463] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:00,463] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:00,464] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:00,465] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:00,465] INFO [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,467] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:00,468] INFO [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{}) (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,469] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:00,470] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 12:42:00,471] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 12:42:00,471] INFO [Controller id=0] Registering handlers (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,472] INFO [Controller id=0] Deleting log dir event notifications (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,472] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:00,473] INFO [Controller id=0] Deleting isr change notifications (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,473] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 12:42:00,473] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:00,473] INFO [Controller id=0] Initializing controller context (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,473] INFO [KafkaServer id=0] started (kafka.server.KafkaServer:66)
[2025-10-07 12:42:00,475] INFO [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25) (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,480] INFO [Controller id=0] Currently active brokers in the cluster: Set(0) (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,481] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:00,481] INFO [Controller id=0] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,481] INFO [Controller id=0] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,481] INFO [Controller id=0] Fetching topic deletions in progress (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,482] INFO [Controller id=0] List of topics to be deleted:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,482] INFO [Controller id=0] List of topics ineligible for deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,482] INFO [Controller id=0] Initializing topic deletion manager (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,482] INFO [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager:66)
[2025-10-07 12:42:00,482] INFO [Controller id=0] Sending update metadata request (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,482] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:00,483] INFO [ReplicaStateMachine controllerId=0] Initializing replica state (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:00,483] INFO [ReplicaStateMachine controllerId=0] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:00,484] INFO [ReplicaStateMachine controllerId=0] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:00,484] INFO [PartitionStateMachine controllerId=0] Initializing partition state (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:42:00,484] INFO [PartitionStateMachine controllerId=0] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:42:00,484] INFO [RequestSendThread controllerId=0] Controller 0 connected to localhost:33469 (id: 0 rack: null) for sending state change requests (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:00,484] INFO [Controller id=0] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,486] INFO [Controller id=0] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,486] INFO [Controller id=0] Partitions that completed preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,486] INFO [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,486] INFO [Controller id=0] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,486] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,487] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,520] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker localhost:33469 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:00,537] INFO [Controller id=0] Acquired new producerId block ProducerIdsBlock{brokerId=0, producerIdStart=0, producerIdLen=1000} by writing to Zk with path version 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:00,537] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:33469 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:01,920] INFO Creating topic connect-offset-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:01,927] INFO [Controller id=0] New topics: [Set(connect-offset-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-offset-topic-elasticsearch-it-connect-cluster,Some(OCYKqj2XQzKuz6hAJDaeEA),HashMap(connect-offset-topic-elasticsearch-it-connect-cluster-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:01,927] INFO [Controller id=0] New partition creation callback for connect-offset-topic-elasticsearch-it-connect-cluster-20,connect-offset-topic-elasticsearch-it-connect-cluster-7,connect-offset-topic-elasticsearch-it-connect-cluster-0,connect-offset-topic-elasticsearch-it-connect-cluster-12,connect-offset-topic-elasticsearch-it-connect-cluster-13,connect-offset-topic-elasticsearch-it-connect-cluster-5,connect-offset-topic-elasticsearch-it-connect-cluster-21,connect-offset-topic-elasticsearch-it-connect-cluster-14,connect-offset-topic-elasticsearch-it-connect-cluster-6,connect-offset-topic-elasticsearch-it-connect-cluster-1,connect-offset-topic-elasticsearch-it-connect-cluster-19,connect-offset-topic-elasticsearch-it-connect-cluster-17,connect-offset-topic-elasticsearch-it-connect-cluster-16,connect-offset-topic-elasticsearch-it-connect-cluster-23,connect-offset-topic-elasticsearch-it-connect-cluster-10,connect-offset-topic-elasticsearch-it-connect-cluster-3,connect-offset-topic-elasticsearch-it-connect-cluster-15,connect-offset-topic-elasticsearch-it-connect-cluster-22,connect-offset-topic-elasticsearch-it-connect-cluster-24,connect-offset-topic-elasticsearch-it-connect-cluster-18,connect-offset-topic-elasticsearch-it-connect-cluster-2,connect-offset-topic-elasticsearch-it-connect-cluster-4,connect-offset-topic-elasticsearch-it-connect-cluster-8,connect-offset-topic-elasticsearch-it-connect-cluster-9,connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:01,927] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,927] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,927] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,927] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,927] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:01,928] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:01,929] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:01,943] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,943] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,944] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,945] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,945] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,945] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,945] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,945] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,945] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,945] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:01,945] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 25 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:01,945] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:01,946] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:01,948] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:01,951] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-offset-topic-elasticsearch-it-connect-cluster-20, connect-offset-topic-elasticsearch-it-connect-cluster-7, connect-offset-topic-elasticsearch-it-connect-cluster-0, connect-offset-topic-elasticsearch-it-connect-cluster-12, connect-offset-topic-elasticsearch-it-connect-cluster-13, connect-offset-topic-elasticsearch-it-connect-cluster-5, connect-offset-topic-elasticsearch-it-connect-cluster-21, connect-offset-topic-elasticsearch-it-connect-cluster-14, connect-offset-topic-elasticsearch-it-connect-cluster-6, connect-offset-topic-elasticsearch-it-connect-cluster-1, connect-offset-topic-elasticsearch-it-connect-cluster-19, connect-offset-topic-elasticsearch-it-connect-cluster-17, connect-offset-topic-elasticsearch-it-connect-cluster-16, connect-offset-topic-elasticsearch-it-connect-cluster-23, connect-offset-topic-elasticsearch-it-connect-cluster-10, connect-offset-topic-elasticsearch-it-connect-cluster-3, connect-offset-topic-elasticsearch-it-connect-cluster-15, connect-offset-topic-elasticsearch-it-connect-cluster-22, connect-offset-topic-elasticsearch-it-connect-cluster-24, connect-offset-topic-elasticsearch-it-connect-cluster-18, connect-offset-topic-elasticsearch-it-connect-cluster-2, connect-offset-topic-elasticsearch-it-connect-cluster-4, connect-offset-topic-elasticsearch-it-connect-cluster-8, connect-offset-topic-elasticsearch-it-connect-cluster-9, connect-offset-topic-elasticsearch-it-connect-cluster-11) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:01,951] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 25 partitions (state.change.logger:66)
Oct 07, 2025 12:42:01 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Oct 07, 2025 12:42:01 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
[2025-10-07 12:42:01,954] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-10, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
Oct 07, 2025 12:42:01 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
[2025-10-07 12:42:01,955] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-10 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
Oct 07, 2025 12:42:01 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
[2025-10-07 12:42:01,956] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 (kafka.cluster.Partition:66)
[2025-10-07 12:42:01,956] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:01,956] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:01,967] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-14, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:01,967] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-14 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:01,968] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 (kafka.cluster.Partition:66)
[2025-10-07 12:42:01,968] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:01,968] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:01,979] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-18, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:01,981] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-18 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:01,981] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 (kafka.cluster.Partition:66)
[2025-10-07 12:42:01,981] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:01,981] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:01,989] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:01,989] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:01,989] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 12:42:01,989] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:01,990] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 12:42:02 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

[2025-10-07 12:42:02,000] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-22, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,001] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-22 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,001] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,001] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,001] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,010] INFO Creating topic test with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:02,013] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-7, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,013] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-7 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,013] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,013] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,013] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,016] INFO [Controller id=0] New topics: [Set(test)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(test,Some(h3x5LNvlR3SNPsWpvSIyiw),Map(test-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:02,016] INFO [Controller id=0] New partition creation callback for test-0 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:02,016] INFO [Controller id=0 epoch=1] Changed partition test-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,016] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:02,016] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:02,019] INFO [Controller id=0 epoch=1] Changed partition test-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,020] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:02,020] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:02,020] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:02,024] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-11, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,024] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-11 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,024] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,024] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,024] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,035] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-15, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,035] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-15 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,035] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,035] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,035] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,046] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,046] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,047] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,047] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,047] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,057] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-19, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,057] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-19 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,057] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,057] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,057] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,068] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,068] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,069] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,069] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,069] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,079] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-23, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,079] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-23 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,079] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,079] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,079] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,089] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-8, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,090] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-8 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,090] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,090] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,090] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,101] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-12, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,101] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-12 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,101] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,101] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,101] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,112] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-16, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,112] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-16 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,112] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,112] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,112] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,122] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,123] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,123] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,123] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,123] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,134] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-20, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,134] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-20 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,134] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,134] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,134] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,145] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-5, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,145] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-5 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,145] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,145] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,145] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,156] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-24, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,156] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-24 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,156] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,157] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,157] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,167] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-9, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,167] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-9 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,168] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,168] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,168] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,179] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-13, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,179] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-13 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,179] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,179] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,179] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,191] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-17, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,191] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-17 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,191] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,191] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,191] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,200] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,201] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,201] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,201] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,201] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,211] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-21, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,211] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-21 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,211] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,211] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,211] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,222] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-6, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,222] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-offset-topic-elasticsearch-it-connect-cluster-6 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,222] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,222] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,222] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,232] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:02,233] INFO [Broker id=0] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger:66)
[2025-10-07 12:42:02,236] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:02,237] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:02,237] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:02,240] INFO [LogLoader partition=test-0, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,240] INFO Created log for partition test-0 in /tmp/EmbeddedKafkaCluster5349058205751246927/test-0 with properties {} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,240] INFO [Partition test-0 broker=0] No checkpointed highwatermark is found for partition test-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,240] INFO [Partition test-0 broker=0] Log loaded for partition test-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,240] INFO [Broker id=0] Leader test-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,249] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:02,250] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger:66)
[2025-10-07 12:42:02,265] INFO Creating topic connect-storage-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:02,268] INFO [Controller id=0] New topics: [Set(connect-storage-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-storage-topic-elasticsearch-it-connect-cluster,Some(7Fak4cv_TuKtv3GPCTfJTw),HashMap(connect-storage-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:02,269] INFO [Controller id=0] New partition creation callback for connect-storage-topic-elasticsearch-it-connect-cluster-2,connect-storage-topic-elasticsearch-it-connect-cluster-0,connect-storage-topic-elasticsearch-it-connect-cluster-3,connect-storage-topic-elasticsearch-it-connect-cluster-4,connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:02,269] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,269] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,269] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,269] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,269] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,269] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:02,269] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:02,278] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,278] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,278] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,278] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,278] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,278] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:02,278] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:02,279] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:02,279] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:02,280] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-storage-topic-elasticsearch-it-connect-cluster-2, connect-storage-topic-elasticsearch-it-connect-cluster-0, connect-storage-topic-elasticsearch-it-connect-cluster-3, connect-storage-topic-elasticsearch-it-connect-cluster-4, connect-storage-topic-elasticsearch-it-connect-cluster-1) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:02,280] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:02,282] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,283] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-storage-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,283] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,283] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,283] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,304] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,304] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-storage-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,304] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,304] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,304] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,314] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,315] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-storage-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,315] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,315] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,315] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,326] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,326] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-storage-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,326] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,326] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,326] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,336] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,337] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-storage-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,337] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,337] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,337] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,346] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:02,347] INFO [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6 (state.change.logger:66)
[2025-10-07 12:42:02,363] INFO Creating topic connect-config-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:02,366] INFO [Controller id=0] New topics: [Set(connect-config-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-config-topic-elasticsearch-it-connect-cluster,Some(lI7WOoovQPubxUSDAyrZ7Q),Map(connect-config-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:02,366] INFO [Controller id=0] New partition creation callback for connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:02,367] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,367] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:02,367] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:02,369] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,369] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:02,369] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:02,370] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:02,370] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:02,370] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(connect-config-topic-elasticsearch-it-connect-cluster-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:02,370] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:02,372] INFO [LogLoader partition=connect-config-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,372] INFO Created log for partition connect-config-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster5349058205751246927/connect-config-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,372] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,372] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-config-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,373] INFO [Broker id=0] Leader connect-config-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,381] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:02,382] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger:66)
[2025-10-07 12:42:02,411] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:02,417] INFO [Controller id=0] New topics: [HashSet(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(tpjkMPFhQR2OiqW2FD4beg),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:02,417] INFO [Controller id=0] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,418] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:02,419] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:02,420] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,443] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 50 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:02,444] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:02,445] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:02,446] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:02,451] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:02,451] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 0 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:02,453] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,453] INFO Created log for partition __consumer_offsets-3 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,454] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,454] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,454] INFO [Broker id=0] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,464] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,464] INFO Created log for partition __consumer_offsets-18 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,464] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,464] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,464] INFO [Broker id=0] Leader __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,474] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,475] INFO Created log for partition __consumer_offsets-41 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,475] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,475] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,475] INFO [Broker id=0] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,485] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,485] INFO Created log for partition __consumer_offsets-10 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,485] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,485] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,485] INFO [Broker id=0] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,496] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,496] INFO Created log for partition __consumer_offsets-33 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,496] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,496] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,496] INFO [Broker id=0] Leader __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,506] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,506] INFO Created log for partition __consumer_offsets-48 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,507] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,507] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,507] INFO [Broker id=0] Leader __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,517] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,518] INFO Created log for partition __consumer_offsets-19 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,518] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,518] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,518] INFO [Broker id=0] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,528] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,528] INFO Created log for partition __consumer_offsets-34 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,528] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,528] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,528] INFO [Broker id=0] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,538] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,538] INFO Created log for partition __consumer_offsets-4 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,538] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,539] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,539] INFO [Broker id=0] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,549] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,549] INFO Created log for partition __consumer_offsets-11 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,549] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,549] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,549] INFO [Broker id=0] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,560] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,561] INFO Created log for partition __consumer_offsets-26 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,561] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,561] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,561] INFO [Broker id=0] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,571] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,571] INFO Created log for partition __consumer_offsets-49 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,571] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,571] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,571] INFO [Broker id=0] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,582] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,582] INFO Created log for partition __consumer_offsets-39 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,582] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,582] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,582] INFO [Broker id=0] Leader __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,593] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,594] INFO Created log for partition __consumer_offsets-9 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,594] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,594] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,594] INFO [Broker id=0] Leader __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,604] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,604] INFO Created log for partition __consumer_offsets-24 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,604] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,604] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,604] INFO [Broker id=0] Leader __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,615] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,615] INFO Created log for partition __consumer_offsets-31 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,616] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,616] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,616] INFO [Broker id=0] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,627] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,627] INFO Created log for partition __consumer_offsets-46 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,627] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,627] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,627] INFO [Broker id=0] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,638] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,638] INFO Created log for partition __consumer_offsets-1 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,638] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,638] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,638] INFO [Broker id=0] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,649] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,649] INFO Created log for partition __consumer_offsets-16 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,649] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,649] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,649] INFO [Broker id=0] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,659] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,659] INFO Created log for partition __consumer_offsets-2 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,660] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,660] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,660] INFO [Broker id=0] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,670] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,671] INFO Created log for partition __consumer_offsets-25 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,671] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,671] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,671] INFO [Broker id=0] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,681] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,681] INFO Created log for partition __consumer_offsets-40 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,681] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,681] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,681] INFO [Broker id=0] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,691] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,691] INFO Created log for partition __consumer_offsets-47 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,691] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,691] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,691] INFO [Broker id=0] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,701] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,702] INFO Created log for partition __consumer_offsets-17 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,702] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,702] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,702] INFO [Broker id=0] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,712] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,712] INFO Created log for partition __consumer_offsets-32 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,712] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,712] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,712] INFO [Broker id=0] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,723] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,723] INFO Created log for partition __consumer_offsets-37 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,724] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,724] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,724] INFO [Broker id=0] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,736] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,736] INFO Created log for partition __consumer_offsets-7 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,736] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,736] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,736] INFO [Broker id=0] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,747] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,747] INFO Created log for partition __consumer_offsets-22 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,747] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,747] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,747] INFO [Broker id=0] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,758] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,758] INFO Created log for partition __consumer_offsets-29 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,758] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,758] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,758] INFO [Broker id=0] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,768] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,768] INFO Created log for partition __consumer_offsets-44 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,768] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,768] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,768] INFO [Broker id=0] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,777] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,777] INFO Created log for partition __consumer_offsets-14 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,777] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,777] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,777] INFO [Broker id=0] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,788] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,788] INFO Created log for partition __consumer_offsets-23 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,788] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,788] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,788] INFO [Broker id=0] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,798] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,798] INFO Created log for partition __consumer_offsets-38 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,798] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,799] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,799] INFO [Broker id=0] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,809] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,809] INFO Created log for partition __consumer_offsets-8 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,809] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,809] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,809] INFO [Broker id=0] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,819] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,820] INFO Created log for partition __consumer_offsets-45 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,820] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,820] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,820] INFO [Broker id=0] Leader __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,830] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,830] INFO Created log for partition __consumer_offsets-15 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,830] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,830] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,830] INFO [Broker id=0] Leader __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,841] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,841] INFO Created log for partition __consumer_offsets-30 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,841] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,841] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,841] INFO [Broker id=0] Leader __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,851] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,851] INFO Created log for partition __consumer_offsets-0 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,851] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,851] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,852] INFO [Broker id=0] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,862] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,863] INFO Created log for partition __consumer_offsets-35 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,863] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,863] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,863] INFO [Broker id=0] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,874] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,874] INFO Created log for partition __consumer_offsets-5 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,874] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,874] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,874] INFO [Broker id=0] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,884] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,885] INFO Created log for partition __consumer_offsets-20 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,885] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,885] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,885] INFO [Broker id=0] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,895] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,895] INFO Created log for partition __consumer_offsets-27 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,895] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,895] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,895] INFO [Broker id=0] Leader __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,906] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,906] INFO Created log for partition __consumer_offsets-42 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,906] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,907] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,907] INFO [Broker id=0] Leader __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,917] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,917] INFO Created log for partition __consumer_offsets-12 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,917] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,917] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,917] INFO [Broker id=0] Leader __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,927] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,928] INFO Created log for partition __consumer_offsets-21 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,928] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,928] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,928] INFO [Broker id=0] Leader __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,939] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,940] INFO Created log for partition __consumer_offsets-36 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,940] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,940] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,940] INFO [Broker id=0] Leader __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,950] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,951] INFO Created log for partition __consumer_offsets-6 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,951] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,951] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,951] INFO [Broker id=0] Leader __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,961] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,961] INFO Created log for partition __consumer_offsets-43 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,961] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,961] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,961] INFO [Broker id=0] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,972] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,972] INFO Created log for partition __consumer_offsets-13 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,972] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,972] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,972] INFO [Broker id=0] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,982] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/EmbeddedKafkaCluster5349058205751246927] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:02,983] INFO Created log for partition __consumer_offsets-28 in /tmp/EmbeddedKafkaCluster5349058205751246927/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:02,983] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,983] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:02,983] INFO [Broker id=0] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:02,992] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,992] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,993] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,993] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,993] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,993] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,993] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,993] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,994] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,994] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,994] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,994] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,994] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,994] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,994] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,995] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,995] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,995] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,995] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,995] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,995] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,995] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,995] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,995] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,995] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,995] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,995] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,995] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,995] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,995] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,995] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,995] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,995] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,995] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,995] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,996] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,996] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,996] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,996] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,996] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,996] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,996] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,996] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,996] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,996] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,996] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,996] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,996] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,996] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,996] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,996] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,996] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,996] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,996] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,997] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,997] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,997] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,997] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,997] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,997] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,997] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,997] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,997] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,997] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,997] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,997] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,997] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,997] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,997] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,997] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,997] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,997] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,997] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,997] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,997] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,998] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,999] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,999] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,999] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,999] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,999] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,999] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,999] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,999] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,999] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,999] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,999] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,999] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,999] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,999] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,999] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,999] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,999] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,999] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,999] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,999] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,999] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:02,999] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,999] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:02,999] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,000] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,000] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,000] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,000] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,000] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,000] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,000] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,000] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,000] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,000] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,000] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,000] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,000] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,000] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,000] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:03,000] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,000] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,000] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,000] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,001] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,001] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,001] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,001] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,001] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,001] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:03,002] INFO [Broker id=0] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 10 (state.change.logger:66)
[2025-10-07 12:42:03,018] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-integration-test-elasticsearch-it-connect-cluster in Empty state. Created a new member id connect-2-3b5a8fc9-d173-4aa9-8945-f83295642cce and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,021] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member connect-2-3b5a8fc9-d173-4aa9-8945-f83295642cce with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,022] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 1 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,030] INFO [GroupCoordinator 0]: Assignment received from leader connect-2-3b5a8fc9-d173-4aa9-8945-f83295642cce for group connect-integration-test-elasticsearch-it-connect-cluster for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,044] INFO ElasticsearchSinkConnectorConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:46493/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 12:42:03,045] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:42:03,045] INFO Using unsecured connection to [http://localhost:46493]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:42:03,071] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 1 (__consumer_offsets-4) (reason: Leader connect-2-3b5a8fc9-d173-4aa9-8945-f83295642cce re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,072] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 2 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,075] INFO [GroupCoordinator 0]: Assignment received from leader connect-2-3b5a8fc9-d173-4aa9-8945-f83295642cce for group connect-integration-test-elasticsearch-it-connect-cluster for generation 2. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,083] INFO [es-connector|worker] ElasticsearchSinkConnectorConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:46493/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 12:42:03,101] ERROR Could not check connector state info. (io.confluent.connect.elasticsearch.integration.BaseConnectorIT:92)
[2025-10-07 12:42:03,105] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 2 (__consumer_offsets-4) (reason: Leader connect-2-3b5a8fc9-d173-4aa9-8945-f83295642cce re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,105] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 3 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,110] INFO [GroupCoordinator 0]: Assignment received from leader connect-2-3b5a8fc9-d173-4aa9-8945-f83295642cce for group connect-integration-test-elasticsearch-it-connect-cluster for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,117] INFO [es-connector|task-0] Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:42:03,117] INFO [es-connector|task-0] ElasticsearchSinkTaskConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:46493/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 0
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:42:03,118] INFO [es-connector|task-0] Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:42:03,118] DEBUG [es-connector|task-0] Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:42:03,118] INFO [es-connector|task-0] Using unsecured connection to [http://localhost:46493]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:42:03,127] INFO [es-connector|task-0] Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:42:03,136] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-es-connector in Empty state. Created a new member id connector-consumer-es-connector-0-388c0377-9239-4684-b1fc-8dc4dee8e34e and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,138] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member connector-consumer-es-connector-0-388c0377-9239-4684-b1fc-8dc4dee8e34e with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,140] INFO [GroupCoordinator 0]: Stabilized group connect-es-connector generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,152] INFO [GroupCoordinator 0]: Assignment received from leader connector-consumer-es-connector-0-388c0377-9239-4684-b1fc-8dc4dee8e34e for group connect-es-connector for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,220] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:03,220] INFO [es-connector|task-0] Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:42:03,223] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:03,224] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:03,230] WARN [es-connector|task-0] Failed to execute bulk request due to org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:46493], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
{
  "name" : "KafkaESClusterNodeold_1",
  "cluster_name" : "KafkaESCluster",
  "cluster_uuid" : "83EJmDNrRVirBWcZDgs9ew",
  "tagline" : "You Know, for Search",
  "version" : {
    "number" : "7.16.3",
    "build_hash" : "83EJmDNrRVirBWcZDgs9ew",
    "build_date" : "2018-04-12T16:25:14.838Z",
    "build_snapshot" : "false",
    "lucene_version" : "6.6.1",
    "minimum_wire_compatibility_version" : "1.1.1",
    "minimum_index_compatibility_version" : "2.2.2"
  }
}. Retrying attempt (1/3) after backoff of 8 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:42:03,341] DEBUG [es-connector|task-0] preCommitting offsets {test-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:42:03,344] DEBUG [es-connector|task-0] Stopping Elasticsearch client. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:146)
[2025-10-07 12:42:03,350] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 1 (__consumer_offsets-23) (reason: Removing member connector-consumer-es-connector-0-388c0377-9239-4684-b1fc-8dc4dee8e34e on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,351] INFO [GroupCoordinator 0]: Group connect-es-connector with generation 2 is now empty (__consumer_offsets-23) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,352] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connector-consumer-es-connector-0-388c0377-9239-4684-b1fc-8dc4dee8e34e, groupInstanceId=None, clientId=connector-consumer-es-connector-0, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group connect-es-connector through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,357] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 3 (__consumer_offsets-4) (reason: Removing member connect-2-3b5a8fc9-d173-4aa9-8945-f83295642cce on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,357] INFO [GroupCoordinator 0]: Group connect-integration-test-elasticsearch-it-connect-cluster with generation 4 is now empty (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,358] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connect-2-3b5a8fc9-d173-4aa9-8945-f83295642cce, groupInstanceId=None, clientId=connect-2, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(sessioned, compatible, default)) has left group connect-integration-test-elasticsearch-it-connect-cluster through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,368] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer:66)
[2025-10-07 12:42:03,369] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer:66)
[2025-10-07 12:42:03,370] INFO [Controller id=0] Shutting down broker 0 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:03,370] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:03,371] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer:66)
[2025-10-07 12:42:03,371] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:03,371] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:03,371] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:03,371] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:03,375] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:03,376] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 12:42:03,379] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 12:42:03,381] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:03,468] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:03,468] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:03,469] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis:66)
[2025-10-07 12:42:03,469] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:03,669] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:03,669] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:03,669] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:03,669] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager:66)
[2025-10-07 12:42:03,670] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:03,670] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:03,670] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:03,672] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:03,672] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,672] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:03,864] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:03,864] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:03,864] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:03,940] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:03,940] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:03,940] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:03,940] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager:66)
[2025-10-07 12:42:03,941] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:03,941] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:03,941] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:03,941] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:03,941] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:03,941] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 12:42:03,941] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 12:42:03,941] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:04,059] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:04,059] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:04,059] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:04,064] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:04,064] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:04,065] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:04,264] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:04,264] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:04,265] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:04,464] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:04,464] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:04,471] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager:66)
[2025-10-07 12:42:04,471] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:04,472] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:04,472] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:04,474] INFO Broker to controller channel manager for alterIsr shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 12:42:04,475] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:04,475] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:04,475] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:04,477] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 12:42:04,477] INFO Shutting down. (kafka.log.LogManager:66)
[2025-10-07 12:42:04,477] INFO Shutting down the log cleaner. (kafka.log.LogCleaner:66)
[2025-10-07 12:42:04,477] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner:66)
[2025-10-07 12:42:04,477] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner:66)
[2025-10-07 12:42:04,477] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner:66)
[2025-10-07 12:42:04,484] INFO [ProducerStateManager partition=test-0] Wrote producer snapshot at offset 4 with 1 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:04,508] INFO [ProducerStateManager partition=__consumer_offsets-23] Wrote producer snapshot at offset 3 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:04,515] INFO [ProducerStateManager partition=connect-config-topic-elasticsearch-it-connect-cluster-0] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:04,522] INFO [ProducerStateManager partition=__consumer_offsets-4] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:04,530] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-2] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:04,539] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-3] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:04,552] INFO Shutdown complete. (kafka.log.LogManager:66)
[2025-10-07 12:42:04,552] INFO [ControllerEventThread controllerId=0] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:04,553] INFO [ControllerEventThread controllerId=0] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:04,553] INFO [ControllerEventThread controllerId=0] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:04,554] INFO [PartitionStateMachine controllerId=0] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:42:04,555] INFO [ReplicaStateMachine controllerId=0] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:04,555] INFO [RequestSendThread controllerId=0] Shutting down (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:04,555] INFO [RequestSendThread controllerId=0] Stopped (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:04,555] INFO [RequestSendThread controllerId=0] Shutdown completed (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:04,556] INFO [Controller id=0] Resigned (kafka.controller.KafkaController:66)
[2025-10-07 12:42:04,557] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:04,557] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:04,557] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:04,557] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:04,660] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:04,661] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:05,401] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:05,401] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:05,401] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:06,402] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:06,402] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:06,402] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:06,403] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:06,403] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:06,403] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:06,404] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:06,404] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:06,404] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer:66)
[2025-10-07 12:42:06,425] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer:66)
[2025-10-07 12:42:06,426] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats:66)
[2025-10-07 12:42:06,426] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer:66)
[2025-10-07 12:42:06,459] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster2253284502884208513
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:40605
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:42:06,461] INFO starting (kafka.server.KafkaServer:66)
[2025-10-07 12:42:06,461] INFO Connecting to zookeeper on 127.0.0.1:40605 (kafka.server.KafkaServer:66)
[2025-10-07 12:42:06,461] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:40605. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:06,462] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:06,466] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:06,483] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:06,487] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 12:42:06,487] INFO Cleared cache (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 12:42:06,490] INFO Cluster ID = 77OfwNVmS4CMW46XB5ptag (kafka.server.KafkaServer:66)
[2025-10-07 12:42:06,491] WARN No meta.properties file under dir /tmp/EmbeddedKafkaCluster2253284502884208513/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2025-10-07 12:42:06,493] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster2253284502884208513
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:40605
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:42:06,495] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster2253284502884208513
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:40605
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:42:06,502] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:06,503] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:06,504] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:06,504] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:06,506] INFO Loading logs from log dirs ArraySeq(/tmp/EmbeddedKafkaCluster2253284502884208513) (kafka.log.LogManager:66)
[2025-10-07 12:42:06,506] INFO Attempting recovery for all logs in /tmp/EmbeddedKafkaCluster2253284502884208513 since no clean shutdown file was found (kafka.log.LogManager:66)
[2025-10-07 12:42:06,506] INFO Loaded 0 logs in 0ms. (kafka.log.LogManager:66)
[2025-10-07 12:42:06,506] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:66)
[2025-10-07 12:42:06,506] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:66)
[2025-10-07 12:42:06,507] INFO Starting the log cleaner (kafka.log.LogCleaner:66)
[2025-10-07 12:42:06,517] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner:66)
[2025-10-07 12:42:06,519] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:06,526] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas:66)
[2025-10-07 12:42:06,527] INFO Awaiting socket connections on localhost:39655. (kafka.network.Acceptor:66)
[2025-10-07 12:42:06,529] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 12:42:06,530] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:06,531] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:06,532] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:06,533] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:06,536] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:06,538] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:06,539] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:06,540] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1759840926539,1759840926539,1,0,0,72057675526373376,204,0,25
 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:06,540] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:39655, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:06,550] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:06,552] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:06,553] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:06,554] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:06,555] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:06,556] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:06,556] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:06,556] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:06,557] INFO [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,557] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:06,558] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:06,558] INFO [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{}) (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,559] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:06,560] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 12:42:06,561] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:06,562] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 12:42:06,562] INFO [Controller id=0] Registering handlers (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,562] INFO [Controller id=0] Deleting log dir event notifications (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,563] INFO [Controller id=0] Deleting isr change notifications (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,563] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:06,563] INFO [Controller id=0] Initializing controller context (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,564] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 12:42:06,564] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:06,564] INFO [KafkaServer id=0] started (kafka.server.KafkaServer:66)
[2025-10-07 12:42:06,565] INFO [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25) (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,568] INFO [Controller id=0] Currently active brokers in the cluster: Set(0) (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,569] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:06,569] INFO [Controller id=0] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,569] INFO [Controller id=0] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,569] INFO [Controller id=0] Fetching topic deletions in progress (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,570] INFO [Controller id=0] List of topics to be deleted:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,570] INFO [Controller id=0] List of topics ineligible for deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,570] INFO [Controller id=0] Initializing topic deletion manager (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,570] INFO [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager:66)
[2025-10-07 12:42:06,571] INFO [Controller id=0] Sending update metadata request (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,571] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:06,571] INFO [ReplicaStateMachine controllerId=0] Initializing replica state (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:06,571] INFO [ReplicaStateMachine controllerId=0] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:06,571] INFO [ReplicaStateMachine controllerId=0] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:06,571] INFO [PartitionStateMachine controllerId=0] Initializing partition state (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:42:06,571] INFO [RequestSendThread controllerId=0] Controller 0 connected to localhost:39655 (id: 0 rack: null) for sending state change requests (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:06,571] INFO [PartitionStateMachine controllerId=0] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:42:06,572] INFO [Controller id=0] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,572] INFO [Controller id=0] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,573] INFO [Controller id=0] Partitions that completed preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,573] INFO [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,573] INFO [Controller id=0] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,573] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,574] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,619] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker localhost:39655 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:06,629] INFO [Controller id=0] Acquired new producerId block ProducerIdsBlock{brokerId=0, producerIdStart=0, producerIdLen=1000} by writing to Zk with path version 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:06,633] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:39655 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:08,529] INFO Creating topic connect-offset-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:08,534] INFO [Controller id=0] New topics: [Set(connect-offset-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-offset-topic-elasticsearch-it-connect-cluster,Some(h6Ozy5DCSTOGnYRH7UIFtA),HashMap(connect-offset-topic-elasticsearch-it-connect-cluster-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:08,534] INFO [Controller id=0] New partition creation callback for connect-offset-topic-elasticsearch-it-connect-cluster-20,connect-offset-topic-elasticsearch-it-connect-cluster-7,connect-offset-topic-elasticsearch-it-connect-cluster-0,connect-offset-topic-elasticsearch-it-connect-cluster-12,connect-offset-topic-elasticsearch-it-connect-cluster-13,connect-offset-topic-elasticsearch-it-connect-cluster-5,connect-offset-topic-elasticsearch-it-connect-cluster-21,connect-offset-topic-elasticsearch-it-connect-cluster-14,connect-offset-topic-elasticsearch-it-connect-cluster-6,connect-offset-topic-elasticsearch-it-connect-cluster-1,connect-offset-topic-elasticsearch-it-connect-cluster-19,connect-offset-topic-elasticsearch-it-connect-cluster-17,connect-offset-topic-elasticsearch-it-connect-cluster-16,connect-offset-topic-elasticsearch-it-connect-cluster-23,connect-offset-topic-elasticsearch-it-connect-cluster-10,connect-offset-topic-elasticsearch-it-connect-cluster-3,connect-offset-topic-elasticsearch-it-connect-cluster-15,connect-offset-topic-elasticsearch-it-connect-cluster-22,connect-offset-topic-elasticsearch-it-connect-cluster-24,connect-offset-topic-elasticsearch-it-connect-cluster-18,connect-offset-topic-elasticsearch-it-connect-cluster-2,connect-offset-topic-elasticsearch-it-connect-cluster-4,connect-offset-topic-elasticsearch-it-connect-cluster-8,connect-offset-topic-elasticsearch-it-connect-cluster-9,connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,535] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,536] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:08,536] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,550] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,551] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,551] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,551] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,551] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,551] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,551] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 25 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:08,551] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:08,552] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:08,552] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:08,555] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-offset-topic-elasticsearch-it-connect-cluster-20, connect-offset-topic-elasticsearch-it-connect-cluster-7, connect-offset-topic-elasticsearch-it-connect-cluster-0, connect-offset-topic-elasticsearch-it-connect-cluster-12, connect-offset-topic-elasticsearch-it-connect-cluster-13, connect-offset-topic-elasticsearch-it-connect-cluster-5, connect-offset-topic-elasticsearch-it-connect-cluster-21, connect-offset-topic-elasticsearch-it-connect-cluster-14, connect-offset-topic-elasticsearch-it-connect-cluster-6, connect-offset-topic-elasticsearch-it-connect-cluster-1, connect-offset-topic-elasticsearch-it-connect-cluster-19, connect-offset-topic-elasticsearch-it-connect-cluster-17, connect-offset-topic-elasticsearch-it-connect-cluster-16, connect-offset-topic-elasticsearch-it-connect-cluster-23, connect-offset-topic-elasticsearch-it-connect-cluster-10, connect-offset-topic-elasticsearch-it-connect-cluster-3, connect-offset-topic-elasticsearch-it-connect-cluster-15, connect-offset-topic-elasticsearch-it-connect-cluster-22, connect-offset-topic-elasticsearch-it-connect-cluster-24, connect-offset-topic-elasticsearch-it-connect-cluster-18, connect-offset-topic-elasticsearch-it-connect-cluster-2, connect-offset-topic-elasticsearch-it-connect-cluster-4, connect-offset-topic-elasticsearch-it-connect-cluster-8, connect-offset-topic-elasticsearch-it-connect-cluster-9, connect-offset-topic-elasticsearch-it-connect-cluster-11) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:08,555] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:08,557] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-10, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,559] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-10 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,559] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,559] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,559] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 12:42:08 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Oct 07, 2025 12:42:08 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Oct 07, 2025 12:42:08 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Oct 07, 2025 12:42:08 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
[2025-10-07 12:42:08,571] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-14, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,571] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-14 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,571] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,571] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,572] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,582] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-18, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,582] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-18 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,582] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,582] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,582] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,593] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,593] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,593] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,593] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,593] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 12:42:08 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

[2025-10-07 12:42:08,604] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-22, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,604] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-22 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,605] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,605] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,605] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,607] INFO Creating topic test with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:08,612] INFO [Controller id=0] New topics: [Set(test)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(test,Some(C43VDRSaTcCXUlNMvgbp9w),Map(test-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:08,612] INFO [Controller id=0] New partition creation callback for test-0 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:08,612] INFO [Controller id=0 epoch=1] Changed partition test-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,613] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:08,613] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:08,615] INFO [Controller id=0 epoch=1] Changed partition test-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,615] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-7, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,615] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:08,615] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:08,615] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-7 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,616] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,616] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,616] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,616] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:08,626] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-11, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,627] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-11 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,627] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,627] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,627] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,637] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-15, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,637] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-15 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,637] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,638] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,638] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,648] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,648] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,648] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,648] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,648] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,659] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-19, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,659] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-19 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,660] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,660] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,660] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,671] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,671] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,671] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,671] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,671] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,682] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-23, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,683] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-23 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,683] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,683] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,683] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,693] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-8, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,693] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-8 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,694] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,694] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,694] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,704] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-12, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,704] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-12 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,704] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,705] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,705] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,715] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-16, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,715] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-16 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,715] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,715] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,715] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,726] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,727] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,727] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,727] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,727] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,737] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-20, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,738] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-20 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,738] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,738] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,738] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,748] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-5, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,748] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-5 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,748] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,748] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,748] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,758] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-24, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,759] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-24 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,759] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,759] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,759] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,769] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-9, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,769] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-9 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,769] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,770] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,770] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,779] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-13, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,780] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-13 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,780] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,780] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,780] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,790] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-17, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,790] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-17 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,790] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,790] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,790] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,801] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,801] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,801] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,801] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,801] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,811] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-21, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,811] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-21 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,812] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,812] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,812] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,822] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-6, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,823] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-offset-topic-elasticsearch-it-connect-cluster-6 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,823] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,823] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,823] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,832] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:08,833] INFO [Broker id=0] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger:66)
[2025-10-07 12:42:08,834] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:08,836] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:08,836] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:08,840] INFO [LogLoader partition=test-0, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,840] INFO Created log for partition test-0 in /tmp/EmbeddedKafkaCluster2253284502884208513/test-0 with properties {} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,841] INFO [Partition test-0 broker=0] No checkpointed highwatermark is found for partition test-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,841] INFO [Partition test-0 broker=0] Log loaded for partition test-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,841] INFO [Broker id=0] Leader test-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,851] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:08,857] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger:66)
[2025-10-07 12:42:08,861] INFO Creating topic connect-storage-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:08,864] INFO [Controller id=0] New topics: [Set(connect-storage-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-storage-topic-elasticsearch-it-connect-cluster,Some(-J72PU5IStijxMFrWHhvEw),HashMap(connect-storage-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:08,864] INFO [Controller id=0] New partition creation callback for connect-storage-topic-elasticsearch-it-connect-cluster-2,connect-storage-topic-elasticsearch-it-connect-cluster-0,connect-storage-topic-elasticsearch-it-connect-cluster-3,connect-storage-topic-elasticsearch-it-connect-cluster-4,connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:08,865] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,865] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,865] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,865] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,865] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,865] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:08,865] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:08,880] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,880] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,880] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,880] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,882] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,883] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:08,883] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:08,889] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:08,889] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:08,890] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-storage-topic-elasticsearch-it-connect-cluster-2, connect-storage-topic-elasticsearch-it-connect-cluster-0, connect-storage-topic-elasticsearch-it-connect-cluster-3, connect-storage-topic-elasticsearch-it-connect-cluster-4, connect-storage-topic-elasticsearch-it-connect-cluster-1) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:08,890] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:08,893] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,893] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-storage-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,894] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,894] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,894] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,905] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,906] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-storage-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,906] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,906] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,906] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,917] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,917] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-storage-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,917] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,917] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,917] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,927] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,928] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-storage-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,928] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,928] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,928] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,938] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,938] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-storage-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,938] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,938] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,938] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,948] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:08,949] INFO [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6 (state.change.logger:66)
[2025-10-07 12:42:08,965] INFO Creating topic connect-config-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:08,968] INFO [Controller id=0] New topics: [Set(connect-config-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-config-topic-elasticsearch-it-connect-cluster,Some(Ji3VwRBCToWrw0UxkF1WPQ),Map(connect-config-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:08,968] INFO [Controller id=0] New partition creation callback for connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:08,968] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:08,968] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:08,969] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:08,971] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:08,971] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:08,971] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:08,971] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:08,971] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:08,972] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(connect-config-topic-elasticsearch-it-connect-cluster-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:08,972] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:08,973] INFO [LogLoader partition=connect-config-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:08,974] INFO Created log for partition connect-config-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster2253284502884208513/connect-config-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:08,974] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,974] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-config-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:08,974] INFO [Broker id=0] Leader connect-config-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:08,983] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:08,984] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger:66)
[2025-10-07 12:42:09,012] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:09,016] INFO [Controller id=0] New topics: [HashSet(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(pyrXwC1-S5qsW7lzx4WGbA),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:09,016] INFO [Controller id=0] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,017] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:09,018] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:09,019] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,033] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,034] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,035] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:09,035] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 50 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:09,035] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:09,035] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:09,035] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:09,041] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:09,041] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 0 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:09,044] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,044] INFO Created log for partition __consumer_offsets-3 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,045] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,045] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,045] INFO [Broker id=0] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,055] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,056] INFO Created log for partition __consumer_offsets-18 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,056] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,056] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,056] INFO [Broker id=0] Leader __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,066] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,066] INFO Created log for partition __consumer_offsets-41 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,066] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,066] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,067] INFO [Broker id=0] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,077] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,077] INFO Created log for partition __consumer_offsets-10 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,077] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,077] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,078] INFO [Broker id=0] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,089] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,089] INFO Created log for partition __consumer_offsets-33 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,089] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,089] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,089] INFO [Broker id=0] Leader __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,099] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,099] INFO Created log for partition __consumer_offsets-48 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,099] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,099] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,099] INFO [Broker id=0] Leader __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,110] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,110] INFO Created log for partition __consumer_offsets-19 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,110] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,110] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,110] INFO [Broker id=0] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,121] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,121] INFO Created log for partition __consumer_offsets-34 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,121] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,121] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,121] INFO [Broker id=0] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,132] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,132] INFO Created log for partition __consumer_offsets-4 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,132] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,132] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,133] INFO [Broker id=0] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,144] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,144] INFO Created log for partition __consumer_offsets-11 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,144] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,144] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,144] INFO [Broker id=0] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,155] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,155] INFO Created log for partition __consumer_offsets-26 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,155] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,155] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,155] INFO [Broker id=0] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,167] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,167] INFO Created log for partition __consumer_offsets-49 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,167] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,167] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,167] INFO [Broker id=0] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,178] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,178] INFO Created log for partition __consumer_offsets-39 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,179] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,179] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,179] INFO [Broker id=0] Leader __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,190] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,190] INFO Created log for partition __consumer_offsets-9 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,190] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,190] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,190] INFO [Broker id=0] Leader __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,201] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,201] INFO Created log for partition __consumer_offsets-24 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,201] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,201] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,201] INFO [Broker id=0] Leader __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,212] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,212] INFO Created log for partition __consumer_offsets-31 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,212] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,212] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,212] INFO [Broker id=0] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,223] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,224] INFO Created log for partition __consumer_offsets-46 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,224] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,224] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,224] INFO [Broker id=0] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,234] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,234] INFO Created log for partition __consumer_offsets-1 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,234] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,234] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,235] INFO [Broker id=0] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,245] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,245] INFO Created log for partition __consumer_offsets-16 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,246] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,246] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,246] INFO [Broker id=0] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,256] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,257] INFO Created log for partition __consumer_offsets-2 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,257] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,257] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,257] INFO [Broker id=0] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,267] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,267] INFO Created log for partition __consumer_offsets-25 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,268] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,268] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,268] INFO [Broker id=0] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,279] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,279] INFO Created log for partition __consumer_offsets-40 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,279] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,279] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,279] INFO [Broker id=0] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,289] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,289] INFO Created log for partition __consumer_offsets-47 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,289] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,289] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,289] INFO [Broker id=0] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,300] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,300] INFO Created log for partition __consumer_offsets-17 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,300] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,301] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,301] INFO [Broker id=0] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,311] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,311] INFO Created log for partition __consumer_offsets-32 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,312] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,312] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,312] INFO [Broker id=0] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,322] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,322] INFO Created log for partition __consumer_offsets-37 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,322] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,322] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,323] INFO [Broker id=0] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,334] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,334] INFO Created log for partition __consumer_offsets-7 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,334] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,334] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,334] INFO [Broker id=0] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,345] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,345] INFO Created log for partition __consumer_offsets-22 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,345] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,345] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,345] INFO [Broker id=0] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,356] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,357] INFO Created log for partition __consumer_offsets-29 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,357] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,357] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,357] INFO [Broker id=0] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,368] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,369] INFO Created log for partition __consumer_offsets-44 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,369] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,369] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,369] INFO [Broker id=0] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,379] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,379] INFO Created log for partition __consumer_offsets-14 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,379] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,379] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,380] INFO [Broker id=0] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,390] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,390] INFO Created log for partition __consumer_offsets-23 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,390] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,390] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,390] INFO [Broker id=0] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,401] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,401] INFO Created log for partition __consumer_offsets-38 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,401] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,401] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,401] INFO [Broker id=0] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,412] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,412] INFO Created log for partition __consumer_offsets-8 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,412] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,412] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,412] INFO [Broker id=0] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,422] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,423] INFO Created log for partition __consumer_offsets-45 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,423] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,423] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,423] INFO [Broker id=0] Leader __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,433] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,434] INFO Created log for partition __consumer_offsets-15 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,434] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,434] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,434] INFO [Broker id=0] Leader __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,444] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,444] INFO Created log for partition __consumer_offsets-30 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,445] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,445] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,445] INFO [Broker id=0] Leader __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,455] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,455] INFO Created log for partition __consumer_offsets-0 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,455] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,455] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,455] INFO [Broker id=0] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,467] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,467] INFO Created log for partition __consumer_offsets-35 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,467] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,467] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,467] INFO [Broker id=0] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,478] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,478] INFO Created log for partition __consumer_offsets-5 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,478] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,478] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,478] INFO [Broker id=0] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,489] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,489] INFO Created log for partition __consumer_offsets-20 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,489] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,490] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,490] INFO [Broker id=0] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,501] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,501] INFO Created log for partition __consumer_offsets-27 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,501] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,501] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,501] INFO [Broker id=0] Leader __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,512] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,512] INFO Created log for partition __consumer_offsets-42 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,512] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,512] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,512] INFO [Broker id=0] Leader __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,522] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,523] INFO Created log for partition __consumer_offsets-12 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,523] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,523] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,523] INFO [Broker id=0] Leader __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,533] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,533] INFO Created log for partition __consumer_offsets-21 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,533] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,533] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,533] INFO [Broker id=0] Leader __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,796] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,798] INFO Created log for partition __consumer_offsets-36 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,798] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,798] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,798] INFO [Broker id=0] Leader __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,808] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,808] INFO Created log for partition __consumer_offsets-6 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,808] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,808] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,808] INFO [Broker id=0] Leader __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,819] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,819] INFO Created log for partition __consumer_offsets-43 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,819] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,819] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,819] INFO [Broker id=0] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,830] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,830] INFO Created log for partition __consumer_offsets-13 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,830] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,830] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,830] INFO [Broker id=0] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,841] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/EmbeddedKafkaCluster2253284502884208513] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:09,841] INFO Created log for partition __consumer_offsets-28 in /tmp/EmbeddedKafkaCluster2253284502884208513/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:09,841] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,841] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:09,842] INFO [Broker id=0] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:09,851] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,852] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,853] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,853] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,853] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,853] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,853] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,853] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,853] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,854] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,854] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,854] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,854] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,854] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,854] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,854] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,854] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,854] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,854] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,854] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,854] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,854] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,854] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,854] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,854] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,854] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,854] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,854] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,855] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,855] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,855] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,855] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,855] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,855] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,855] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,855] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,855] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,855] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,855] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,855] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,855] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,856] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,857] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,857] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,857] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,857] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,857] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,857] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,857] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,857] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,857] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,857] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,857] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,857] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,858] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,858] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,858] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,858] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,858] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,858] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,858] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,858] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,858] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,858] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,858] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,858] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,858] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,858] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,858] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,858] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,858] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,858] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,858] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,858] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,858] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,859] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,859] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,859] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,859] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,859] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,859] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,858] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,859] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,859] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,859] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,859] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,859] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,859] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,859] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:09,859] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,859] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,859] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,859] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,860] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 3 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,860] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,860] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,860] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,860] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,860] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,860] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,860] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,860] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,860] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,860] INFO [Broker id=0] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 10 (state.change.logger:66)
[2025-10-07 12:42:09,860] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,860] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,861] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,861] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,861] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:09,906] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-integration-test-elasticsearch-it-connect-cluster in Empty state. Created a new member id connect-3-dbbebc33-9a48-43b9-bcef-873dc38940f0 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,911] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member connect-3-dbbebc33-9a48-43b9-bcef-873dc38940f0 with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,913] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 1 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,918] INFO [GroupCoordinator 0]: Assignment received from leader connect-3-dbbebc33-9a48-43b9-bcef-873dc38940f0 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,929] INFO ElasticsearchSinkConnectorConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:35097/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 12:42:09,931] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:42:09,931] INFO Using unsecured connection to [http://localhost:35097]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:42:09,950] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 1 (__consumer_offsets-4) (reason: Leader connect-3-dbbebc33-9a48-43b9-bcef-873dc38940f0 re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,951] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 2 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,953] INFO [GroupCoordinator 0]: Assignment received from leader connect-3-dbbebc33-9a48-43b9-bcef-873dc38940f0 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 2. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,956] INFO [es-connector|worker] ElasticsearchSinkConnectorConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:35097/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 12:42:09,975] ERROR Could not check connector state info. (io.confluent.connect.elasticsearch.integration.BaseConnectorIT:92)
[2025-10-07 12:42:09,978] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 2 (__consumer_offsets-4) (reason: Leader connect-3-dbbebc33-9a48-43b9-bcef-873dc38940f0 re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,978] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 3 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,981] INFO [GroupCoordinator 0]: Assignment received from leader connect-3-dbbebc33-9a48-43b9-bcef-873dc38940f0 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:09,988] INFO [es-connector|task-0] Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:42:09,988] INFO [es-connector|task-0] ElasticsearchSinkTaskConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:35097/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 0
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:42:09,988] INFO [es-connector|task-0] Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:42:09,989] DEBUG [es-connector|task-0] Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:42:09,989] INFO [es-connector|task-0] Using unsecured connection to [http://localhost:35097]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:42:09,999] INFO [es-connector|task-0] Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:42:10,005] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-es-connector in Empty state. Created a new member id connector-consumer-es-connector-0-04f59192-25b5-4a3f-a193-b631ad42faa6 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:10,007] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member connector-consumer-es-connector-0-04f59192-25b5-4a3f-a193-b631ad42faa6 with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:10,008] INFO [GroupCoordinator 0]: Stabilized group connect-es-connector generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:10,013] INFO [GroupCoordinator 0]: Assignment received from leader connector-consumer-es-connector-0-04f59192-25b5-4a3f-a193-b631ad42faa6 for group connect-es-connector for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:10,092] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:10,092] INFO [es-connector|task-0] Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:42:10,095] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:10,096] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:10,098] WARN [es-connector|task-0] Failed to execute bulk request due to null. Retrying attempt (1/3) after backoff of 16 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:42:10,115] WARN [es-connector|task-0] Failed to execute bulk request due to null. Retrying attempt (2/3) after backoff of 18 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:42:10,135] ERROR [es-connector|task-0] Failed to execute bulk request due to 'ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]
	at org.elasticsearch.rest.BytesRestResponse.errorFromXContent(BytesRestResponse.java:178)
	at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2484)
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
	Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:35097], URI [/_bulk?timeout=1m], status line [HTTP/1.1 429 Too Many Requests]
{
  "error": {
    "type": "circuit_breaking_exception",
    "reason": "Data too large",
    "bytes_wanted": 123848638,
    "bytes_limit": 123273216,
    "durability": "TRANSIENT"
  },
  "status": 429
}
		at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
		at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
		at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
		... 13 more
[2025-10-07 12:42:10,135] WARN [es-connector|task-0] Bulk request 1 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]
	at org.elasticsearch.rest.BytesRestResponse.errorFromXContent(BytesRestResponse.java:178)
	at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2484)
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:35097], URI [/_bulk?timeout=1m], status line [HTTP/1.1 429 Too Many Requests]
{
  "error": {
    "type": "circuit_breaking_exception",
    "reason": "Data too large",
    "bytes_wanted": 123848638,
    "bytes_limit": 123273216,
    "durability": "TRANSIENT"
  },
  "status": 429
}
		at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
		at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
		at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
		... 13 more
[2025-10-07 12:42:10,137] ERROR [es-connector|task-0] WorkerSinkTask{id=es-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Bulk request failed (org.apache.kafka.connect.runtime.WorkerSinkTask:629)
org.apache.kafka.connect.errors.ConnectException: Bulk request failed
	at io.confluent.connect.elasticsearch.ElasticsearchClient$2.afterBulk(ElasticsearchClient.java:455)
	at org.elasticsearch.action.bulk.BulkRequestHandler$1.onFailure(BulkRequestHandler.java:64)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.ActionListener$RunAfterActionListener.onFailure(ActionListener.java:350)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onFailure(Retry.java:123)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:227)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	... 5 more
Caused by: ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]
	at org.elasticsearch.rest.BytesRestResponse.errorFromXContent(BytesRestResponse.java:178)
	at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2484)
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:35097], URI [/_bulk?timeout=1m], status line [HTTP/1.1 429 Too Many Requests]
{
  "error": {
    "type": "circuit_breaking_exception",
    "reason": "Data too large",
    "bytes_wanted": 123848638,
    "bytes_limit": 123273216,
    "durability": "TRANSIENT"
  },
  "status": 429
}
		at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
		at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
		at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
		... 13 more
[2025-10-07 12:42:10,138] DEBUG [es-connector|task-0] Tried to flush data to Elasticsearch, but BulkProcessor is already closed. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:137)
java.lang.IllegalStateException: bulk process already closed
	at org.elasticsearch.action.bulk.BulkProcessor.ensureOpen(BulkProcessor.java:454)
	at org.elasticsearch.action.bulk.BulkProcessor.flush(BulkProcessor.java:571)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.flush(ElasticsearchClient.java:303)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.preCommit(ElasticsearchSinkTask.java:135)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:404)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.closePartitions(WorkerSinkTask.java:666)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.closeAllPartitions(WorkerSinkTask.java:661)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:204)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[2025-10-07 12:42:10,138] DEBUG [es-connector|task-0] preCommitting offsets {} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:42:10,139] ERROR [es-connector|task-0] WorkerSinkTask{id=es-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:195)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:631)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:333)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:234)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:203)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.kafka.connect.errors.ConnectException: Bulk request failed
	at io.confluent.connect.elasticsearch.ElasticsearchClient$2.afterBulk(ElasticsearchClient.java:455)
	at org.elasticsearch.action.bulk.BulkRequestHandler$1.onFailure(BulkRequestHandler.java:64)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.ActionListener$RunAfterActionListener.onFailure(ActionListener.java:350)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onFailure(Retry.java:123)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:227)
	... 5 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	... 5 more
Caused by: ElasticsearchStatusException[Elasticsearch exception [type=circuit_breaking_exception, reason=Data too large]]
	at org.elasticsearch.rest.BytesRestResponse.errorFromXContent(BytesRestResponse.java:178)
	at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2484)
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:35097], URI [/_bulk?timeout=1m], status line [HTTP/1.1 429 Too Many Requests]
{
  "error": {
    "type": "circuit_breaking_exception",
    "reason": "Data too large",
    "bytes_wanted": 123848638,
    "bytes_limit": 123273216,
    "durability": "TRANSIENT"
  },
  "status": 429
}
		at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
		at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
		at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
		... 13 more
[2025-10-07 12:42:10,139] DEBUG [es-connector|task-0] Stopping Elasticsearch client. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:146)
[2025-10-07 12:42:10,140] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 1 (__consumer_offsets-23) (reason: Removing member connector-consumer-es-connector-0-04f59192-25b5-4a3f-a193-b631ad42faa6 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:10,140] INFO [GroupCoordinator 0]: Group connect-es-connector with generation 2 is now empty (__consumer_offsets-23) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:10,141] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connector-consumer-es-connector-0-04f59192-25b5-4a3f-a193-b631ad42faa6, groupInstanceId=None, clientId=connector-consumer-es-connector-0, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group connect-es-connector through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:10,231] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 3 (__consumer_offsets-4) (reason: Removing member connect-3-dbbebc33-9a48-43b9-bcef-873dc38940f0 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:10,232] INFO [GroupCoordinator 0]: Group connect-integration-test-elasticsearch-it-connect-cluster with generation 4 is now empty (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:10,233] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connect-3-dbbebc33-9a48-43b9-bcef-873dc38940f0, groupInstanceId=None, clientId=connect-3, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(sessioned, compatible, default)) has left group connect-integration-test-elasticsearch-it-connect-cluster through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:10,243] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer:66)
[2025-10-07 12:42:10,243] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer:66)
[2025-10-07 12:42:10,245] INFO [Controller id=0] Shutting down broker 0 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:10,246] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:10,246] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer:66)
[2025-10-07 12:42:10,247] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:10,247] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:10,247] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:10,247] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:10,248] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:10,248] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 12:42:10,249] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 12:42:10,250] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:10,401] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:10,401] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:10,401] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis:66)
[2025-10-07 12:42:10,401] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:10,601] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:10,601] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:10,601] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:10,602] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager:66)
[2025-10-07 12:42:10,602] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:10,602] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:10,602] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:10,604] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:10,604] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:10,604] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:10,802] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:10,802] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:10,802] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:10,808] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:10,808] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:10,808] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:10,808] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager:66)
[2025-10-07 12:42:10,809] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:10,809] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:10,809] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:10,809] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:10,809] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:10,809] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 12:42:10,809] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 12:42:10,809] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:10,933] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:10,933] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:10,934] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:11,002] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:11,002] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:11,002] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:11,202] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:11,202] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:11,203] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:11,401] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:11,401] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:11,408] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager:66)
[2025-10-07 12:42:11,409] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:11,409] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:11,409] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:11,411] INFO Broker to controller channel manager for alterIsr shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 12:42:11,411] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:11,411] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:11,411] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:11,413] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 12:42:11,413] INFO Shutting down. (kafka.log.LogManager:66)
[2025-10-07 12:42:11,413] INFO Shutting down the log cleaner. (kafka.log.LogCleaner:66)
[2025-10-07 12:42:11,413] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner:66)
[2025-10-07 12:42:11,413] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner:66)
[2025-10-07 12:42:11,413] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner:66)
[2025-10-07 12:42:11,421] INFO [ProducerStateManager partition=test-0] Wrote producer snapshot at offset 5 with 1 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:11,432] INFO [ProducerStateManager partition=__consumer_offsets-23] Wrote producer snapshot at offset 2 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:11,439] INFO [ProducerStateManager partition=connect-config-topic-elasticsearch-it-connect-cluster-0] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:11,448] INFO [ProducerStateManager partition=__consumer_offsets-4] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:11,462] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-2] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:11,472] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-3] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:11,485] INFO Shutdown complete. (kafka.log.LogManager:66)
[2025-10-07 12:42:11,485] INFO [ControllerEventThread controllerId=0] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:11,485] INFO [ControllerEventThread controllerId=0] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:11,485] INFO [ControllerEventThread controllerId=0] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:11,486] INFO [PartitionStateMachine controllerId=0] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:42:11,486] INFO [ReplicaStateMachine controllerId=0] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:11,486] INFO [RequestSendThread controllerId=0] Shutting down (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:11,487] INFO [RequestSendThread controllerId=0] Stopped (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:11,487] INFO [RequestSendThread controllerId=0] Shutdown completed (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:11,489] INFO [Controller id=0] Resigned (kafka.controller.KafkaController:66)
[2025-10-07 12:42:11,489] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:11,489] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:11,489] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:11,489] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:11,594] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:11,595] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:12,504] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:12,504] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:12,504] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:13,504] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:13,504] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:13,505] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:14,505] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:14,505] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:14,505] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:14,506] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:14,506] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:14,506] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer:66)
[2025-10-07 12:42:14,512] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer:66)
[2025-10-07 12:42:14,513] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats:66)
[2025-10-07 12:42:14,513] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer:66)
[2025-10-07 12:42:14,549] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster6284526723200303364
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:34949
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:42:14,551] INFO starting (kafka.server.KafkaServer:66)
[2025-10-07 12:42:14,551] INFO Connecting to zookeeper on 127.0.0.1:34949 (kafka.server.KafkaServer:66)
[2025-10-07 12:42:14,552] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:34949. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:14,552] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:14,556] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:14,571] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:14,573] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 12:42:14,573] INFO Cleared cache (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 12:42:14,576] INFO Cluster ID = t09pwzwsSwW_-G5tw3RxIA (kafka.server.KafkaServer:66)
[2025-10-07 12:42:14,576] WARN No meta.properties file under dir /tmp/EmbeddedKafkaCluster6284526723200303364/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2025-10-07 12:42:14,579] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster6284526723200303364
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:34949
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:42:14,581] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster6284526723200303364
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:34949
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:42:14,589] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:14,589] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:14,590] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:14,591] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:14,593] INFO Loading logs from log dirs ArraySeq(/tmp/EmbeddedKafkaCluster6284526723200303364) (kafka.log.LogManager:66)
[2025-10-07 12:42:14,593] INFO Attempting recovery for all logs in /tmp/EmbeddedKafkaCluster6284526723200303364 since no clean shutdown file was found (kafka.log.LogManager:66)
[2025-10-07 12:42:14,593] INFO Loaded 0 logs in 0ms. (kafka.log.LogManager:66)
[2025-10-07 12:42:14,593] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:66)
[2025-10-07 12:42:14,593] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:66)
[2025-10-07 12:42:14,594] INFO Starting the log cleaner (kafka.log.LogCleaner:66)
[2025-10-07 12:42:14,627] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner:66)
[2025-10-07 12:42:14,629] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:14,637] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas:66)
[2025-10-07 12:42:14,637] INFO Awaiting socket connections on localhost:44985. (kafka.network.Acceptor:66)
[2025-10-07 12:42:14,639] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 12:42:14,641] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:14,641] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:14,642] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:14,643] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:14,644] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:14,646] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:14,647] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:14,648] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1759840934648,1759840934648,1,0,0,72057676056625152,204,0,25
 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:14,649] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:44985, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:14,658] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:14,659] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:14,660] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:14,661] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:14,661] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:14,662] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:14,662] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:14,662] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:14,663] INFO [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,663] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:14,664] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:14,664] INFO [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{}) (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,665] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:14,665] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 12:42:14,666] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 12:42:14,666] INFO [Controller id=0] Registering handlers (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,667] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:14,668] INFO [Controller id=0] Deleting log dir event notifications (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,669] INFO [Controller id=0] Deleting isr change notifications (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,669] INFO [Controller id=0] Initializing controller context (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,669] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:14,670] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 12:42:14,670] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:14,670] INFO [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25) (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,670] INFO [KafkaServer id=0] started (kafka.server.KafkaServer:66)
[2025-10-07 12:42:14,674] INFO [Controller id=0] Currently active brokers in the cluster: Set(0) (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,675] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:14,675] INFO [Controller id=0] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,675] INFO [Controller id=0] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,675] INFO [Controller id=0] Fetching topic deletions in progress (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,676] INFO [Controller id=0] List of topics to be deleted:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,676] INFO [Controller id=0] List of topics ineligible for deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,676] INFO [Controller id=0] Initializing topic deletion manager (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,676] INFO [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager:66)
[2025-10-07 12:42:14,676] INFO [Controller id=0] Sending update metadata request (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,676] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:14,676] INFO [ReplicaStateMachine controllerId=0] Initializing replica state (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:14,676] INFO [ReplicaStateMachine controllerId=0] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:14,676] INFO [ReplicaStateMachine controllerId=0] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:14,676] INFO [PartitionStateMachine controllerId=0] Initializing partition state (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:42:14,676] INFO [RequestSendThread controllerId=0] Controller 0 connected to localhost:44985 (id: 0 rack: null) for sending state change requests (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:14,676] INFO [PartitionStateMachine controllerId=0] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:42:14,677] INFO [Controller id=0] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,678] INFO [Controller id=0] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,678] INFO [Controller id=0] Partitions that completed preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,678] INFO [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,678] INFO [Controller id=0] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,678] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,679] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController:66)
[2025-10-07 12:42:14,730] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker localhost:44985 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:14,742] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:44985 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:14,743] INFO [Controller id=0] Acquired new producerId block ProducerIdsBlock{brokerId=0, producerIdStart=0, producerIdLen=1000} by writing to Zk with path version 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:16,316] INFO Creating topic connect-offset-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:16,321] INFO [Controller id=0] New topics: [Set(connect-offset-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-offset-topic-elasticsearch-it-connect-cluster,Some(fhjST2ZqSPWku8yvvxXfWQ),HashMap(connect-offset-topic-elasticsearch-it-connect-cluster-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:16,321] INFO [Controller id=0] New partition creation callback for connect-offset-topic-elasticsearch-it-connect-cluster-20,connect-offset-topic-elasticsearch-it-connect-cluster-7,connect-offset-topic-elasticsearch-it-connect-cluster-0,connect-offset-topic-elasticsearch-it-connect-cluster-12,connect-offset-topic-elasticsearch-it-connect-cluster-13,connect-offset-topic-elasticsearch-it-connect-cluster-5,connect-offset-topic-elasticsearch-it-connect-cluster-21,connect-offset-topic-elasticsearch-it-connect-cluster-14,connect-offset-topic-elasticsearch-it-connect-cluster-6,connect-offset-topic-elasticsearch-it-connect-cluster-1,connect-offset-topic-elasticsearch-it-connect-cluster-19,connect-offset-topic-elasticsearch-it-connect-cluster-17,connect-offset-topic-elasticsearch-it-connect-cluster-16,connect-offset-topic-elasticsearch-it-connect-cluster-23,connect-offset-topic-elasticsearch-it-connect-cluster-10,connect-offset-topic-elasticsearch-it-connect-cluster-3,connect-offset-topic-elasticsearch-it-connect-cluster-15,connect-offset-topic-elasticsearch-it-connect-cluster-22,connect-offset-topic-elasticsearch-it-connect-cluster-24,connect-offset-topic-elasticsearch-it-connect-cluster-18,connect-offset-topic-elasticsearch-it-connect-cluster-2,connect-offset-topic-elasticsearch-it-connect-cluster-4,connect-offset-topic-elasticsearch-it-connect-cluster-8,connect-offset-topic-elasticsearch-it-connect-cluster-9,connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,322] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,323] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,323] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,323] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:16,324] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:16,333] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,333] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,333] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,333] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,333] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,333] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,333] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 25 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:16,334] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:16,335] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:16,335] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:16,338] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-offset-topic-elasticsearch-it-connect-cluster-20, connect-offset-topic-elasticsearch-it-connect-cluster-7, connect-offset-topic-elasticsearch-it-connect-cluster-0, connect-offset-topic-elasticsearch-it-connect-cluster-12, connect-offset-topic-elasticsearch-it-connect-cluster-13, connect-offset-topic-elasticsearch-it-connect-cluster-5, connect-offset-topic-elasticsearch-it-connect-cluster-21, connect-offset-topic-elasticsearch-it-connect-cluster-14, connect-offset-topic-elasticsearch-it-connect-cluster-6, connect-offset-topic-elasticsearch-it-connect-cluster-1, connect-offset-topic-elasticsearch-it-connect-cluster-19, connect-offset-topic-elasticsearch-it-connect-cluster-17, connect-offset-topic-elasticsearch-it-connect-cluster-16, connect-offset-topic-elasticsearch-it-connect-cluster-23, connect-offset-topic-elasticsearch-it-connect-cluster-10, connect-offset-topic-elasticsearch-it-connect-cluster-3, connect-offset-topic-elasticsearch-it-connect-cluster-15, connect-offset-topic-elasticsearch-it-connect-cluster-22, connect-offset-topic-elasticsearch-it-connect-cluster-24, connect-offset-topic-elasticsearch-it-connect-cluster-18, connect-offset-topic-elasticsearch-it-connect-cluster-2, connect-offset-topic-elasticsearch-it-connect-cluster-4, connect-offset-topic-elasticsearch-it-connect-cluster-8, connect-offset-topic-elasticsearch-it-connect-cluster-9, connect-offset-topic-elasticsearch-it-connect-cluster-11) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:16,338] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 25 partitions (state.change.logger:66)
Oct 07, 2025 12:42:16 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Oct 07, 2025 12:42:16 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Oct 07, 2025 12:42:16 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Oct 07, 2025 12:42:16 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
[2025-10-07 12:42:16,340] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-10, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,342] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-10 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,342] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,342] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,342] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,353] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-14, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,353] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-14 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,353] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,353] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,353] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,364] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-18, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,364] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-18 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,364] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,364] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,364] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,375] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,375] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,375] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,375] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,375] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 12:42:16 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

[2025-10-07 12:42:16,387] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-22, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,387] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-22 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,387] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,387] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,387] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,389] INFO Creating topic test with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:16,392] INFO [Controller id=0] New topics: [Set(test)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(test,Some(zDWv89kRTQ6R6TjIZeTuSQ),Map(test-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:16,392] INFO [Controller id=0] New partition creation callback for test-0 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:16,392] INFO [Controller id=0 epoch=1] Changed partition test-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,392] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:16,393] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:16,396] INFO [Controller id=0 epoch=1] Changed partition test-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,396] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:16,396] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:16,397] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:16,398] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-7, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,398] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-7 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,398] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,398] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,398] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,409] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-11, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,409] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-11 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,409] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,409] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,409] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,420] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-15, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,420] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-15 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,420] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,420] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,420] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,430] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,430] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,431] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,431] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,431] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,441] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-19, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,442] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-19 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,442] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,442] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,442] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,452] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,453] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,453] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,453] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,453] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,463] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-23, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,463] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-23 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,463] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,463] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,463] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,474] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-8, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,474] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-8 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,474] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,474] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,474] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,485] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-12, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,485] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-12 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,485] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,485] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,485] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,496] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-16, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,496] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-16 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,496] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,496] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,496] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,507] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,507] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,507] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,507] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,507] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,518] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-20, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,518] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-20 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,518] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,518] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,519] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,530] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-5, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,530] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-5 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,530] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,530] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,530] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,541] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-24, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,541] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-24 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,541] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,541] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,541] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,552] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-9, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,552] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-9 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,552] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,552] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,552] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,563] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-13, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,563] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-13 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,563] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,563] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,563] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,574] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-17, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,574] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-17 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,575] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,575] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,575] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,586] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,586] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,586] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,586] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,586] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,597] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-21, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,597] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-21 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,597] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,597] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,597] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,609] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-6, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,609] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-offset-topic-elasticsearch-it-connect-cluster-6 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,610] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,610] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,610] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,615] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:16,617] INFO [Broker id=0] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger:66)
[2025-10-07 12:42:16,618] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:16,620] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:16,620] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:16,622] INFO [LogLoader partition=test-0, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,623] INFO Created log for partition test-0 in /tmp/EmbeddedKafkaCluster6284526723200303364/test-0 with properties {} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,623] INFO [Partition test-0 broker=0] No checkpointed highwatermark is found for partition test-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,623] INFO [Partition test-0 broker=0] Log loaded for partition test-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,623] INFO [Broker id=0] Leader test-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,635] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:16,637] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger:66)
[2025-10-07 12:42:16,640] INFO Creating topic connect-storage-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:16,646] INFO [Controller id=0] New topics: [Set(connect-storage-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-storage-topic-elasticsearch-it-connect-cluster,Some(7CJJbXCXQkyYuVCPuMGoMg),HashMap(connect-storage-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:16,646] INFO [Controller id=0] New partition creation callback for connect-storage-topic-elasticsearch-it-connect-cluster-2,connect-storage-topic-elasticsearch-it-connect-cluster-0,connect-storage-topic-elasticsearch-it-connect-cluster-3,connect-storage-topic-elasticsearch-it-connect-cluster-4,connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:16,646] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,646] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,646] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,646] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,647] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,647] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:16,647] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:16,665] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,665] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,665] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,665] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,665] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,665] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:16,665] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:16,666] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:16,666] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:16,667] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-storage-topic-elasticsearch-it-connect-cluster-2, connect-storage-topic-elasticsearch-it-connect-cluster-0, connect-storage-topic-elasticsearch-it-connect-cluster-3, connect-storage-topic-elasticsearch-it-connect-cluster-4, connect-storage-topic-elasticsearch-it-connect-cluster-1) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:16,667] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:16,669] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,670] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-storage-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,670] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,670] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,670] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,680] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,680] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-storage-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,680] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,680] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,680] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,691] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,691] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-storage-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,691] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,691] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,691] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,702] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,702] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-storage-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,702] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,702] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,702] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,712] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,712] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-storage-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,712] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,712] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,712] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,722] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:16,723] INFO [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6 (state.change.logger:66)
[2025-10-07 12:42:16,740] INFO Creating topic connect-config-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:16,743] INFO [Controller id=0] New topics: [Set(connect-config-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-config-topic-elasticsearch-it-connect-cluster,Some(6lq0a851QsOcZ9D1d0BNQQ),Map(connect-config-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:16,743] INFO [Controller id=0] New partition creation callback for connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:16,743] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,743] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:16,744] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:16,745] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,745] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:16,746] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:16,746] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:16,746] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:16,746] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(connect-config-topic-elasticsearch-it-connect-cluster-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:16,746] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:16,748] INFO [LogLoader partition=connect-config-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,748] INFO Created log for partition connect-config-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster6284526723200303364/connect-config-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,748] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,748] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-config-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,748] INFO [Broker id=0] Leader connect-config-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,758] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:16,759] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger:66)
[2025-10-07 12:42:16,791] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:16,795] INFO [Controller id=0] New topics: [HashSet(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(3EH9j6jATyCmySwNpYO0Og),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:16,795] INFO [Controller id=0] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:16,795] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,795] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,796] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,797] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,797] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,797] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,797] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,797] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,797] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,797] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,797] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,797] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,797] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,797] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,797] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,797] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:16,797] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:16,797] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,811] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,812] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,813] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,813] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:16,813] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 50 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:16,813] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:16,814] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:16,814] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:16,818] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:16,818] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 0 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:16,819] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,820] INFO Created log for partition __consumer_offsets-3 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,820] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,820] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,820] INFO [Broker id=0] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,830] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,831] INFO Created log for partition __consumer_offsets-18 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,831] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,831] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,831] INFO [Broker id=0] Leader __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,841] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,841] INFO Created log for partition __consumer_offsets-41 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,841] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,841] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,841] INFO [Broker id=0] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,852] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,853] INFO Created log for partition __consumer_offsets-10 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,853] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,853] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,853] INFO [Broker id=0] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,863] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,864] INFO Created log for partition __consumer_offsets-33 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,864] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,864] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,864] INFO [Broker id=0] Leader __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,874] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,875] INFO Created log for partition __consumer_offsets-48 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,875] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,875] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,875] INFO [Broker id=0] Leader __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,882] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,882] INFO Created log for partition __consumer_offsets-19 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,882] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,882] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,882] INFO [Broker id=0] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,893] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,893] INFO Created log for partition __consumer_offsets-34 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,893] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,893] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,893] INFO [Broker id=0] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,904] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,905] INFO Created log for partition __consumer_offsets-4 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,905] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,905] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,905] INFO [Broker id=0] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,915] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,915] INFO Created log for partition __consumer_offsets-11 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,916] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,916] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,916] INFO [Broker id=0] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,926] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,926] INFO Created log for partition __consumer_offsets-26 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,926] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,926] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,926] INFO [Broker id=0] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,938] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,938] INFO Created log for partition __consumer_offsets-49 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,938] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,938] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,938] INFO [Broker id=0] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,948] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,948] INFO Created log for partition __consumer_offsets-39 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,949] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,949] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,949] INFO [Broker id=0] Leader __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,959] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,959] INFO Created log for partition __consumer_offsets-9 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,959] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,959] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,959] INFO [Broker id=0] Leader __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,970] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,970] INFO Created log for partition __consumer_offsets-24 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,970] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,970] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,970] INFO [Broker id=0] Leader __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,980] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,981] INFO Created log for partition __consumer_offsets-31 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,981] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,981] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,981] INFO [Broker id=0] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:16,991] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:16,991] INFO Created log for partition __consumer_offsets-46 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:16,991] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,991] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:16,991] INFO [Broker id=0] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,001] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,001] INFO Created log for partition __consumer_offsets-1 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,002] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,002] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,002] INFO [Broker id=0] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,013] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,013] INFO Created log for partition __consumer_offsets-16 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,013] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,013] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,013] INFO [Broker id=0] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,023] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,024] INFO Created log for partition __consumer_offsets-2 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,024] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,024] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,024] INFO [Broker id=0] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,034] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,034] INFO Created log for partition __consumer_offsets-25 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,034] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,034] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,034] INFO [Broker id=0] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,045] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,045] INFO Created log for partition __consumer_offsets-40 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,045] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,045] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,045] INFO [Broker id=0] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,056] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,057] INFO Created log for partition __consumer_offsets-47 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,057] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,057] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,057] INFO [Broker id=0] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,068] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,068] INFO Created log for partition __consumer_offsets-17 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,068] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,068] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,068] INFO [Broker id=0] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,079] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,079] INFO Created log for partition __consumer_offsets-32 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,079] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,079] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,079] INFO [Broker id=0] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,090] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,090] INFO Created log for partition __consumer_offsets-37 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,090] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,090] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,090] INFO [Broker id=0] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,101] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,101] INFO Created log for partition __consumer_offsets-7 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,101] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,101] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,101] INFO [Broker id=0] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,112] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,113] INFO Created log for partition __consumer_offsets-22 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,113] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,113] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,113] INFO [Broker id=0] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,123] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,124] INFO Created log for partition __consumer_offsets-29 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,124] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,124] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,124] INFO [Broker id=0] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,134] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,135] INFO Created log for partition __consumer_offsets-44 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,135] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,135] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,135] INFO [Broker id=0] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,147] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,147] INFO Created log for partition __consumer_offsets-14 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,147] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,147] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,147] INFO [Broker id=0] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,158] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,158] INFO Created log for partition __consumer_offsets-23 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,158] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,158] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,158] INFO [Broker id=0] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,169] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,169] INFO Created log for partition __consumer_offsets-38 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,169] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,169] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,169] INFO [Broker id=0] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,179] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,180] INFO Created log for partition __consumer_offsets-8 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,180] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,180] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,180] INFO [Broker id=0] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,190] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,191] INFO Created log for partition __consumer_offsets-45 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,191] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,191] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,191] INFO [Broker id=0] Leader __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,201] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,202] INFO Created log for partition __consumer_offsets-15 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,202] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,202] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,202] INFO [Broker id=0] Leader __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,212] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,212] INFO Created log for partition __consumer_offsets-30 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,213] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,213] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,213] INFO [Broker id=0] Leader __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,223] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,223] INFO Created log for partition __consumer_offsets-0 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,223] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,223] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,223] INFO [Broker id=0] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,234] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,234] INFO Created log for partition __consumer_offsets-35 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,235] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,235] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,235] INFO [Broker id=0] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,245] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,245] INFO Created log for partition __consumer_offsets-5 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,246] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,246] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,246] INFO [Broker id=0] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,257] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,257] INFO Created log for partition __consumer_offsets-20 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,257] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,257] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,257] INFO [Broker id=0] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,268] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,268] INFO Created log for partition __consumer_offsets-27 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,268] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,268] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,268] INFO [Broker id=0] Leader __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,278] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,278] INFO Created log for partition __consumer_offsets-42 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,278] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,278] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,279] INFO [Broker id=0] Leader __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,290] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,290] INFO Created log for partition __consumer_offsets-12 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,290] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,290] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,290] INFO [Broker id=0] Leader __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,301] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,301] INFO Created log for partition __consumer_offsets-21 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,301] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,301] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,301] INFO [Broker id=0] Leader __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,311] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,312] INFO Created log for partition __consumer_offsets-36 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,312] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,312] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,312] INFO [Broker id=0] Leader __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,322] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,322] INFO Created log for partition __consumer_offsets-6 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,323] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,323] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,323] INFO [Broker id=0] Leader __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,333] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,333] INFO Created log for partition __consumer_offsets-43 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,333] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,333] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,333] INFO [Broker id=0] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,344] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,344] INFO Created log for partition __consumer_offsets-13 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,344] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,344] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,344] INFO [Broker id=0] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,354] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/EmbeddedKafkaCluster6284526723200303364] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:17,354] INFO Created log for partition __consumer_offsets-28 in /tmp/EmbeddedKafkaCluster6284526723200303364/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:17,354] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,354] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:17,355] INFO [Broker id=0] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:17,364] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,364] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,365] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,365] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,365] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,365] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,366] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,366] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,366] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,366] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,366] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,366] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,366] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,366] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,366] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,366] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,366] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,366] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,366] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,366] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,366] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,366] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,366] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,366] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,366] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,366] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,366] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,366] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,366] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,366] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,367] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,367] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,367] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,367] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,367] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,367] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,367] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,367] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,367] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,367] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,367] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,367] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,367] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,367] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,367] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,367] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,367] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,367] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,367] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,367] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,367] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,367] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,367] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,367] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,368] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,368] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,368] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,368] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,368] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,368] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,368] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,368] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,368] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,368] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,368] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,370] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,370] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,369] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,370] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,370] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,370] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,370] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,370] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,370] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,370] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,370] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,370] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,370] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,370] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,370] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:17,370] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,370] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,370] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,370] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,371] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 3 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,371] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,371] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,371] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,371] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,371] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,371] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,371] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,371] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,371] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,371] INFO [Broker id=0] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 10 (state.change.logger:66)
[2025-10-07 12:42:17,371] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,372] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,372] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,372] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,372] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,372] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,372] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,372] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,372] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,372] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,372] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,372] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,372] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,373] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,373] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,373] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:17,402] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-integration-test-elasticsearch-it-connect-cluster in Empty state. Created a new member id connect-4-0cd54f80-6906-46c6-812f-ae23993a7f76 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,407] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member connect-4-0cd54f80-6906-46c6-812f-ae23993a7f76 with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,408] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 1 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,411] INFO [GroupCoordinator 0]: Assignment received from leader connect-4-0cd54f80-6906-46c6-812f-ae23993a7f76 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,420] INFO ElasticsearchSinkConnectorConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:36233/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 12:42:17,421] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:42:17,422] INFO Using unsecured connection to [http://localhost:36233]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:42:17,440] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 1 (__consumer_offsets-4) (reason: Leader connect-4-0cd54f80-6906-46c6-812f-ae23993a7f76 re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,440] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 2 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,442] INFO [GroupCoordinator 0]: Assignment received from leader connect-4-0cd54f80-6906-46c6-812f-ae23993a7f76 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 2. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,446] INFO [es-connector|worker] ElasticsearchSinkConnectorConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:36233/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 12:42:17,459] ERROR Could not check connector state info. (io.confluent.connect.elasticsearch.integration.BaseConnectorIT:92)
[2025-10-07 12:42:17,462] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 2 (__consumer_offsets-4) (reason: Leader connect-4-0cd54f80-6906-46c6-812f-ae23993a7f76 re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,463] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 3 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,465] INFO [GroupCoordinator 0]: Assignment received from leader connect-4-0cd54f80-6906-46c6-812f-ae23993a7f76 for group connect-integration-test-elasticsearch-it-connect-cluster for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,471] INFO [es-connector|task-0] Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:42:17,471] INFO [es-connector|task-0] ElasticsearchSinkTaskConfig values: 
	batch.size = 4
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:36233/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 1
	max.retries = 2
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 1000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 0
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:42:17,472] INFO [es-connector|task-0] Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:42:17,472] DEBUG [es-connector|task-0] Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:42:17,472] INFO [es-connector|task-0] Using unsecured connection to [http://localhost:36233]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:42:17,482] INFO [es-connector|task-0] Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:42:17,487] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-es-connector in Empty state. Created a new member id connector-consumer-es-connector-0-fedbe39f-352b-4889-837b-a3fa98aa28dc and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,488] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member connector-consumer-es-connector-0-fedbe39f-352b-4889-837b-a3fa98aa28dc with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,489] INFO [GroupCoordinator 0]: Stabilized group connect-es-connector generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,496] INFO [GroupCoordinator 0]: Assignment received from leader connector-consumer-es-connector-0-fedbe39f-352b-4889-837b-a3fa98aa28dc for group connect-es-connector for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,574] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:17,574] INFO [es-connector|task-0] Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:42:17,577] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:17,578] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:17,580] WARN [es-connector|task-0] Failed to execute bulk request due to org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:36233], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
. Retrying attempt (1/3) after backoff of 19 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:42:17,600] WARN [es-connector|task-0] Failed to execute bulk request due to org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:36233], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
. Retrying attempt (2/3) after backoff of 1 ms (io.confluent.connect.elasticsearch.RetryUtil:171)
[2025-10-07 12:42:17,602] ERROR [es-connector|task-0] Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36233], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];' after 3 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36233], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:36233], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 12:42:17,602] WARN [es-connector|task-0] Bulk request 1 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36233], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36233], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:36233], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 12:42:17,604] ERROR [es-connector|task-0] WorkerSinkTask{id=es-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Bulk request failed (org.apache.kafka.connect.runtime.WorkerSinkTask:629)
org.apache.kafka.connect.errors.ConnectException: Bulk request failed
	at io.confluent.connect.elasticsearch.ElasticsearchClient$2.afterBulk(ElasticsearchClient.java:455)
	at org.elasticsearch.action.bulk.BulkRequestHandler$1.onFailure(BulkRequestHandler.java:64)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.ActionListener$RunAfterActionListener.onFailure(ActionListener.java:350)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onFailure(Retry.java:123)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:227)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36233], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	... 5 more
Caused by: ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36233], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:36233], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 12:42:17,606] DEBUG [es-connector|task-0] Tried to flush data to Elasticsearch, but BulkProcessor is already closed. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:137)
java.lang.IllegalStateException: bulk process already closed
	at org.elasticsearch.action.bulk.BulkProcessor.ensureOpen(BulkProcessor.java:454)
	at org.elasticsearch.action.bulk.BulkProcessor.flush(BulkProcessor.java:571)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.flush(ElasticsearchClient.java:303)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.preCommit(ElasticsearchSinkTask.java:135)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:404)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.closePartitions(WorkerSinkTask.java:666)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.closeAllPartitions(WorkerSinkTask.java:661)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:204)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[2025-10-07 12:42:17,607] DEBUG [es-connector|task-0] preCommitting offsets {} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:42:17,607] ERROR [es-connector|task-0] WorkerSinkTask{id=es-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:195)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:631)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:333)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:234)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:203)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.kafka.connect.errors.ConnectException: Bulk request failed
	at io.confluent.connect.elasticsearch.ElasticsearchClient$2.afterBulk(ElasticsearchClient.java:455)
	at org.elasticsearch.action.bulk.BulkRequestHandler$1.onFailure(BulkRequestHandler.java:64)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.ActionListener$RunAfterActionListener.onFailure(ActionListener.java:350)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onFailure(Retry.java:123)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:227)
	... 5 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36233], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];' after 3 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	... 5 more
Caused by: ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36233], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:36233], URI [/_bulk?timeout=1m], status line [HTTP/1.1 503 Service Unavailable]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 12:42:17,608] DEBUG [es-connector|task-0] Stopping Elasticsearch client. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:146)
[2025-10-07 12:42:17,608] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 1 (__consumer_offsets-23) (reason: Removing member connector-consumer-es-connector-0-fedbe39f-352b-4889-837b-a3fa98aa28dc on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,609] INFO [GroupCoordinator 0]: Group connect-es-connector with generation 2 is now empty (__consumer_offsets-23) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,609] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connector-consumer-es-connector-0-fedbe39f-352b-4889-837b-a3fa98aa28dc, groupInstanceId=None, clientId=connector-consumer-es-connector-0, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group connect-es-connector through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,704] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 3 (__consumer_offsets-4) (reason: Removing member connect-4-0cd54f80-6906-46c6-812f-ae23993a7f76 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,705] INFO [GroupCoordinator 0]: Group connect-integration-test-elasticsearch-it-connect-cluster with generation 4 is now empty (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,706] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connect-4-0cd54f80-6906-46c6-812f-ae23993a7f76, groupInstanceId=None, clientId=connect-4, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(sessioned, compatible, default)) has left group connect-integration-test-elasticsearch-it-connect-cluster through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:17,721] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer:66)
[2025-10-07 12:42:17,721] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer:66)
[2025-10-07 12:42:17,722] INFO [Controller id=0] Shutting down broker 0 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:17,723] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:17,723] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer:66)
[2025-10-07 12:42:17,724] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:17,724] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:17,724] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:17,724] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:17,726] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:17,726] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 12:42:17,726] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 12:42:17,727] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:17,868] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:17,868] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:17,868] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis:66)
[2025-10-07 12:42:17,868] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,068] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,068] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,068] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:18,068] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager:66)
[2025-10-07 12:42:18,068] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:18,069] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:18,069] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:18,069] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:18,069] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:18,069] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,268] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,268] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,268] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,288] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,288] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,288] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:18,289] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager:66)
[2025-10-07 12:42:18,289] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:18,289] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:18,289] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:18,289] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:18,289] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:18,289] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 12:42:18,289] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 12:42:18,289] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,348] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,348] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,348] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,468] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,468] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,469] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,668] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,668] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,668] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,868] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,868] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:18,875] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager:66)
[2025-10-07 12:42:18,876] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:18,876] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:18,876] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:18,877] INFO Broker to controller channel manager for alterIsr shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 12:42:18,877] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:18,878] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:18,878] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:18,878] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 12:42:18,878] INFO Shutting down. (kafka.log.LogManager:66)
[2025-10-07 12:42:18,878] INFO Shutting down the log cleaner. (kafka.log.LogCleaner:66)
[2025-10-07 12:42:18,878] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner:66)
[2025-10-07 12:42:18,878] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner:66)
[2025-10-07 12:42:18,878] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner:66)
[2025-10-07 12:42:18,886] INFO [ProducerStateManager partition=test-0] Wrote producer snapshot at offset 5 with 1 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:18,896] INFO [ProducerStateManager partition=__consumer_offsets-23] Wrote producer snapshot at offset 2 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:18,904] INFO [ProducerStateManager partition=connect-config-topic-elasticsearch-it-connect-cluster-0] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:18,911] INFO [ProducerStateManager partition=__consumer_offsets-4] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:18,926] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-2] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:18,936] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-3] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:18,949] INFO Shutdown complete. (kafka.log.LogManager:66)
[2025-10-07 12:42:18,949] INFO [ControllerEventThread controllerId=0] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:18,949] INFO [ControllerEventThread controllerId=0] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:18,949] INFO [ControllerEventThread controllerId=0] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:18,951] INFO [PartitionStateMachine controllerId=0] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:42:18,951] INFO [ReplicaStateMachine controllerId=0] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:18,951] INFO [RequestSendThread controllerId=0] Shutting down (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:18,951] INFO [RequestSendThread controllerId=0] Stopped (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:18,951] INFO [RequestSendThread controllerId=0] Shutdown completed (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:18,952] INFO [Controller id=0] Resigned (kafka.controller.KafkaController:66)
[2025-10-07 12:42:18,952] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:18,952] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:18,952] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:18,952] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:19,056] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:19,056] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:19,867] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:19,868] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:19,868] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:20,868] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:20,868] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:20,868] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:21,868] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:21,868] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:21,868] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:22,868] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:22,868] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:22,868] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer:66)
[2025-10-07 12:42:22,875] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer:66)
[2025-10-07 12:42:22,876] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats:66)
[2025-10-07 12:42:22,876] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer:66)
[2025-10-07 12:42:22,904] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster9062231768575482232
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:33595
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:42:22,905] INFO starting (kafka.server.KafkaServer:66)
[2025-10-07 12:42:22,905] INFO Connecting to zookeeper on 127.0.0.1:33595 (kafka.server.KafkaServer:66)
[2025-10-07 12:42:22,906] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:33595. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:22,906] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:22,915] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:22,929] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:22,930] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 12:42:22,930] INFO Cleared cache (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 12:42:22,933] INFO Cluster ID = TMx_wEpkQ8eFOzUNOi5DeQ (kafka.server.KafkaServer:66)
[2025-10-07 12:42:22,933] WARN No meta.properties file under dir /tmp/EmbeddedKafkaCluster9062231768575482232/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2025-10-07 12:42:22,935] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster9062231768575482232
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:33595
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:42:22,937] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster9062231768575482232
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:33595
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:42:22,944] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:22,945] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:22,945] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:22,948] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:22,950] INFO Loading logs from log dirs ArraySeq(/tmp/EmbeddedKafkaCluster9062231768575482232) (kafka.log.LogManager:66)
[2025-10-07 12:42:22,950] INFO Attempting recovery for all logs in /tmp/EmbeddedKafkaCluster9062231768575482232 since no clean shutdown file was found (kafka.log.LogManager:66)
[2025-10-07 12:42:22,950] INFO Loaded 0 logs in 0ms. (kafka.log.LogManager:66)
[2025-10-07 12:42:22,950] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:66)
[2025-10-07 12:42:22,950] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:66)
[2025-10-07 12:42:22,951] INFO Starting the log cleaner (kafka.log.LogCleaner:66)
[2025-10-07 12:42:22,994] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner:66)
[2025-10-07 12:42:22,996] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:23,004] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas:66)
[2025-10-07 12:42:23,005] INFO Awaiting socket connections on localhost:34237. (kafka.network.Acceptor:66)
[2025-10-07 12:42:23,006] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 12:42:23,008] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:23,009] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:23,010] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:23,011] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:23,011] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:23,013] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:23,014] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:23,015] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1759840943015,1759840943015,1,0,0,72057676604112896,204,0,25
 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:23,015] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:34237, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:23,025] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:23,026] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:23,027] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:23,031] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:23,031] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:23,032] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:23,032] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:23,032] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:23,033] INFO [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,033] INFO [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{}) (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,033] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:23,034] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:23,034] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 12:42:23,034] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:23,035] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 12:42:23,035] INFO [Controller id=0] Registering handlers (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,036] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:23,037] INFO [Controller id=0] Deleting log dir event notifications (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,037] INFO [Controller id=0] Deleting isr change notifications (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,038] INFO [Controller id=0] Initializing controller context (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,039] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:23,039] INFO [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25) (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,039] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 12:42:23,039] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:23,040] INFO [KafkaServer id=0] started (kafka.server.KafkaServer:66)
[2025-10-07 12:42:23,041] INFO [Controller id=0] Currently active brokers in the cluster: Set(0) (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,041] INFO [Controller id=0] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,041] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:23,041] INFO [Controller id=0] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,041] INFO [Controller id=0] Fetching topic deletions in progress (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,042] INFO [Controller id=0] List of topics to be deleted:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,043] INFO [Controller id=0] List of topics ineligible for deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,043] INFO [Controller id=0] Initializing topic deletion manager (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,043] INFO [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager:66)
[2025-10-07 12:42:23,043] INFO [Controller id=0] Sending update metadata request (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,044] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:23,044] INFO [ReplicaStateMachine controllerId=0] Initializing replica state (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:23,044] INFO [ReplicaStateMachine controllerId=0] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:23,044] INFO [ReplicaStateMachine controllerId=0] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:23,044] INFO [PartitionStateMachine controllerId=0] Initializing partition state (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:42:23,044] INFO [PartitionStateMachine controllerId=0] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:42:23,044] INFO [Controller id=0] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,044] INFO [RequestSendThread controllerId=0] Controller 0 connected to localhost:34237 (id: 0 rack: null) for sending state change requests (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:23,045] INFO [Controller id=0] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,045] INFO [Controller id=0] Partitions that completed preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,045] INFO [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,045] INFO [Controller id=0] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,045] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,046] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController:66)
[2025-10-07 12:42:23,096] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker localhost:34237 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:23,108] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:34237 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:23,108] INFO [Controller id=0] Acquired new producerId block ProducerIdsBlock{brokerId=0, producerIdStart=0, producerIdLen=1000} by writing to Zk with path version 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:24,889] INFO Creating topic connect-offset-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:24,894] INFO [Controller id=0] New topics: [Set(connect-offset-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-offset-topic-elasticsearch-it-connect-cluster,Some(ez5SogCoQcKRAgOa9aN5Qw),HashMap(connect-offset-topic-elasticsearch-it-connect-cluster-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:24,894] INFO [Controller id=0] New partition creation callback for connect-offset-topic-elasticsearch-it-connect-cluster-20,connect-offset-topic-elasticsearch-it-connect-cluster-7,connect-offset-topic-elasticsearch-it-connect-cluster-0,connect-offset-topic-elasticsearch-it-connect-cluster-12,connect-offset-topic-elasticsearch-it-connect-cluster-13,connect-offset-topic-elasticsearch-it-connect-cluster-5,connect-offset-topic-elasticsearch-it-connect-cluster-21,connect-offset-topic-elasticsearch-it-connect-cluster-14,connect-offset-topic-elasticsearch-it-connect-cluster-6,connect-offset-topic-elasticsearch-it-connect-cluster-1,connect-offset-topic-elasticsearch-it-connect-cluster-19,connect-offset-topic-elasticsearch-it-connect-cluster-17,connect-offset-topic-elasticsearch-it-connect-cluster-16,connect-offset-topic-elasticsearch-it-connect-cluster-23,connect-offset-topic-elasticsearch-it-connect-cluster-10,connect-offset-topic-elasticsearch-it-connect-cluster-3,connect-offset-topic-elasticsearch-it-connect-cluster-15,connect-offset-topic-elasticsearch-it-connect-cluster-22,connect-offset-topic-elasticsearch-it-connect-cluster-24,connect-offset-topic-elasticsearch-it-connect-cluster-18,connect-offset-topic-elasticsearch-it-connect-cluster-2,connect-offset-topic-elasticsearch-it-connect-cluster-4,connect-offset-topic-elasticsearch-it-connect-cluster-8,connect-offset-topic-elasticsearch-it-connect-cluster-9,connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:24,894] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,894] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,895] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:24,896] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:24,904] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,904] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,905] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 25 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:24,906] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:24,906] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:24,906] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:24,909] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-offset-topic-elasticsearch-it-connect-cluster-20, connect-offset-topic-elasticsearch-it-connect-cluster-7, connect-offset-topic-elasticsearch-it-connect-cluster-0, connect-offset-topic-elasticsearch-it-connect-cluster-12, connect-offset-topic-elasticsearch-it-connect-cluster-13, connect-offset-topic-elasticsearch-it-connect-cluster-5, connect-offset-topic-elasticsearch-it-connect-cluster-21, connect-offset-topic-elasticsearch-it-connect-cluster-14, connect-offset-topic-elasticsearch-it-connect-cluster-6, connect-offset-topic-elasticsearch-it-connect-cluster-1, connect-offset-topic-elasticsearch-it-connect-cluster-19, connect-offset-topic-elasticsearch-it-connect-cluster-17, connect-offset-topic-elasticsearch-it-connect-cluster-16, connect-offset-topic-elasticsearch-it-connect-cluster-23, connect-offset-topic-elasticsearch-it-connect-cluster-10, connect-offset-topic-elasticsearch-it-connect-cluster-3, connect-offset-topic-elasticsearch-it-connect-cluster-15, connect-offset-topic-elasticsearch-it-connect-cluster-22, connect-offset-topic-elasticsearch-it-connect-cluster-24, connect-offset-topic-elasticsearch-it-connect-cluster-18, connect-offset-topic-elasticsearch-it-connect-cluster-2, connect-offset-topic-elasticsearch-it-connect-cluster-4, connect-offset-topic-elasticsearch-it-connect-cluster-8, connect-offset-topic-elasticsearch-it-connect-cluster-9, connect-offset-topic-elasticsearch-it-connect-cluster-11) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:24,909] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:24,911] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-10, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:24,913] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-10 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:24,913] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,913] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,913] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 12:42:24 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Oct 07, 2025 12:42:24 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Oct 07, 2025 12:42:24 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Oct 07, 2025 12:42:24 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
[2025-10-07 12:42:24,925] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-14, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:24,926] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-14 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:24,926] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,926] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,926] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:24,936] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-18, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:24,936] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-18 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:24,936] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,936] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,937] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 12:42:24 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

[2025-10-07 12:42:24,947] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:24,948] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:24,948] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,948] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,948] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:24,950] INFO Creating topic test with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:24,954] INFO [Controller id=0] New topics: [Set(test)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(test,Some(m8_nACyVSx-vHzOvGca2Ug),Map(test-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:24,954] INFO [Controller id=0] New partition creation callback for test-0 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:24,954] INFO [Controller id=0 epoch=1] Changed partition test-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:24,954] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:24,954] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:24,957] INFO [Controller id=0 epoch=1] Changed partition test-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:24,957] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:24,957] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:24,958] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:24,958] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-22, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:24,959] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-22 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:24,959] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,959] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,959] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:24,969] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-7, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:24,969] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-7 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:24,969] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,970] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,970] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:24,981] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-11, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:24,981] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-11 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:24,981] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,981] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,981] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:24,992] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-15, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:24,992] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-15 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:24,992] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,992] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:24,992] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,002] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,003] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,003] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,003] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,003] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,014] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-19, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,014] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-19 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,014] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,014] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,014] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,025] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,025] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,025] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,025] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,025] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,035] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-23, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,036] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-23 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,036] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,036] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,036] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,046] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-8, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,046] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-8 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,047] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,047] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,047] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,057] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-12, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,057] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-12 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,057] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,057] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,057] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,067] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-16, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,067] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-16 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,067] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,067] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,067] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,078] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,078] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,078] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,078] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,078] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,088] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-20, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,089] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-20 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,089] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,089] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,089] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,099] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-5, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,099] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-5 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,099] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,099] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,099] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,110] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-24, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,110] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-24 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,110] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,110] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,110] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,120] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-9, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,120] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-9 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,121] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,121] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,121] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,131] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-13, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,131] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-13 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,131] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,131] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,131] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,141] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-17, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,142] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-17 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,142] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,142] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,142] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,152] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,152] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,152] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,152] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,152] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,162] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-21, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,163] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-21 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,163] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,163] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,163] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,173] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-6, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,173] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-offset-topic-elasticsearch-it-connect-cluster-6 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,173] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,173] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,173] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,183] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:25,184] INFO [Broker id=0] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger:66)
[2025-10-07 12:42:25,186] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:25,188] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:25,188] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:25,190] INFO [LogLoader partition=test-0, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,191] INFO Created log for partition test-0 in /tmp/EmbeddedKafkaCluster9062231768575482232/test-0 with properties {} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,191] INFO [Partition test-0 broker=0] No checkpointed highwatermark is found for partition test-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,191] INFO [Partition test-0 broker=0] Log loaded for partition test-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,191] INFO [Broker id=0] Leader test-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,200] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:25,202] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger:66)
[2025-10-07 12:42:25,204] INFO Creating topic connect-storage-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:25,207] INFO [Controller id=0] New topics: [Set(connect-storage-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-storage-topic-elasticsearch-it-connect-cluster,Some(C5axd5PkQxin98ZM7U144w),HashMap(connect-storage-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:25,208] INFO [Controller id=0] New partition creation callback for connect-storage-topic-elasticsearch-it-connect-cluster-2,connect-storage-topic-elasticsearch-it-connect-cluster-0,connect-storage-topic-elasticsearch-it-connect-cluster-3,connect-storage-topic-elasticsearch-it-connect-cluster-4,connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:25,208] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,208] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,208] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,208] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,208] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,208] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:25,208] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:25,211] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,211] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,211] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,211] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,211] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,211] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:25,211] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:25,211] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:25,211] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:25,212] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-storage-topic-elasticsearch-it-connect-cluster-2, connect-storage-topic-elasticsearch-it-connect-cluster-0, connect-storage-topic-elasticsearch-it-connect-cluster-3, connect-storage-topic-elasticsearch-it-connect-cluster-4, connect-storage-topic-elasticsearch-it-connect-cluster-1) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:25,212] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:25,230] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,230] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-storage-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,230] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,231] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,231] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,240] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,240] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-storage-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,240] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,240] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,240] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,251] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,251] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-storage-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,251] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,251] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,251] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,262] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,262] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-storage-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,262] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,262] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,262] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,272] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,273] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-storage-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,273] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,273] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,273] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,282] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:25,283] INFO [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6 (state.change.logger:66)
[2025-10-07 12:42:25,296] INFO Creating topic connect-config-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:25,299] INFO [Controller id=0] New topics: [Set(connect-config-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-config-topic-elasticsearch-it-connect-cluster,Some(AnSlQqoPTrCL9SP4CTUyWQ),Map(connect-config-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:25,300] INFO [Controller id=0] New partition creation callback for connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:25,300] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,300] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:25,300] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:25,302] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,302] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:25,302] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:25,302] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:25,302] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:25,303] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(connect-config-topic-elasticsearch-it-connect-cluster-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:25,303] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:25,304] INFO [LogLoader partition=connect-config-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,304] INFO Created log for partition connect-config-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster9062231768575482232/connect-config-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,304] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,305] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-config-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,305] INFO [Broker id=0] Leader connect-config-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,314] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:25,314] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger:66)
[2025-10-07 12:42:25,334] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:25,338] INFO [Controller id=0] New topics: [HashSet(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(Xut6bP4nSfWK25A_jMP2mw),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:25,338] INFO [Controller id=0] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,339] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:25,340] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:25,353] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,354] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,355] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,355] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,355] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,355] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,355] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,355] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,355] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,355] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,355] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,355] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,355] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,355] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:25,355] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 50 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:25,355] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:25,356] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:25,356] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:25,363] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:25,363] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 0 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:25,365] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,365] INFO Created log for partition __consumer_offsets-3 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,365] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,365] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,365] INFO [Broker id=0] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,376] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,376] INFO Created log for partition __consumer_offsets-18 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,376] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,376] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,377] INFO [Broker id=0] Leader __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,387] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,387] INFO Created log for partition __consumer_offsets-41 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,387] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,387] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,387] INFO [Broker id=0] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,397] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,397] INFO Created log for partition __consumer_offsets-10 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,397] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,397] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,397] INFO [Broker id=0] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,407] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,408] INFO Created log for partition __consumer_offsets-33 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,408] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,408] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,408] INFO [Broker id=0] Leader __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,418] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,418] INFO Created log for partition __consumer_offsets-48 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,418] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,418] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,419] INFO [Broker id=0] Leader __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,429] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,429] INFO Created log for partition __consumer_offsets-19 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,429] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,429] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,429] INFO [Broker id=0] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,439] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,439] INFO Created log for partition __consumer_offsets-34 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,439] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,439] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,439] INFO [Broker id=0] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,449] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,450] INFO Created log for partition __consumer_offsets-4 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,450] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,450] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,450] INFO [Broker id=0] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,460] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,460] INFO Created log for partition __consumer_offsets-11 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,461] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,461] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,461] INFO [Broker id=0] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,471] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,471] INFO Created log for partition __consumer_offsets-26 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,471] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,471] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,471] INFO [Broker id=0] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,481] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,481] INFO Created log for partition __consumer_offsets-49 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,481] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,481] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,481] INFO [Broker id=0] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,492] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,493] INFO Created log for partition __consumer_offsets-39 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,493] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,493] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,493] INFO [Broker id=0] Leader __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,503] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,503] INFO Created log for partition __consumer_offsets-9 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,504] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,504] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,504] INFO [Broker id=0] Leader __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,514] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,514] INFO Created log for partition __consumer_offsets-24 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,514] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,514] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,514] INFO [Broker id=0] Leader __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,524] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,525] INFO Created log for partition __consumer_offsets-31 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,525] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,525] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,525] INFO [Broker id=0] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,535] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,535] INFO Created log for partition __consumer_offsets-46 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,535] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,536] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,536] INFO [Broker id=0] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,547] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,547] INFO Created log for partition __consumer_offsets-1 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,547] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,547] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,547] INFO [Broker id=0] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,557] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,557] INFO Created log for partition __consumer_offsets-16 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,558] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,558] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,558] INFO [Broker id=0] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,568] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,568] INFO Created log for partition __consumer_offsets-2 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,568] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,568] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,569] INFO [Broker id=0] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,579] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,579] INFO Created log for partition __consumer_offsets-25 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,579] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,579] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,579] INFO [Broker id=0] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,591] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,591] INFO Created log for partition __consumer_offsets-40 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,591] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,591] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,591] INFO [Broker id=0] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,599] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,599] INFO Created log for partition __consumer_offsets-47 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,599] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,599] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,599] INFO [Broker id=0] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,611] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,611] INFO Created log for partition __consumer_offsets-17 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,611] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,611] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,611] INFO [Broker id=0] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,622] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,622] INFO Created log for partition __consumer_offsets-32 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,622] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,622] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,622] INFO [Broker id=0] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,632] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,633] INFO Created log for partition __consumer_offsets-37 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,633] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,633] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,633] INFO [Broker id=0] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,643] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,643] INFO Created log for partition __consumer_offsets-7 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,643] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,643] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,643] INFO [Broker id=0] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,653] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,654] INFO Created log for partition __consumer_offsets-22 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,654] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,654] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,654] INFO [Broker id=0] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,664] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,664] INFO Created log for partition __consumer_offsets-29 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,664] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,664] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,664] INFO [Broker id=0] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,675] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,675] INFO Created log for partition __consumer_offsets-44 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,675] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,675] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,675] INFO [Broker id=0] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,685] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,685] INFO Created log for partition __consumer_offsets-14 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,685] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,686] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,686] INFO [Broker id=0] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,696] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,696] INFO Created log for partition __consumer_offsets-23 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,696] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,696] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,696] INFO [Broker id=0] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,706] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,706] INFO Created log for partition __consumer_offsets-38 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,706] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,706] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,706] INFO [Broker id=0] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,716] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,717] INFO Created log for partition __consumer_offsets-8 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,717] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,717] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,717] INFO [Broker id=0] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,728] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,728] INFO Created log for partition __consumer_offsets-45 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,728] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,728] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,728] INFO [Broker id=0] Leader __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,738] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,738] INFO Created log for partition __consumer_offsets-15 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,738] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,739] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,739] INFO [Broker id=0] Leader __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,749] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,749] INFO Created log for partition __consumer_offsets-30 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,749] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,749] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,749] INFO [Broker id=0] Leader __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,760] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,760] INFO Created log for partition __consumer_offsets-0 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,760] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,760] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,761] INFO [Broker id=0] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,770] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,771] INFO Created log for partition __consumer_offsets-35 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,771] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,771] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,771] INFO [Broker id=0] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,781] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,781] INFO Created log for partition __consumer_offsets-5 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,781] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,782] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,782] INFO [Broker id=0] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,791] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,791] INFO Created log for partition __consumer_offsets-20 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,791] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,791] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,791] INFO [Broker id=0] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,801] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,801] INFO Created log for partition __consumer_offsets-27 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,801] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,801] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,801] INFO [Broker id=0] Leader __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,811] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,812] INFO Created log for partition __consumer_offsets-42 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,812] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,812] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,812] INFO [Broker id=0] Leader __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,822] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,822] INFO Created log for partition __consumer_offsets-12 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,822] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,822] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,823] INFO [Broker id=0] Leader __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,834] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,834] INFO Created log for partition __consumer_offsets-21 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,834] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,834] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,834] INFO [Broker id=0] Leader __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,844] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,845] INFO Created log for partition __consumer_offsets-36 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,845] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,845] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,845] INFO [Broker id=0] Leader __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,855] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,855] INFO Created log for partition __consumer_offsets-6 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,855] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,855] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,855] INFO [Broker id=0] Leader __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,865] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,866] INFO Created log for partition __consumer_offsets-43 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,866] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,866] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,866] INFO [Broker id=0] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,877] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,877] INFO Created log for partition __consumer_offsets-13 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,877] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,877] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,877] INFO [Broker id=0] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,888] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/EmbeddedKafkaCluster9062231768575482232] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:25,888] INFO Created log for partition __consumer_offsets-28 in /tmp/EmbeddedKafkaCluster9062231768575482232/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:25,888] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,888] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:25,888] INFO [Broker id=0] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:25,898] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,899] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,899] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,899] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,899] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,899] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,899] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,899] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,899] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,899] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,899] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,899] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,899] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,899] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,900] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,899] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,901] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,902] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:25,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,903] INFO [Broker id=0] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 10 (state.change.logger:66)
[2025-10-07 12:42:25,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 4 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:25,942] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-integration-test-elasticsearch-it-connect-cluster in Empty state. Created a new member id connect-5-2ddc3c6c-bcf3-4480-baa1-8995be3a66ea and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,943] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member connect-5-2ddc3c6c-bcf3-4480-baa1-8995be3a66ea with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,945] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 1 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,947] INFO [GroupCoordinator 0]: Assignment received from leader connect-5-2ddc3c6c-bcf3-4480-baa1-8995be3a66ea for group connect-integration-test-elasticsearch-it-connect-cluster for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,957] INFO ElasticsearchSinkConnectorConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:39637/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 60000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 12:42:25,959] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:42:25,959] INFO Using unsecured connection to [http://localhost:39637]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:42:25,979] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 1 (__consumer_offsets-4) (reason: Leader connect-5-2ddc3c6c-bcf3-4480-baa1-8995be3a66ea re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,979] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 2 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,981] INFO [GroupCoordinator 0]: Assignment received from leader connect-5-2ddc3c6c-bcf3-4480-baa1-8995be3a66ea for group connect-integration-test-elasticsearch-it-connect-cluster for generation 2. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,986] INFO [es-connector|worker] ElasticsearchSinkConnectorConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:39637/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 60000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 12:42:25,995] ERROR Could not check connector state info. (io.confluent.connect.elasticsearch.integration.BaseConnectorIT:92)
[2025-10-07 12:42:25,999] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 2 (__consumer_offsets-4) (reason: Leader connect-5-2ddc3c6c-bcf3-4480-baa1-8995be3a66ea re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:25,999] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 3 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:26,013] INFO [GroupCoordinator 0]: Assignment received from leader connect-5-2ddc3c6c-bcf3-4480-baa1-8995be3a66ea for group connect-integration-test-elasticsearch-it-connect-cluster for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:26,026] INFO [es-connector|task-0] Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:42:26,026] INFO [es-connector|task-0] ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:39637/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 180000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 60000
	max.buffered.records = 20000
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 60000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 0
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:42:26,026] INFO [es-connector|task-0] Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:42:26,027] DEBUG [es-connector|task-0] Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:42:26,027] INFO [es-connector|task-0] Using unsecured connection to [http://localhost:39637]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:42:26,037] INFO [es-connector|task-0] Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:42:26,042] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-es-connector in Empty state. Created a new member id connector-consumer-es-connector-0-39ebb3af-5db3-4d4c-af1d-9b69e821ada4 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:26,043] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member connector-consumer-es-connector-0-39ebb3af-5db3-4d4c-af1d-9b69e821ada4 with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:26,044] INFO [GroupCoordinator 0]: Stabilized group connect-es-connector generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:26,050] INFO [GroupCoordinator 0]: Assignment received from leader connector-consumer-es-connector-0-39ebb3af-5db3-4d4c-af1d-9b69e821ada4 for group connect-es-connector for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:26,113] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:26,113] INFO [es-connector|task-0] Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:42:26,116] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:26,118] DEBUG [es-connector|task-0] Putting 5 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:26,225] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:26,226] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:26,325] DEBUG [es-connector|task-0] preCommitting offsets {test-0=OffsetAndMetadata{offset=10, leaderEpoch=null, metadata=''}} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:42:26,327] DEBUG [es-connector|task-0] Stopping Elasticsearch client. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:146)
[2025-10-07 12:42:26,328] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 1 (__consumer_offsets-23) (reason: Removing member connector-consumer-es-connector-0-39ebb3af-5db3-4d4c-af1d-9b69e821ada4 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:26,329] INFO [GroupCoordinator 0]: Group connect-es-connector with generation 2 is now empty (__consumer_offsets-23) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:26,329] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connector-consumer-es-connector-0-39ebb3af-5db3-4d4c-af1d-9b69e821ada4, groupInstanceId=None, clientId=connector-consumer-es-connector-0, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group connect-es-connector through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:26,335] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 3 (__consumer_offsets-4) (reason: Removing member connect-5-2ddc3c6c-bcf3-4480-baa1-8995be3a66ea on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:26,335] INFO [GroupCoordinator 0]: Group connect-integration-test-elasticsearch-it-connect-cluster with generation 4 is now empty (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:26,335] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connect-5-2ddc3c6c-bcf3-4480-baa1-8995be3a66ea, groupInstanceId=None, clientId=connect-5, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(sessioned, compatible, default)) has left group connect-integration-test-elasticsearch-it-connect-cluster through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:26,348] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer:66)
[2025-10-07 12:42:26,348] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer:66)
[2025-10-07 12:42:26,349] INFO [Controller id=0] Shutting down broker 0 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:26,350] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:26,350] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer:66)
[2025-10-07 12:42:26,351] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:26,351] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:26,351] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:26,351] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:26,353] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:26,353] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 12:42:26,353] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 12:42:26,354] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:26,414] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:26,414] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:26,414] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis:66)
[2025-10-07 12:42:26,415] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:26,614] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:26,614] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:26,615] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:26,615] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager:66)
[2025-10-07 12:42:26,615] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:26,615] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:26,615] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:26,615] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:26,616] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:26,616] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:26,814] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:26,814] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:26,815] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:26,844] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:26,844] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:26,844] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:26,845] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager:66)
[2025-10-07 12:42:26,845] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:26,845] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:26,845] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:26,845] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:26,845] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:26,845] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 12:42:26,845] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 12:42:26,845] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:27,036] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:27,036] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:27,036] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:27,215] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:27,215] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:27,215] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:27,415] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:27,415] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:27,415] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:27,615] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:27,615] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:27,622] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager:66)
[2025-10-07 12:42:27,622] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:27,622] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:27,622] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:27,623] INFO Broker to controller channel manager for alterIsr shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 12:42:27,623] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:27,623] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:27,623] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:27,624] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 12:42:27,624] INFO Shutting down. (kafka.log.LogManager:66)
[2025-10-07 12:42:27,624] INFO Shutting down the log cleaner. (kafka.log.LogCleaner:66)
[2025-10-07 12:42:27,624] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner:66)
[2025-10-07 12:42:27,624] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner:66)
[2025-10-07 12:42:27,624] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner:66)
[2025-10-07 12:42:27,631] INFO [ProducerStateManager partition=test-0] Wrote producer snapshot at offset 10 with 1 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:27,641] INFO [ProducerStateManager partition=__consumer_offsets-23] Wrote producer snapshot at offset 3 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:27,648] INFO [ProducerStateManager partition=connect-config-topic-elasticsearch-it-connect-cluster-0] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:27,655] INFO [ProducerStateManager partition=__consumer_offsets-4] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:27,663] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-2] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:27,673] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-3] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:27,686] INFO Shutdown complete. (kafka.log.LogManager:66)
[2025-10-07 12:42:27,687] INFO [ControllerEventThread controllerId=0] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:27,687] INFO [ControllerEventThread controllerId=0] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:27,687] INFO [ControllerEventThread controllerId=0] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:27,688] INFO [PartitionStateMachine controllerId=0] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:42:27,688] INFO [ReplicaStateMachine controllerId=0] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:27,688] INFO [RequestSendThread controllerId=0] Shutting down (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:27,689] INFO [RequestSendThread controllerId=0] Stopped (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:27,689] INFO [RequestSendThread controllerId=0] Shutdown completed (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:27,689] INFO [Controller id=0] Resigned (kafka.controller.KafkaController:66)
[2025-10-07 12:42:27,689] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:27,689] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:27,689] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:27,689] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:27,792] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:27,792] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:27,945] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:27,945] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:27,946] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:27,949] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:27,949] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:27,949] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:28,949] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:28,949] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:28,950] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:29,949] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:29,949] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:29,950] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer:66)
[2025-10-07 12:42:29,956] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer:66)
[2025-10-07 12:42:29,957] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats:66)
[2025-10-07 12:42:29,957] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer:66)
[2025-10-07 12:42:29,985] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster4808168088233903563
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:38959
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:42:29,987] INFO starting (kafka.server.KafkaServer:66)
[2025-10-07 12:42:29,987] INFO Connecting to zookeeper on 127.0.0.1:38959 (kafka.server.KafkaServer:66)
[2025-10-07 12:42:29,987] INFO [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:38959. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:29,987] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:29,991] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:30,004] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:30,006] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 12:42:30,006] INFO Cleared cache (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 12:42:30,008] INFO Cluster ID = L1a9b_g1TQG-Slbo7HGF7g (kafka.server.KafkaServer:66)
[2025-10-07 12:42:30,009] WARN No meta.properties file under dir /tmp/EmbeddedKafkaCluster4808168088233903563/meta.properties (kafka.server.BrokerMetadataCheckpoint:70)
[2025-10-07 12:42:30,010] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster4808168088233903563
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:38959
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:42:30,012] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.0-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/EmbeddedKafkaCluster4808168088233903563
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:38959
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig:376)
[2025-10-07 12:42:30,020] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:30,021] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:30,021] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:30,022] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:30,023] INFO Loading logs from log dirs ArraySeq(/tmp/EmbeddedKafkaCluster4808168088233903563) (kafka.log.LogManager:66)
[2025-10-07 12:42:30,023] INFO Attempting recovery for all logs in /tmp/EmbeddedKafkaCluster4808168088233903563 since no clean shutdown file was found (kafka.log.LogManager:66)
[2025-10-07 12:42:30,023] INFO Loaded 0 logs in 0ms. (kafka.log.LogManager:66)
[2025-10-07 12:42:30,024] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager:66)
[2025-10-07 12:42:30,024] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager:66)
[2025-10-07 12:42:30,024] INFO Starting the log cleaner (kafka.log.LogCleaner:66)
[2025-10-07 12:42:30,039] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner:66)
[2025-10-07 12:42:30,041] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:30,047] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas:66)
[2025-10-07 12:42:30,048] INFO Awaiting socket connections on localhost:45911. (kafka.network.Acceptor:66)
[2025-10-07 12:42:30,049] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 12:42:30,051] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:30,051] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:30,051] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:30,051] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:30,052] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:30,053] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:30,054] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:30,055] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1759840950054,1759840950054,1,0,0,72057677068173312,204,0,25
 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:30,055] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:45911, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:30,064] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:30,064] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:30,064] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:30,064] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:30,065] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:30,065] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:30,066] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient:66)
[2025-10-07 12:42:30,066] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:30,066] INFO [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,066] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:30,067] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:30,068] INFO [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{}) (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,068] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:30,068] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener:66)
[2025-10-07 12:42:30,070] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:30,070] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache:66)
[2025-10-07 12:42:30,070] INFO [Controller id=0] Registering handlers (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,071] INFO [Controller id=0] Deleting log dir event notifications (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,071] INFO [Controller id=0] Deleting isr change notifications (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,071] INFO [Controller id=0] Initializing controller context (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,072] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:30,072] INFO [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25) (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,072] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer:66)
[2025-10-07 12:42:30,072] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:30,073] INFO [KafkaServer id=0] started (kafka.server.KafkaServer:66)
[2025-10-07 12:42:30,075] INFO [Controller id=0] Currently active brokers in the cluster: Set(0) (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,075] INFO [Controller id=0] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,075] INFO [Controller id=0] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,075] INFO [Controller id=0] Fetching topic deletions in progress (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,075] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:30,077] INFO [Controller id=0] List of topics to be deleted:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,077] INFO [Controller id=0] List of topics ineligible for deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,077] INFO [Controller id=0] Initializing topic deletion manager (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,077] INFO [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager:66)
[2025-10-07 12:42:30,077] INFO [Controller id=0] Sending update metadata request (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,077] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:30,077] INFO [ReplicaStateMachine controllerId=0] Initializing replica state (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:30,077] INFO [ReplicaStateMachine controllerId=0] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:30,077] INFO [ReplicaStateMachine controllerId=0] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:30,077] INFO [PartitionStateMachine controllerId=0] Initializing partition state (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:42:30,077] INFO [RequestSendThread controllerId=0] Controller 0 connected to localhost:45911 (id: 0 rack: null) for sending state change requests (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:30,077] INFO [PartitionStateMachine controllerId=0] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:42:30,078] INFO [Controller id=0] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,079] INFO [Controller id=0] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,079] INFO [Controller id=0] Partitions that completed preferred replica election:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,079] INFO [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,079] INFO [Controller id=0] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,079] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,080] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,141] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker localhost:45911 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:30,147] INFO [Controller id=0] Acquired new producerId block ProducerIdsBlock{brokerId=0, producerIdStart=0, producerIdLen=1000} by writing to Zk with path version 1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:30,151] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:45911 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:31,869] INFO Creating topic connect-offset-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:31,873] INFO [Controller id=0] New topics: [Set(connect-offset-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-offset-topic-elasticsearch-it-connect-cluster,Some(cVArFfLMTNCcOcepKY6dkQ),HashMap(connect-offset-topic-elasticsearch-it-connect-cluster-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-offset-topic-elasticsearch-it-connect-cluster-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:31,874] INFO [Controller id=0] New partition creation callback for connect-offset-topic-elasticsearch-it-connect-cluster-20,connect-offset-topic-elasticsearch-it-connect-cluster-7,connect-offset-topic-elasticsearch-it-connect-cluster-0,connect-offset-topic-elasticsearch-it-connect-cluster-12,connect-offset-topic-elasticsearch-it-connect-cluster-13,connect-offset-topic-elasticsearch-it-connect-cluster-5,connect-offset-topic-elasticsearch-it-connect-cluster-21,connect-offset-topic-elasticsearch-it-connect-cluster-14,connect-offset-topic-elasticsearch-it-connect-cluster-6,connect-offset-topic-elasticsearch-it-connect-cluster-1,connect-offset-topic-elasticsearch-it-connect-cluster-19,connect-offset-topic-elasticsearch-it-connect-cluster-17,connect-offset-topic-elasticsearch-it-connect-cluster-16,connect-offset-topic-elasticsearch-it-connect-cluster-23,connect-offset-topic-elasticsearch-it-connect-cluster-10,connect-offset-topic-elasticsearch-it-connect-cluster-3,connect-offset-topic-elasticsearch-it-connect-cluster-15,connect-offset-topic-elasticsearch-it-connect-cluster-22,connect-offset-topic-elasticsearch-it-connect-cluster-24,connect-offset-topic-elasticsearch-it-connect-cluster-18,connect-offset-topic-elasticsearch-it-connect-cluster-2,connect-offset-topic-elasticsearch-it-connect-cluster-4,connect-offset-topic-elasticsearch-it-connect-cluster-8,connect-offset-topic-elasticsearch-it-connect-cluster-9,connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:31,874] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,874] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,874] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,874] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,874] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,874] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,874] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,874] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,874] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,874] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,874] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,874] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,874] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,875] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,875] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,875] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,875] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,875] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,875] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,875] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,875] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,875] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,875] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,875] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,875] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,875] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:31,881] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:31,890] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,890] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,890] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,890] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,890] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,890] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,890] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,890] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,890] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,890] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,890] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,890] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,891] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,891] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,891] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,891] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,891] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,891] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,891] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,891] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,891] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,891] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,891] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,891] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,891] INFO [Controller id=0 epoch=1] Changed partition connect-offset-topic-elasticsearch-it-connect-cluster-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,891] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 25 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:31,891] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:31,892] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:31,893] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
Oct 07, 2025 12:42:31 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Oct 07, 2025 12:42:31 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Oct 07, 2025 12:42:31 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Oct 07, 2025 12:42:31 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
[2025-10-07 12:42:31,896] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-offset-topic-elasticsearch-it-connect-cluster-20, connect-offset-topic-elasticsearch-it-connect-cluster-7, connect-offset-topic-elasticsearch-it-connect-cluster-0, connect-offset-topic-elasticsearch-it-connect-cluster-12, connect-offset-topic-elasticsearch-it-connect-cluster-13, connect-offset-topic-elasticsearch-it-connect-cluster-5, connect-offset-topic-elasticsearch-it-connect-cluster-21, connect-offset-topic-elasticsearch-it-connect-cluster-14, connect-offset-topic-elasticsearch-it-connect-cluster-6, connect-offset-topic-elasticsearch-it-connect-cluster-1, connect-offset-topic-elasticsearch-it-connect-cluster-19, connect-offset-topic-elasticsearch-it-connect-cluster-17, connect-offset-topic-elasticsearch-it-connect-cluster-16, connect-offset-topic-elasticsearch-it-connect-cluster-23, connect-offset-topic-elasticsearch-it-connect-cluster-10, connect-offset-topic-elasticsearch-it-connect-cluster-3, connect-offset-topic-elasticsearch-it-connect-cluster-15, connect-offset-topic-elasticsearch-it-connect-cluster-22, connect-offset-topic-elasticsearch-it-connect-cluster-24, connect-offset-topic-elasticsearch-it-connect-cluster-18, connect-offset-topic-elasticsearch-it-connect-cluster-2, connect-offset-topic-elasticsearch-it-connect-cluster-4, connect-offset-topic-elasticsearch-it-connect-cluster-8, connect-offset-topic-elasticsearch-it-connect-cluster-9, connect-offset-topic-elasticsearch-it-connect-cluster-11) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:31,896] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:31,897] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-10, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:31,899] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-10 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:31,899] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,899] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-10 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,899] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:31,911] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-14, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:31,911] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-14 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:31,911] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,911] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-14 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,911] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:31,921] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-18, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:31,921] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-18 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:31,921] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,921] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-18 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,921] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
Oct 07, 2025 12:42:31 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

[2025-10-07 12:42:31,932] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:31,933] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:31,933] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,933] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,933] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:31,935] INFO Creating topic test with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:31,938] INFO [Controller id=0] New topics: [Set(test)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(test,Some(TCxexiHITf-eUE841evrwg),Map(test-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:31,938] INFO [Controller id=0] New partition creation callback for test-0 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:31,939] INFO [Controller id=0 epoch=1] Changed partition test-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:31,939] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:31,939] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:31,942] INFO [Controller id=0 epoch=1] Changed partition test-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:31,942] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:31,942] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:31,942] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:31,943] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-22, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:31,943] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-22 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:31,943] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,943] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-22 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,944] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:31,954] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-7, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:31,954] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-7 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:31,954] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,954] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-7 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,954] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:31,965] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-11, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:31,965] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-11 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:31,966] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,966] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-11 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,966] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:31,973] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-15, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:31,973] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-15 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:31,973] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,973] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-15 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,973] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:31,983] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:31,984] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:31,984] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,984] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,984] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:31,992] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-19, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:31,992] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-19 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:31,992] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,992] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-19 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:31,992] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,003] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,003] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,003] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,003] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,003] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,009] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-23, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,010] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-23 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,010] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,010] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-23 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,010] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,017] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-8, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,017] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-8 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,017] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,017] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-8 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,017] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,025] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-12, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,025] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-12 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,026] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,026] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-12 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,026] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,037] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-16, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,037] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-16 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,037] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,037] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-16 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,037] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,048] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,048] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,048] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,048] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,048] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,059] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-20, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,059] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-20 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,059] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,060] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-20 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,060] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,071] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-5, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,071] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-5 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,071] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,071] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-5 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,071] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,082] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-24, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,082] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-24 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,082] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,082] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-24 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,082] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,093] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-9, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,093] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-9 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,093] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,093] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-9 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,093] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,105] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-13, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,105] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-13 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,105] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,105] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-13 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,105] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,116] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-17, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,116] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-17 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,116] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,116] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-17 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,116] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,126] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,127] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,127] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,127] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,127] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,138] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-21, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,138] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-21 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,138] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,138] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-21 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,139] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,149] INFO [LogLoader partition=connect-offset-topic-elasticsearch-it-connect-cluster-6, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,149] INFO Created log for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-offset-topic-elasticsearch-it-connect-cluster-6 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,149] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] No checkpointed highwatermark is found for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,149] INFO [Partition connect-offset-topic-elasticsearch-it-connect-cluster-6 broker=0] Log loaded for partition connect-offset-topic-elasticsearch-it-connect-cluster-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,149] INFO [Broker id=0] Leader connect-offset-topic-elasticsearch-it-connect-cluster-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,159] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 0 for 25 partitions (state.change.logger:66)
[2025-10-07 12:42:32,160] INFO [Broker id=0] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger:66)
[2025-10-07 12:42:32,162] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:32,163] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:32,163] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:32,166] INFO [LogLoader partition=test-0, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,166] INFO Created log for partition test-0 in /tmp/EmbeddedKafkaCluster4808168088233903563/test-0 with properties {} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,166] INFO [Partition test-0 broker=0] No checkpointed highwatermark is found for partition test-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,166] INFO [Partition test-0 broker=0] Log loaded for partition test-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,166] INFO [Broker id=0] Leader test-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,179] INFO Creating topic connect-storage-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:32,179] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:32,180] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger:66)
[2025-10-07 12:42:32,183] INFO [Controller id=0] New topics: [Set(connect-storage-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-storage-topic-elasticsearch-it-connect-cluster,Some(IM1CUTjoTjCEE6JxCYUKbA),HashMap(connect-storage-topic-elasticsearch-it-connect-cluster-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), connect-storage-topic-elasticsearch-it-connect-cluster-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:32,183] INFO [Controller id=0] New partition creation callback for connect-storage-topic-elasticsearch-it-connect-cluster-2,connect-storage-topic-elasticsearch-it-connect-cluster-0,connect-storage-topic-elasticsearch-it-connect-cluster-3,connect-storage-topic-elasticsearch-it-connect-cluster-4,connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:32,183] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,183] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,183] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,183] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,183] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,183] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:32,183] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:32,186] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,188] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,188] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,188] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,188] INFO [Controller id=0 epoch=1] Changed partition connect-storage-topic-elasticsearch-it-connect-cluster-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,188] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:32,188] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:32,189] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:32,189] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:32,189] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(connect-storage-topic-elasticsearch-it-connect-cluster-2, connect-storage-topic-elasticsearch-it-connect-cluster-0, connect-storage-topic-elasticsearch-it-connect-cluster-3, connect-storage-topic-elasticsearch-it-connect-cluster-4, connect-storage-topic-elasticsearch-it-connect-cluster-1) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:32,189] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:32,195] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-4, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,195] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-storage-topic-elasticsearch-it-connect-cluster-4 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,195] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,196] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-4 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,196] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,208] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-3, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,208] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-storage-topic-elasticsearch-it-connect-cluster-3 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,208] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,208] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-3 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,208] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,218] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-2, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,219] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-storage-topic-elasticsearch-it-connect-cluster-2 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,219] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,219] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-2 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,219] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,229] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-1, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,229] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-storage-topic-elasticsearch-it-connect-cluster-1 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,229] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,229] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-1 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,229] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,239] INFO [LogLoader partition=connect-storage-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,240] INFO Created log for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-storage-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,240] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,240] INFO [Partition connect-storage-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-storage-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,240] INFO [Broker id=0] Leader connect-storage-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,250] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 5 from controller 0 for 5 partitions (state.change.logger:66)
[2025-10-07 12:42:32,251] INFO [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6 (state.change.logger:66)
[2025-10-07 12:42:32,268] INFO Creating topic connect-config-topic-elasticsearch-it-connect-cluster with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:32,271] INFO [Controller id=0] New topics: [Set(connect-config-topic-elasticsearch-it-connect-cluster)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect-config-topic-elasticsearch-it-connect-cluster,Some(UjBpUXQtRbKx_lpXHKbRZQ),Map(connect-config-topic-elasticsearch-it-connect-cluster-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:32,271] INFO [Controller id=0] New partition creation callback for connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:32,272] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,272] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:32,272] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:32,274] INFO [Controller id=0 epoch=1] Changed partition connect-config-topic-elasticsearch-it-connect-cluster-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,274] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:32,274] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:32,274] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:32,274] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:32,274] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(connect-config-topic-elasticsearch-it-connect-cluster-0) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:32,275] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:32,276] INFO [LogLoader partition=connect-config-topic-elasticsearch-it-connect-cluster-0, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,276] INFO Created log for partition connect-config-topic-elasticsearch-it-connect-cluster-0 in /tmp/EmbeddedKafkaCluster4808168088233903563/connect-config-topic-elasticsearch-it-connect-cluster-0 with properties {cleanup.policy=compact} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,277] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] No checkpointed highwatermark is found for partition connect-config-topic-elasticsearch-it-connect-cluster-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,277] INFO [Partition connect-config-topic-elasticsearch-it-connect-cluster-0 broker=0] Log loaded for partition connect-config-topic-elasticsearch-it-connect-cluster-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,277] INFO [Broker id=0] Leader connect-config-topic-elasticsearch-it-connect-cluster-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,286] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 7 from controller 0 for 1 partitions (state.change.logger:66)
[2025-10-07 12:42:32,287] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger:66)
[2025-10-07 12:42:32,307] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient:66)
[2025-10-07 12:42:32,310] INFO [Controller id=0] New topics: [HashSet(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(PxFbyZzNR0qlSKj-yCTaRQ),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController:66)
[2025-10-07 12:42:32,310] INFO [Controller id=0] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:32,310] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,310] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,310] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,310] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,310] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,311] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,312] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,312] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,312] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,312] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,312] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,312] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,312] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,312] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,312] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger:66)
[2025-10-07 12:42:32,312] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:32,312] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:32,323] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,323] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,323] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,323] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,323] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,323] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,323] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,323] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,323] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,323] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,323] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,324] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,325] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,325] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,325] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,325] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,325] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,325] INFO [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger:66)
[2025-10-07 12:42:32,325] INFO [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 50 become-leader and 0 become-follower partitions (state.change.logger:66)
[2025-10-07 12:42:32,325] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:32,325] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:32,325] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:32,329] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:32,329] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 0 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:32,330] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,331] INFO Created log for partition __consumer_offsets-3 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,331] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,331] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,331] INFO [Broker id=0] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,341] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,342] INFO Created log for partition __consumer_offsets-18 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,342] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,342] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,342] INFO [Broker id=0] Leader __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,352] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,352] INFO Created log for partition __consumer_offsets-41 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,352] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,352] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,352] INFO [Broker id=0] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,363] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,363] INFO Created log for partition __consumer_offsets-10 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,363] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,363] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,363] INFO [Broker id=0] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,374] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,374] INFO Created log for partition __consumer_offsets-33 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,374] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,374] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,374] INFO [Broker id=0] Leader __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,385] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,385] INFO Created log for partition __consumer_offsets-48 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,385] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,385] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,385] INFO [Broker id=0] Leader __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,396] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,396] INFO Created log for partition __consumer_offsets-19 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,396] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,396] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,397] INFO [Broker id=0] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,408] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,408] INFO Created log for partition __consumer_offsets-34 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,408] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,408] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,408] INFO [Broker id=0] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,419] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,419] INFO Created log for partition __consumer_offsets-4 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,419] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,419] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,419] INFO [Broker id=0] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,430] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,430] INFO Created log for partition __consumer_offsets-11 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,430] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,430] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,430] INFO [Broker id=0] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,441] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,441] INFO Created log for partition __consumer_offsets-26 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,441] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,441] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,441] INFO [Broker id=0] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,452] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,452] INFO Created log for partition __consumer_offsets-49 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,452] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,452] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,452] INFO [Broker id=0] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,463] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,463] INFO Created log for partition __consumer_offsets-39 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,463] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,463] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,463] INFO [Broker id=0] Leader __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,474] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,474] INFO Created log for partition __consumer_offsets-9 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,474] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,474] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,474] INFO [Broker id=0] Leader __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,485] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,485] INFO Created log for partition __consumer_offsets-24 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,485] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,485] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,485] INFO [Broker id=0] Leader __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,496] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,496] INFO Created log for partition __consumer_offsets-31 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,496] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,496] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,496] INFO [Broker id=0] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,507] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,508] INFO Created log for partition __consumer_offsets-46 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,508] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,508] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,508] INFO [Broker id=0] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,518] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,518] INFO Created log for partition __consumer_offsets-1 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,518] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,518] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,518] INFO [Broker id=0] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,529] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,530] INFO Created log for partition __consumer_offsets-16 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,530] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,530] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,530] INFO [Broker id=0] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,541] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,541] INFO Created log for partition __consumer_offsets-2 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,541] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,541] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,541] INFO [Broker id=0] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,548] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,548] INFO Created log for partition __consumer_offsets-25 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,548] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,548] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,548] INFO [Broker id=0] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,559] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,559] INFO Created log for partition __consumer_offsets-40 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,559] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,559] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,559] INFO [Broker id=0] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,570] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,570] INFO Created log for partition __consumer_offsets-47 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,570] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,570] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,570] INFO [Broker id=0] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,581] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,581] INFO Created log for partition __consumer_offsets-17 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,581] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,581] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,581] INFO [Broker id=0] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,592] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,592] INFO Created log for partition __consumer_offsets-32 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,593] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,593] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,593] INFO [Broker id=0] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,603] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,604] INFO Created log for partition __consumer_offsets-37 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,604] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,604] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,604] INFO [Broker id=0] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,615] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,615] INFO Created log for partition __consumer_offsets-7 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,615] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,615] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,615] INFO [Broker id=0] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,626] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,626] INFO Created log for partition __consumer_offsets-22 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,626] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,626] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,626] INFO [Broker id=0] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,637] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,637] INFO Created log for partition __consumer_offsets-29 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,637] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,637] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,637] INFO [Broker id=0] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,648] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,648] INFO Created log for partition __consumer_offsets-44 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,648] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,648] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,648] INFO [Broker id=0] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,659] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,659] INFO Created log for partition __consumer_offsets-14 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,659] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,659] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,659] INFO [Broker id=0] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,671] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,671] INFO Created log for partition __consumer_offsets-23 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,671] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,671] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,671] INFO [Broker id=0] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,681] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,681] INFO Created log for partition __consumer_offsets-38 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,681] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,681] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,681] INFO [Broker id=0] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,692] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,692] INFO Created log for partition __consumer_offsets-8 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,692] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,692] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,692] INFO [Broker id=0] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,703] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,703] INFO Created log for partition __consumer_offsets-45 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,703] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,703] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,703] INFO [Broker id=0] Leader __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,713] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,713] INFO Created log for partition __consumer_offsets-15 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,713] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,713] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,713] INFO [Broker id=0] Leader __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,724] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,724] INFO Created log for partition __consumer_offsets-30 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,724] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,724] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,724] INFO [Broker id=0] Leader __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,735] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,735] INFO Created log for partition __consumer_offsets-0 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,735] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,735] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,735] INFO [Broker id=0] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,746] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,746] INFO Created log for partition __consumer_offsets-35 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,746] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,746] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,746] INFO [Broker id=0] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,757] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,757] INFO Created log for partition __consumer_offsets-5 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,757] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,757] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,757] INFO [Broker id=0] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,768] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,768] INFO Created log for partition __consumer_offsets-20 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,768] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,768] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,768] INFO [Broker id=0] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,778] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,779] INFO Created log for partition __consumer_offsets-27 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,779] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,779] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,779] INFO [Broker id=0] Leader __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,789] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,790] INFO Created log for partition __consumer_offsets-42 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,790] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,790] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,790] INFO [Broker id=0] Leader __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,800] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,801] INFO Created log for partition __consumer_offsets-12 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,801] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,801] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,801] INFO [Broker id=0] Leader __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,811] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,811] INFO Created log for partition __consumer_offsets-21 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,811] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,812] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,812] INFO [Broker id=0] Leader __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,821] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,822] INFO Created log for partition __consumer_offsets-36 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,822] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,822] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,822] INFO [Broker id=0] Leader __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,832] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,832] INFO Created log for partition __consumer_offsets-6 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,832] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,832] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,832] INFO [Broker id=0] Leader __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,843] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,843] INFO Created log for partition __consumer_offsets-43 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,843] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,843] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,843] INFO [Broker id=0] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,854] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,854] INFO Created log for partition __consumer_offsets-13 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,854] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,854] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,854] INFO [Broker id=0] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,865] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/EmbeddedKafkaCluster4808168088233903563] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$:2007)
[2025-10-07 12:42:32,865] INFO Created log for partition __consumer_offsets-28 in /tmp/EmbeddedKafkaCluster4808168088233903563/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager:66)
[2025-10-07 12:42:32,865] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,865] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition:66)
[2025-10-07 12:42:32,865] INFO [Broker id=0] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger:66)
[2025-10-07 12:42:32,875] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,875] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,876] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,876] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,876] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,876] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,876] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,876] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,876] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,876] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,876] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,877] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,877] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,877] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,877] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,877] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,877] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,877] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,877] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,877] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,877] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,877] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,877] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,877] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,877] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,877] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,877] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,877] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,877] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,878] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,878] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,878] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,878] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,878] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,878] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,878] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,878] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,878] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,878] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,878] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,878] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,878] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,878] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,878] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,878] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,878] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,878] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,878] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,878] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,878] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,879] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,879] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,879] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,879] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,879] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,879] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,879] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,879] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,879] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,879] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,879] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,879] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,879] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,879] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,879] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,879] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,879] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,879] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,879] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,879] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,879] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,879] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,880] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,880] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,880] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,880] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,880] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,880] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,880] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,880] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,880] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,880] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,880] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,880] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,880] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,880] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,880] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,880] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,880] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,880] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,880] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,880] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,880] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,880] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,882] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,882] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,882] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,882] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,882] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,882] INFO [GroupCoordinator 0]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,882] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,881] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,882] INFO [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 9 from controller 0 for 50 partitions (state.change.logger:66)
[2025-10-07 12:42:32,882] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,882] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,882] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,882] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,882] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,882] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,882] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,882] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,882] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,883] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,883] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,883] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,883] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,883] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,883] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,883] INFO [Broker id=0] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 10 (state.change.logger:66)
[2025-10-07 12:42:32,883] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,883] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,883] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,883] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)
[2025-10-07 12:42:32,916] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-integration-test-elasticsearch-it-connect-cluster in Empty state. Created a new member id connect-6-d7c7748e-8bff-4489-8c07-d7dfa5696fde and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,918] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member connect-6-d7c7748e-8bff-4489-8c07-d7dfa5696fde with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,920] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 1 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,922] INFO [GroupCoordinator 0]: Assignment received from leader connect-6-d7c7748e-8bff-4489-8c07-d7dfa5696fde for group connect-integration-test-elasticsearch-it-connect-cluster for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,930] INFO ElasticsearchSinkConnectorConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:36481/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 600000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 600000
	max.buffered.records = 10
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 600000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 12:42:32,931] DEBUG Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:42:32,931] INFO Using unsecured connection to [http://localhost:36481]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:42:32,947] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 1 (__consumer_offsets-4) (reason: Leader connect-6-d7c7748e-8bff-4489-8c07-d7dfa5696fde re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,947] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 2 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,949] INFO [GroupCoordinator 0]: Assignment received from leader connect-6-d7c7748e-8bff-4489-8c07-d7dfa5696fde for group connect-integration-test-elasticsearch-it-connect-cluster for generation 2. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,953] INFO [es-connector|worker] ElasticsearchSinkConnectorConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:36481/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 600000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 600000
	max.buffered.records = 10
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 600000
	retry.backoff.ms = 10
	schema.ignore = true
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkConnectorConfig:376)
[2025-10-07 12:42:32,959] ERROR Could not check connector state info. (io.confluent.connect.elasticsearch.integration.BaseConnectorIT:92)
[2025-10-07 12:42:32,963] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 2 (__consumer_offsets-4) (reason: Leader connect-6-d7c7748e-8bff-4489-8c07-d7dfa5696fde re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,964] INFO [GroupCoordinator 0]: Stabilized group connect-integration-test-elasticsearch-it-connect-cluster generation 3 (__consumer_offsets-4) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,965] INFO [GroupCoordinator 0]: Assignment received from leader connect-6-d7c7748e-8bff-4489-8c07-d7dfa5696fde for group connect-integration-test-elasticsearch-it-connect-cluster for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,970] INFO [es-connector|task-0] Starting ElasticsearchSinkTask. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:61)
[2025-10-07 12:42:32,970] INFO [es-connector|task-0] ElasticsearchSinkTaskConfig values: 
	batch.size = 1
	behavior.on.malformed.documents = FAIL
	behavior.on.null.values = FAIL
	bulk.size.bytes = 5242880
	compact.map.entries = true
	connection.compression = false
	connection.password = null
	connection.timeout.ms = 1000
	connection.url = [http://localhost:36481/]
	connection.username = null
	data.stream.dataset = 
	data.stream.namespace = ${topic}
	data.stream.timestamp.field = []
	data.stream.type = NONE
	drop.invalid.message = false
	elastic.https.ssl.cipher.suites = null
	elastic.https.ssl.enabled.protocols = [TLSv1.2]
	elastic.https.ssl.endpoint.identification.algorithm = https
	elastic.https.ssl.engine.factory.class = null
	elastic.https.ssl.key.password = null
	elastic.https.ssl.keymanager.algorithm = SunX509
	elastic.https.ssl.keystore.certificate.chain = null
	elastic.https.ssl.keystore.key = null
	elastic.https.ssl.keystore.location = null
	elastic.https.ssl.keystore.password = null
	elastic.https.ssl.keystore.type = JKS
	elastic.https.ssl.protocol = TLSv1.2
	elastic.https.ssl.provider = null
	elastic.https.ssl.secure.random.implementation = null
	elastic.https.ssl.trustmanager.algorithm = PKIX
	elastic.https.ssl.truststore.certificates = null
	elastic.https.ssl.truststore.location = null
	elastic.https.ssl.truststore.password = null
	elastic.https.ssl.truststore.type = JKS
	elastic.security.protocol = PLAINTEXT
	external.resource.usage = DISABLED
	external.version.header = 
	flush.synchronously = false
	flush.timeout.ms = 600000
	kerberos.keytab.path = null
	kerberos.user.principal = null
	key.ignore = true
	linger.ms = 600000
	max.buffered.records = 10
	max.connection.idle.time.ms = 60000
	max.external.resource.mappings = 15
	max.in.flight.requests = 4
	max.retries = 0
	proxy.host = 
	proxy.password = null
	proxy.port = 8080
	proxy.username = 
	read.timeout.ms = 600000
	retry.backoff.ms = 10
	schema.ignore = true
	taskId = 0
	topic.key.ignore = []
	topic.schema.ignore = []
	topic.to.external.resource.mapping = []
	use.autogenerated.ids = false
	write.method = INSERT
 (io.confluent.connect.elasticsearch.ElasticsearchSinkTaskConfig:376)
[2025-10-07 12:42:32,971] INFO [es-connector|task-0] Errant record reporter not configured. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:86)
[2025-10-07 12:42:32,971] DEBUG [es-connector|task-0] Connection pool config: maxPerRoute: 10, maxTotal 10 (io.confluent.connect.elasticsearch.ConfigCallbackHandler:187)
[2025-10-07 12:42:32,971] INFO [es-connector|task-0] Using unsecured connection to [http://localhost:36481]. (io.confluent.connect.elasticsearch.ConfigCallbackHandler:115)
[2025-10-07 12:42:32,980] INFO [es-connector|task-0] Started ElasticsearchSinkTask. Connecting to ES server version: 7.16.3 (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:105)
[2025-10-07 12:42:32,987] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-es-connector in Empty state. Created a new member id connector-consumer-es-connector-0-8fabaae1-f6d8-4918-9ade-875f559d09f0 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,988] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member connector-consumer-es-connector-0-8fabaae1-f6d8-4918-9ade-875f559d09f0 with group instance id None) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,989] INFO [GroupCoordinator 0]: Stabilized group connect-es-connector generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:32,991] INFO [GroupCoordinator 0]: Assignment received from leader connector-consumer-es-connector-0-8fabaae1-f6d8-4918-9ade-875f559d09f0 for group connect-es-connector for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:33,073] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,073] INFO [es-connector|task-0] Creating index test. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:242)
[2025-10-07 12:42:33,173] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,175] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,179] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,181] DEBUG [es-connector|task-0] Putting 4 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,189] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,190] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,191] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,192] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,192] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,194] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,194] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,195] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,196] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,197] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,197] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,198] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,199] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,200] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,201] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,202] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,203] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,204] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,204] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,205] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,206] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,207] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,208] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,209] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,210] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,211] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,211] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,212] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,213] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,213] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,214] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,215] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,215] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,216] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,217] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,218] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,218] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,219] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,220] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,220] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,221] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,222] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,222] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,223] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,224] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,225] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,226] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,226] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,227] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,228] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,229] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,229] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,230] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,231] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,231] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,232] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,233] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,233] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,234] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,235] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,235] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,236] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,237] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,237] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,238] DEBUG [es-connector|task-0] Putting 2 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,239] DEBUG [es-connector|task-0] Putting 1 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,239] DEBUG [es-connector|task-0] Pausing all partitions (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:363)
[2025-10-07 12:42:33,340] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,440] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,540] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,641] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,741] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,842] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:33,942] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:34,043] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:34,143] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:34,244] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:34,344] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:34,445] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:34,545] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:34,645] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:34,746] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:34,846] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:34,947] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:35,047] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:35,081] INFO [Controller id=0] Processing automatic preferred replica leader election (kafka.controller.KafkaController:66)
[2025-10-07 12:42:35,148] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:35,248] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:35,349] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:35,449] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:35,549] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:35,650] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:35,699] ERROR [es-connector|task-0] Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36481], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];' after 1 attempt(s) (io.confluent.connect.elasticsearch.RetryUtil:164)
ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36481], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:36481], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 12:42:35,700] WARN [es-connector|task-0] Bulk request 1 failed (io.confluent.connect.elasticsearch.ElasticsearchClient:454)
org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36481], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];' after 1 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36481], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:36481], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 12:42:35,750] DEBUG [es-connector|task-0] Putting 0 records to Elasticsearch. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:111)
[2025-10-07 12:42:35,752] ERROR [es-connector|task-0] WorkerSinkTask{id=es-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: Bulk request failed (org.apache.kafka.connect.runtime.WorkerSinkTask:629)
org.apache.kafka.connect.errors.ConnectException: Bulk request failed
	at io.confluent.connect.elasticsearch.ElasticsearchClient$2.afterBulk(ElasticsearchClient.java:455)
	at org.elasticsearch.action.bulk.BulkRequestHandler$1.onFailure(BulkRequestHandler.java:64)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.ActionListener$RunAfterActionListener.onFailure(ActionListener.java:350)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onFailure(Retry.java:123)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:227)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36481], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];' after 1 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	... 5 more
Caused by: ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36481], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:36481], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 12:42:35,754] DEBUG [es-connector|task-0] Tried to flush data to Elasticsearch, but BulkProcessor is already closed. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:137)
java.lang.IllegalStateException: bulk process already closed
	at org.elasticsearch.action.bulk.BulkProcessor.ensureOpen(BulkProcessor.java:454)
	at org.elasticsearch.action.bulk.BulkProcessor.flush(BulkProcessor.java:571)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.flush(ElasticsearchClient.java:303)
	at io.confluent.connect.elasticsearch.ElasticsearchSinkTask.preCommit(ElasticsearchSinkTask.java:135)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.commitOffsets(WorkerSinkTask.java:404)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.closePartitions(WorkerSinkTask.java:666)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.closeAllPartitions(WorkerSinkTask.java:661)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:204)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[2025-10-07 12:42:35,754] DEBUG [es-connector|task-0] preCommitting offsets {} (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:140)
[2025-10-07 12:42:35,754] ERROR [es-connector|task-0] WorkerSinkTask{id=es-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:195)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:631)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:333)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:234)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:203)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.kafka.connect.errors.ConnectException: Bulk request failed
	at io.confluent.connect.elasticsearch.ElasticsearchClient$2.afterBulk(ElasticsearchClient.java:455)
	at org.elasticsearch.action.bulk.BulkRequestHandler$1.onFailure(BulkRequestHandler.java:64)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.ActionListener$RunAfterActionListener.onFailure(ActionListener.java:350)
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66)
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onFailure(Retry.java:123)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:227)
	... 5 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to execute bulk request due to 'ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36481], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];' after 1 attempt(s)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:165)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:119)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.callWithRetries(ElasticsearchClient.java:483)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$1(ElasticsearchClient.java:221)
	... 5 more
Caused by: ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://localhost:36481], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]
];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2464)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2184)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:2137)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:2105)
	at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:620)
	at io.confluent.connect.elasticsearch.ElasticsearchClient.lambda$null$0(ElasticsearchClient.java:223)
	at io.confluent.connect.elasticsearch.RetryUtil.callWithRetries(RetryUtil.java:158)
	... 8 more
	Suppressed: java.lang.IllegalStateException: Elasticsearch didn't return the [Content-Type] header, unable to parse response body
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2477)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2461)
		... 14 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://localhost:36481], URI [/_bulk?timeout=1m], status line [HTTP/1.1 500 Server Error]

	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:313)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:288)
	at org.elasticsearch.client.RestHighLevelClient.performClientRequest(RestHighLevelClient.java:2699)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:2171)
	... 13 more
[2025-10-07 12:42:35,754] DEBUG [es-connector|task-0] Stopping Elasticsearch client. (io.confluent.connect.elasticsearch.ElasticsearchSinkTask:146)
[2025-10-07 12:42:35,755] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-es-connector in state PreparingRebalance with old generation 1 (__consumer_offsets-23) (reason: Removing member connector-consumer-es-connector-0-8fabaae1-f6d8-4918-9ade-875f559d09f0 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:35,755] INFO [GroupCoordinator 0]: Group connect-es-connector with generation 2 is now empty (__consumer_offsets-23) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:35,756] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connector-consumer-es-connector-0-8fabaae1-f6d8-4918-9ade-875f559d09f0, groupInstanceId=None, clientId=connector-consumer-es-connector-0, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group connect-es-connector through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:35,822] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-integration-test-elasticsearch-it-connect-cluster in state PreparingRebalance with old generation 3 (__consumer_offsets-4) (reason: Removing member connect-6-d7c7748e-8bff-4489-8c07-d7dfa5696fde on LeaveGroup) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:35,822] INFO [GroupCoordinator 0]: Group connect-integration-test-elasticsearch-it-connect-cluster with generation 4 is now empty (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:35,823] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connect-6-d7c7748e-8bff-4489-8c07-d7dfa5696fde, groupInstanceId=None, clientId=connect-6, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(sessioned, compatible, default)) has left group connect-integration-test-elasticsearch-it-connect-cluster through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:35,831] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer:66)
[2025-10-07 12:42:35,832] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer:66)
[2025-10-07 12:42:35,833] INFO [Controller id=0] Shutting down broker 0 (kafka.controller.KafkaController:66)
[2025-10-07 12:42:35,834] INFO [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger:66)
[2025-10-07 12:42:35,834] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer:66)
[2025-10-07 12:42:35,834] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:35,835] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:35,835] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread:66)
[2025-10-07 12:42:35,835] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:35,836] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer:66)
[2025-10-07 12:42:35,836] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 12:42:35,836] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool:66)
[2025-10-07 12:42:35,837] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:35,857] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:35,857] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:35,857] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis:66)
[2025-10-07 12:42:35,857] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,057] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,057] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,057] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:36,057] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager:66)
[2025-10-07 12:42:36,057] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:36,058] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:36,058] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager:66)
[2025-10-07 12:42:36,058] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator:66)
[2025-10-07 12:42:36,058] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:36,058] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,243] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,243] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,244] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,392] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,392] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,393] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator:66)
[2025-10-07 12:42:36,393] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager:66)
[2025-10-07 12:42:36,393] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:36,393] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:36,393] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler:66)
[2025-10-07 12:42:36,394] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:36,394] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager:66)
[2025-10-07 12:42:36,394] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 12:42:36,394] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager:66)
[2025-10-07 12:42:36,394] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,523] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,523] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,523] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,657] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,657] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,657] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,857] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,857] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:36,857] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:37,057] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:37,057] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper:66)
[2025-10-07 12:42:37,063] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager:66)
[2025-10-07 12:42:37,064] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:37,064] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:37,064] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:37,064] INFO Broker to controller channel manager for alterIsr shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 12:42:37,064] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:37,065] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:37,065] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread:66)
[2025-10-07 12:42:37,065] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl:66)
[2025-10-07 12:42:37,065] INFO Shutting down. (kafka.log.LogManager:66)
[2025-10-07 12:42:37,065] INFO Shutting down the log cleaner. (kafka.log.LogCleaner:66)
[2025-10-07 12:42:37,065] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner:66)
[2025-10-07 12:42:37,065] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner:66)
[2025-10-07 12:42:37,065] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner:66)
[2025-10-07 12:42:37,078] INFO [ProducerStateManager partition=test-0] Wrote producer snapshot at offset 1001 with 1 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:37,088] INFO [ProducerStateManager partition=__consumer_offsets-23] Wrote producer snapshot at offset 2 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:37,095] INFO [ProducerStateManager partition=connect-config-topic-elasticsearch-it-connect-cluster-0] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:37,102] INFO [ProducerStateManager partition=__consumer_offsets-4] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:37,116] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-2] Wrote producer snapshot at offset 4 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:37,126] INFO [ProducerStateManager partition=connect-storage-topic-elasticsearch-it-connect-cluster-3] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager:66)
[2025-10-07 12:42:37,139] INFO Shutdown complete. (kafka.log.LogManager:66)
[2025-10-07 12:42:37,139] INFO [ControllerEventThread controllerId=0] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:37,139] INFO [ControllerEventThread controllerId=0] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:37,139] INFO [ControllerEventThread controllerId=0] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread:66)
[2025-10-07 12:42:37,140] INFO [PartitionStateMachine controllerId=0] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine:66)
[2025-10-07 12:42:37,140] INFO [ReplicaStateMachine controllerId=0] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine:66)
[2025-10-07 12:42:37,141] INFO [RequestSendThread controllerId=0] Shutting down (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:37,141] INFO [RequestSendThread controllerId=0] Stopped (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:37,141] INFO [RequestSendThread controllerId=0] Shutdown completed (kafka.controller.RequestSendThread:66)
[2025-10-07 12:42:37,141] INFO [Controller id=0] Resigned (kafka.controller.KafkaController:66)
[2025-10-07 12:42:37,142] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:37,142] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:37,142] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread:66)
[2025-10-07 12:42:37,142] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:37,245] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient:66)
[2025-10-07 12:42:37,245] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:38,021] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:38,021] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:38,021] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:38,021] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:38,022] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:38,022] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:38,022] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:38,022] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:38,022] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:38,023] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:38,023] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper:66)
[2025-10-07 12:42:38,023] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer:66)
[2025-10-07 12:42:38,029] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer:66)
[2025-10-07 12:42:38,029] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats:66)
[2025-10-07 12:42:38,030] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer:66)
[WARNING] Tests run: 7, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 51.796 s - in io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorNetworkIT
[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Errors: 
[ERROR]   ElasticsearchConnectorIT.setupBeforeAll:80 Â» ContainerLaunch Container startup...
[ERROR]   ElasticsearchConnectorKerberosIT.setupBeforeAll:37 Â» ContainerLaunch Container...
[ERROR]   ElasticsearchConnectorKerberosWithSslIT.setupBeforeAll:27 Â» ContainerLaunch Co...
[INFO] 
[ERROR] Tests run: 24, Failures: 0, Errors: 3, Skipped: 2
[INFO] 
[INFO] 
[INFO] --- failsafe:3.0.0-M3:verify (verify) @ kafka-connect-elasticsearch ---
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  20:05 min
[INFO] Finished at: 2025-10-07T12:42:39Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-failsafe-plugin:3.0.0-M3:verify (verify) on project kafka-connect-elasticsearch: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/semaphore/kafka-connect-elasticsearch/target/failsafe-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[0m[0mExporting environment variables
Exporting SEMAPHORE_JOB_RESULT
. publish-test-results
Collecting results from surefire-reports tests in each module
Collecting results from failsafe-reports tests in each module
  Copying failsafe-reports test logs from './target/failsafe-reports'
* Using generic parser
* Saving results to /tmp/test-results-3415878318/result-2189559497.json
* Using generic parser
* Saving results to /tmp/test-results-3415878318/result-1861467701.json
* Using generic parser
* Saving results to /tmp/test-results-3415878318/result-536578157.json
* Using generic parser
* Saving results to /tmp/test-results-3415878318/result-3183666321.json
* Using generic parser
* Saving results to /tmp/test-results-3415878318/result-4031405628.json
* Saving results to /tmp/test-results491987512
* Pushing artifacts:
$ /usr/local/bin/artifact push job /tmp/test-results491987512 -d test-results/junit.json
* starting to generate summary
* Saving results to /tmp/test-results49364716
* Pushing artifacts:
$ /usr/local/bin/artifact push job /tmp/test-results49364716 -d test-results/summary.json
* Pushing artifacts:
$ /usr/local/bin/artifact push workflow /tmp/test-results491987512 -d test-results/86e10d08-043c-4a0f-9199-dfe4f1f9e2d1/8de7930c-8a36-4a96-956e-bd88faad4487.json
* Pushing artifacts:
$ /usr/local/bin/artifact push job target/test-results/TEST-io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorIT.xml -d test-results/junit-0.xml
* Pushing artifacts:
$ /usr/local/bin/artifact push job target/test-results/TEST-io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosIT.xml -d test-results/junit-1.xml
* Pushing artifacts:
$ /usr/local/bin/artifact push job target/test-results/TEST-io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorKerberosWithSslIT.xml -d test-results/junit-2.xml
* Pushing artifacts:
$ /usr/local/bin/artifact push job target/test-results/TEST-io.confluent.connect.elasticsearch.integration.ElasticsearchConnectorNetworkIT.xml -d test-results/junit-3.xml
* Pushing artifacts:
$ /usr/local/bin/artifact push job target/test-results/TEST-io.confluent.connect.elasticsearch.integration.ElasticsearchSinkTaskIT.xml -d test-results/junit-4.xml
Publish tests to semaphore
artifact push workflow target/test-results
[Oct  7 12:42:42.004] Successfully pushed artifact for current workflow.
[Oct  7 12:42:42.004] * Local source: target/test-results.
[Oct  7 12:42:42.004] * Remote destination: artifacts/workflows/0524bcb5-2c4f-4f5b-a5a2-c31a835190ef/test-results.
artifact push workflow target
[Oct  7 12:43:01.660] Successfully pushed artifact for current workflow.
[Oct  7 12:43:01.660] * Local source: target.
[Oct  7 12:43:01.660] * Remote destination: artifacts/workflows/0524bcb5-2c4f-4f5b-a5a2-c31a835190ef/target.
Running the post-job hook configured in the agent
Running: bash /opt/semaphore/agent/hooks/post-job-hook
+ echo 'Running post-job script'
Running post-job script
+ [[ s1-prod-ubuntu24-04-amd64-4 == *\i\n\i\t\i\a\l\i\z\a\t\i\o\n ]]
+ case $SEMAPHORE_ORGANIZATION_URL in
+ cd /opt/semaphore/agent/hooks/
+ set +x
+ uv run --no-sync python post_job.py
2025-10-07 12:43:05,048 - root - INFO - Collecting pipeline metrics
2025-10-07 12:43:05,048 - root - INFO - Creating the event to be sent to Kafka
2025-10-07 12:43:05,048 - root - INFO - Job 8de7930c-8a36-4a96-956e-bd88faad4487 started at 1759839661 and ran for 1324 seconds with result failed
2025-10-07 12:43:10,497 - root - INFO - CI metrics event: {'branch': 'CC-36923/ext_resource_usage_limit', 'commit': '08ce85034268ec229d7d73cfb04c0d3c0faad98a', 'author_name': 'airlock-confluentinc[bot]', 'repo': 'confluentinc/kafka-connect-elasticsearch', 'name': 'kafka-connect-elasticsearch', 'job_url': 'https://semaphore.ci.confluent.io/jobs/8de7930c-8a36-4a96-956e-bd88faad4487', 'pipeline_url': 'https://semaphore.ci.confluent.io/workflows/0524bcb5-2c4f-4f5b-a5a2-c31a835190ef?pipeline_id=86e10d08-043c-4a0f-9199-dfe4f1f9e2d1', 'job_id': '8de7930c-8a36-4a96-956e-bd88faad4487', 'job_name': 'Test', 'job_type': 'unknown', 'job_status': 'failed', 'job_start_time': '1759839661', 'job_duration': 1324, 'pipeline_id': '86e10d08-043c-4a0f-9199-dfe4f1f9e2d1', 'ref_type': 'pull-request'}
2025-10-07 12:43:10,497 - root - INFO - Producing an event
2025-10-07 12:43:11,123 - root - INFO - Message delivered to ci_pipeline_metrics [6]
2025-10-07 12:43:11,123 - root - INFO - Message produced to Kafka broker
%6|1759840991.123|GETSUBSCRIPTIONS|rdkafka#producer-1| [thrd:main]: Telemetry client instance id changed from AAAAAAAAAAAAAAAAAAAAAA to oPScRsH7TdOMXDHKxxmFQg
2025-10-07 12:43:11,125 - root - INFO - Collecting granular job metrics
{
    "resource_metrics": [
        {
            "resource": {
                "attributes": {
                    "telemetry.sdk.language": "python",
                    "telemetry.sdk.name": "opentelemetry",
                    "telemetry.sdk.version": "1.36.0",
                    "service.namespace": "ci-metrics",
                    "service.name": "post_job.py"
                },
                "schema_url": ""
            },
            "scope_metrics": [
                {
                    "scope": {
                        "name": "ci-metrics-meter",
                        "version": "0.1.0",
                        "schema_url": "",
                        "attributes": null
                    },
                    "metrics": [
                        {
                            "name": "ci_command_duration",
                            "description": "Histogram of duration for common commands in CI jobs",
                            "unit": "s",
                            "data": {
                                "data_points": [
                                    {
                                        "attributes": {
                                            "command": "Running the pre-job hook configured in the agent",
                                            "git_repo_name": "confluentinc/kafka-connect-elasticsearch",
                                            "pipeline_id": "86e10d08-043c-4a0f-9199-dfe4f1f9e2d1",
                                            "job_id": "8de7930c-8a36-4a96-956e-bd88faad4487",
                                            "job_name": "Test",
                                            "pipeline_name": "build-test-release",
                                            "branch_name": "CC-36923/ext_resource_usage_limit",
                                            "ref_type": "pull-request",
                                            "exit_code": 0,
                                            "job_result": "failed",
                                            "command_type": "pre-job",
                                            "workflow_rerun": "false"
                                        },
                                        "start_time_unix_nano": 1759840991346549107,
                                        "time_unix_nano": 1759840991346689280,
                                        "count": 1,
                                        "sum": 7.0,
                                        "bucket_counts": [
                                            0,
                                            0,
                                            1,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0
                                        ],
                                        "explicit_bounds": [
                                            0.0,
                                            5.0,
                                            10.0,
                                            25.0,
                                            50.0,
                                            75.0,
                                            100.0,
                                            250.0,
                                            500.0,
                                            750.0,
                                            1000.0,
                                            2500.0,
                                            5000.0,
                                            7500.0,
                                            10000.0
                                        ],
                                        "min": 7.0,
                                        "max": 7.0,
                                        "exemplars": []
                                    },
                                    {
                                        "attributes": {
                                            "command": "Running the post-job hook configured in the agent",
                                            "git_repo_name": "confluentinc/kafka-connect-elasticsearch",
                                            "pipeline_id": "86e10d08-043c-4a0f-9199-dfe4f1f9e2d1",
                                            "job_id": "8de7930c-8a36-4a96-956e-bd88faad4487",
                                            "job_name": "Test",
                                            "pipeline_name": "build-test-release",
                                            "branch_name": "CC-36923/ext_resource_usage_limit",
                                            "ref_type": "pull-request",
                                            "exit_code": 0,
                                            "job_result": "failed",
                                            "command_type": "post-job",
                                            "workflow_rerun": "false"
                                        },
                                        "start_time_unix_nano": 1759840991346609078,
                                        "time_unix_nano": 1759840991346689280,
                                        "count": 1,
                                        "sum": 10.0,
                                        "bucket_counts": [
                                            0,
                                            0,
                                            1,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0,
                                            0
                                        ],
                                        "explicit_bounds": [
                                            0.0,
                                            5.0,
                                            10.0,
                                            25.0,
                                            50.0,
                                            75.0,
                                            100.0,
                                            250.0,
                                            500.0,
                                            750.0,
                                            1000.0,
                                            2500.0,
                                            5000.0,
                                            7500.0,
                                            10000.0
                                        ],
                                        "min": 10.0,
                                        "max": 10.0,
                                        "exemplars": []
                                    }
                                ],
                                "aggregation_temporality": 2
                            }
                        }
                    ],
                    "schema_url": ""
                }
            ],
            "schema_url": ""
        }
    ]
}
+ set +exuo pipefail
Successfully completed post-job script